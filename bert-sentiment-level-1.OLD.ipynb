{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8882386,"sourceType":"datasetVersion","datasetId":5345224}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T06:11:39.035900Z","iopub.execute_input":"2024-08-18T06:11:39.036861Z","iopub.status.idle":"2024-08-18T06:11:39.044198Z","shell.execute_reply.started":"2024-08-18T06:11:39.036822Z","shell.execute_reply":"2024-08-18T06:11:39.043215Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"/kaggle/input/microtext/Dataset - tachygraphy.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:46.564005Z","iopub.execute_input":"2024-08-18T06:11:46.564731Z","iopub.status.idle":"2024-08-18T06:11:46.569158Z","shell.execute_reply.started":"2024-08-18T06:11:46.564693Z","shell.execute_reply":"2024-08-18T06:11:46.568119Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:48.869462Z","iopub.execute_input":"2024-08-18T06:11:48.870139Z","iopub.status.idle":"2024-08-18T06:11:48.874733Z","shell.execute_reply.started":"2024-08-18T06:11:48.870109Z","shell.execute_reply":"2024-08-18T06:11:48.873853Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:51.400103Z","iopub.execute_input":"2024-08-18T06:11:51.400824Z","iopub.status.idle":"2024-08-18T06:11:51.407495Z","shell.execute_reply.started":"2024-08-18T06:11:51.400789Z","shell.execute_reply":"2024-08-18T06:11:51.406390Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/microtext/Dataset - tachygraphy.csv\")\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:53.340781Z","iopub.execute_input":"2024-08-18T06:11:53.341405Z","iopub.status.idle":"2024-08-18T06:11:53.363299Z","shell.execute_reply.started":"2024-08-18T06:11:53.341370Z","shell.execute_reply":"2024-08-18T06:11:53.362324Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"(4958, 3)"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:54.976600Z","iopub.execute_input":"2024-08-18T06:11:54.977398Z","iopub.status.idle":"2024-08-18T06:11:54.988723Z","shell.execute_reply.started":"2024-08-18T06:11:54.977360Z","shell.execute_reply":"2024-08-18T06:11:54.987546Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"                                                Text  \\\n0   Last session of the day http://twitpic.com/67ezh   \n1  Shanghai is also really exciting (precisely --...   \n2                            submit the report ASAP!   \n3                                        happy bday!   \n4                              The OGs - I like it!!   \n\n                                             Meaning Sentiment  \n0   Last session of the day http://twitpic.com/67ezh   neutral  \n1  Shanghai is also really exciting (precisely --...  positive  \n2              submit the report as soon as possilbe  negative  \n3                                    Happy Birthday!  positive  \n4                The original gangsters - i like it!  positive  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Meaning</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Last session of the day http://twitpic.com/67ezh</td>\n      <td>Last session of the day http://twitpic.com/67ezh</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Shanghai is also really exciting (precisely --...</td>\n      <td>Shanghai is also really exciting (precisely --...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>submit the report ASAP!</td>\n      <td>submit the report as soon as possilbe</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy bday!</td>\n      <td>Happy Birthday!</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The OGs - I like it!!</td>\n      <td>The original gangsters - i like it!</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=df.drop('Meaning',axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:57.661193Z","iopub.execute_input":"2024-08-18T06:11:57.661841Z","iopub.status.idle":"2024-08-18T06:11:57.667071Z","shell.execute_reply.started":"2024-08-18T06:11:57.661808Z","shell.execute_reply":"2024-08-18T06:11:57.666088Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:11:59.511119Z","iopub.execute_input":"2024-08-18T06:11:59.511705Z","iopub.status.idle":"2024-08-18T06:11:59.517418Z","shell.execute_reply.started":"2024-08-18T06:11:59.511675Z","shell.execute_reply":"2024-08-18T06:11:59.516520Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"(4958, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df = df.rename(columns={'Text': 'tweet'})","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:01.433200Z","iopub.execute_input":"2024-08-18T06:12:01.433864Z","iopub.status.idle":"2024-08-18T06:12:01.439154Z","shell.execute_reply.started":"2024-08-18T06:12:01.433829Z","shell.execute_reply":"2024-08-18T06:12:01.437875Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"df = df.rename(columns={'Sentiment': 'class'})","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:05.398072Z","iopub.execute_input":"2024-08-18T06:12:05.398492Z","iopub.status.idle":"2024-08-18T06:12:05.404367Z","shell.execute_reply.started":"2024-08-18T06:12:05.398445Z","shell.execute_reply":"2024-08-18T06:12:05.403338Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"df['class'] = df['class'].str.lower()\nsentiment_counts = df['class'].value_counts()\nsentiment_counts","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:07.708061Z","iopub.execute_input":"2024-08-18T06:12:07.708502Z","iopub.status.idle":"2024-08-18T06:12:07.720926Z","shell.execute_reply.started":"2024-08-18T06:12:07.708453Z","shell.execute_reply":"2024-08-18T06:12:07.719941Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"class\nneutral     1743\npositive    1643\nnegative    1572\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df = df[df['class'] != \"neutral\"]  ###droping neutral columns","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:16.863352Z","iopub.execute_input":"2024-08-18T06:12:16.864222Z","iopub.status.idle":"2024-08-18T06:12:16.871120Z","shell.execute_reply.started":"2024-08-18T06:12:16.864189Z","shell.execute_reply":"2024-08-18T06:12:16.869933Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(df['class'])\nencoded_labels = label_encoder.transform(df['class'])\ndf['class'] = encoded_labels","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:18.891704Z","iopub.execute_input":"2024-08-18T06:12:18.892327Z","iopub.status.idle":"2024-08-18T06:12:18.898915Z","shell.execute_reply.started":"2024-08-18T06:12:18.892295Z","shell.execute_reply":"2024-08-18T06:12:18.897955Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:21.177325Z","iopub.execute_input":"2024-08-18T06:12:21.177703Z","iopub.status.idle":"2024-08-18T06:12:21.189563Z","shell.execute_reply.started":"2024-08-18T06:12:21.177672Z","shell.execute_reply":"2024-08-18T06:12:21.188182Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"                                               tweet  class\n1  Shanghai is also really exciting (precisely --...      1\n2                            submit the report ASAP!      0\n3                                        happy bday!      1\n4                              The OGs - I like it!!      1\n5                    that`s great!! weee!! visitors!      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Shanghai is also really exciting (precisely --...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>submit the report ASAP!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy bday!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The OGs - I like it!!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that`s great!! weee!! visitors!</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:23.255144Z","iopub.execute_input":"2024-08-18T06:12:23.255505Z","iopub.status.idle":"2024-08-18T06:12:23.263757Z","shell.execute_reply.started":"2024-08-18T06:12:23.255462Z","shell.execute_reply":"2024-08-18T06:12:23.262761Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"tweet    1\nclass    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df=df.fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:25.428288Z","iopub.execute_input":"2024-08-18T06:12:25.429163Z","iopub.status.idle":"2024-08-18T06:12:25.434609Z","shell.execute_reply.started":"2024-08-18T06:12:25.429123Z","shell.execute_reply":"2024-08-18T06:12:25.433545Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:27.084415Z","iopub.execute_input":"2024-08-18T06:12:27.085203Z","iopub.status.idle":"2024-08-18T06:12:27.096337Z","shell.execute_reply.started":"2024-08-18T06:12:27.085160Z","shell.execute_reply":"2024-08-18T06:12:27.095323Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"                                               tweet  class\n1  Shanghai is also really exciting (precisely --...      1\n2                            submit the report ASAP!      0\n3                                        happy bday!      1\n4                              The OGs - I like it!!      1\n5                    that`s great!! weee!! visitors!      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Shanghai is also really exciting (precisely --...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>submit the report ASAP!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy bday!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The OGs - I like it!!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that`s great!! weee!! visitors!</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup\n\ndef text_cleaning(text):\n    text = str(text)\n    soup = BeautifulSoup(text, \"html.parser\")    ###removing html tages\n    text = re.sub(r'\\[[^]]*\\]', '', soup.get_text())      ##removing text within square brackets\n    pattern = r\"[^a-zA-Z0-9\\s,']\"                  # Removing unwanted characters\n    text = re.sub(pattern, '', text)\n    url_pattern = r'http\\S+|www\\S+'            ###removing urls\n    text = re.sub(url_pattern, '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:29.367070Z","iopub.execute_input":"2024-08-18T06:12:29.367421Z","iopub.status.idle":"2024-08-18T06:12:29.373405Z","shell.execute_reply.started":"2024-08-18T06:12:29.367391Z","shell.execute_reply":"2024-08-18T06:12:29.372489Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"df['tweet'] = df['tweet'].apply(text_cleaning).tolist()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:31.961358Z","iopub.execute_input":"2024-08-18T06:12:31.961717Z","iopub.status.idle":"2024-08-18T06:12:32.151768Z","shell.execute_reply.started":"2024-08-18T06:12:31.961689Z","shell.execute_reply":"2024-08-18T06:12:32.150769Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2827669698.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(text, \"html.parser\")    ###removing html tages\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:33.823533Z","iopub.execute_input":"2024-08-18T06:12:33.823899Z","iopub.status.idle":"2024-08-18T06:12:33.834373Z","shell.execute_reply.started":"2024-08-18T06:12:33.823869Z","shell.execute_reply":"2024-08-18T06:12:33.833149Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"                                                tweet  class\n1   Shanghai is also really exciting precisely  sk...      1\n2                              submit the report ASAP      0\n3                                          happy bday      1\n4                                  The OGs  I like it      1\n5                           thats great weee visitors      1\n6               I THINK EVERYONE HATES ME ON HERE lol      0\n7   soooooo wish i could, but im in school and mys...      0\n10  My bike was put on holdshould have known that ...      0\n13  Im in VA for the weekend, my youngest son turn...      0\n14  Its coming out the socket I feel like my phone...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Shanghai is also really exciting precisely  sk...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>submit the report ASAP</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy bday</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The OGs  I like it</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>thats great weee visitors</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I THINK EVERYONE HATES ME ON HERE lol</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>soooooo wish i could, but im in school and mys...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>My bike was put on holdshould have known that ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Im in VA for the weekend, my youngest son turn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Its coming out the socket I feel like my phone...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"lens=[len(i.split()) for i in df['tweet']]","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:36.117894Z","iopub.execute_input":"2024-08-18T06:12:36.118515Z","iopub.status.idle":"2024-08-18T06:12:36.127455Z","shell.execute_reply.started":"2024-08-18T06:12:36.118460Z","shell.execute_reply":"2024-08-18T06:12:36.126207Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(lens)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:38.735276Z","iopub.execute_input":"2024-08-18T06:12:38.735655Z","iopub.status.idle":"2024-08-18T06:12:39.018101Z","shell.execute_reply.started":"2024-08-18T06:12:38.735623Z","shell.execute_reply":"2024-08-18T06:12:39.017047Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"(array([132., 493., 643., 585., 430., 438., 237., 174.,  74.,   9.]),\n array([ 0. ,  3.2,  6.4,  9.6, 12.8, 16. , 19.2, 22.4, 25.6, 28.8, 32. ]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:06.746753Z","iopub.execute_input":"2024-08-18T06:13:06.747114Z","iopub.status.idle":"2024-08-18T06:13:06.751748Z","shell.execute_reply.started":"2024-08-18T06:13:06.747086Z","shell.execute_reply":"2024-08-18T06:13:06.750850Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# split into train, validation and test sets in the ration 70 : 15 : 15\nfrom sklearn.model_selection import train_test_split\n\ntrain_text, temp_text, train_labels, temp_labels = train_test_split(df['tweet'], df['class'], \n                                                                    random_state=2021, \n                                                                    test_size=0.2, \n                                                                    stratify=df['class'])\n\n\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2021, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:15.878063Z","iopub.execute_input":"2024-08-18T06:13:15.878500Z","iopub.status.idle":"2024-08-18T06:13:15.893853Z","shell.execute_reply.started":"2024-08-18T06:13:15.878452Z","shell.execute_reply":"2024-08-18T06:13:15.892864Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"bert = AutoModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:19.894400Z","iopub.execute_input":"2024-08-18T06:13:19.895314Z","iopub.status.idle":"2024-08-18T06:13:20.139495Z","shell.execute_reply.started":"2024-08-18T06:13:19.895273Z","shell.execute_reply":"2024-08-18T06:13:20.138529Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:22.631451Z","iopub.execute_input":"2024-08-18T06:13:22.632028Z","iopub.status.idle":"2024-08-18T06:13:22.727993Z","shell.execute_reply.started":"2024-08-18T06:13:22.631999Z","shell.execute_reply":"2024-08-18T06:13:22.727048Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"train_lens=[len(i.split()) for i in train_text]\nplt.hist(train_lens)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:24.857241Z","iopub.execute_input":"2024-08-18T06:13:24.857850Z","iopub.status.idle":"2024-08-18T06:13:25.073585Z","shell.execute_reply.started":"2024-08-18T06:13:24.857818Z","shell.execute_reply":"2024-08-18T06:13:25.072709Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"(array([190., 470., 502., 424., 316., 268., 194., 145.,  55.,   8.]),\n array([ 1. ,  4.1,  7.2, 10.3, 13.4, 16.5, 19.6, 22.7, 25.8, 28.9, 32. ]),\n <BarContainer object of 10 artists>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\nsequence_lengths = df['tweet'].apply(lambda x: len(x.split()))\nprint(sequence_lengths.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:28.063341Z","iopub.execute_input":"2024-08-18T06:13:28.063761Z","iopub.status.idle":"2024-08-18T06:13:28.080690Z","shell.execute_reply.started":"2024-08-18T06:13:28.063728Z","shell.execute_reply":"2024-08-18T06:13:28.079499Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"count    3215.000000\nmean       12.231726\nstd         6.215442\nmin         0.000000\n25%         7.000000\n50%        11.000000\n75%        17.000000\nmax        32.000000\nName: tweet, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"pad_len = 25","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:15:46.942384Z","iopub.execute_input":"2024-08-18T06:15:46.942772Z","iopub.status.idle":"2024-08-18T06:15:46.947541Z","shell.execute_reply.started":"2024-08-18T06:15:46.942742Z","shell.execute_reply":"2024-08-18T06:15:46.946396Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# tokenizing and encoding sequences \ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = pad_len,\n    pad_to_max_length=True,\n    truncation=True\n)\n\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = pad_len,\n    pad_to_max_length=True,\n    truncation=True\n)\n\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = pad_len,\n    pad_to_max_length=True,\n    truncation=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:15:49.148293Z","iopub.execute_input":"2024-08-18T06:15:49.148956Z","iopub.status.idle":"2024-08-18T06:15:49.453671Z","shell.execute_reply.started":"2024-08-18T06:15:49.148920Z","shell.execute_reply":"2024-08-18T06:15:49.452830Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:15:53.438780Z","iopub.execute_input":"2024-08-18T06:15:53.439137Z","iopub.status.idle":"2024-08-18T06:15:53.503093Z","shell.execute_reply.started":"2024-08-18T06:15:53.439109Z","shell.execute_reply":"2024-08-18T06:15:53.502151Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nbatch_size = 64  ##batch size can be adjusted or modified\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\nval_data = TensorDataset(val_seq, val_mask, val_y)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:16:43.436959Z","iopub.execute_input":"2024-08-18T06:16:43.437796Z","iopub.status.idle":"2024-08-18T06:16:43.443744Z","shell.execute_reply.started":"2024-08-18T06:16:43.437759Z","shell.execute_reply":"2024-08-18T06:16:43.442762Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"for param in bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:16:46.129423Z","iopub.execute_input":"2024-08-18T06:16:46.130449Z","iopub.status.idle":"2024-08-18T06:16:46.135781Z","shell.execute_reply.started":"2024-08-18T06:16:46.130401Z","shell.execute_reply":"2024-08-18T06:16:46.134829Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\n##model building\nclass SentimentClassifier(nn.Module):\n     \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    \n    # Forward propagaion\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        ) \n        output = self.drop(pooled_output)  ##drop out layer\n        return self.out(output)\nclass BERT_architecture(nn.Module):\n\n    def __init__(self, bert):\n        super(BERT_architecture, self).__init__()\n\n        self.bert = bert \n        \n        self.dropout = nn.Dropout(0.2) ##needs adjusment\n        self.relu = nn.ReLU() ##activation function\n        \n        self.fc1 = nn.Linear(768, 512) #dense layer\n        \n        # Dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512, 2)  #2 classes\n\n        # Softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)  \n\n    # Defining forward pass\n    def forward(self, sent_id, mask): \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        # Output layer\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:16:48.999853Z","iopub.execute_input":"2024-08-18T06:16:49.000247Z","iopub.status.idle":"2024-08-18T06:16:49.011333Z","shell.execute_reply.started":"2024-08-18T06:16:49.000215Z","shell.execute_reply":"2024-08-18T06:16:49.010538Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"model = BERT_architecture(bert)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:16:53.021999Z","iopub.execute_input":"2024-08-18T06:16:53.022351Z","iopub.status.idle":"2024-08-18T06:16:53.034672Z","shell.execute_reply.started":"2024-08-18T06:16:53.022323Z","shell.execute_reply":"2024-08-18T06:16:53.033906Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"test_seq = test_seq.to(device)\ntest_mask = test_mask.to(device)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:16:54.921457Z","iopub.execute_input":"2024-08-18T06:16:54.922368Z","iopub.status.idle":"2024-08-18T06:16:55.230849Z","shell.execute_reply.started":"2024-08-18T06:16:54.922326Z","shell.execute_reply":"2024-08-18T06:16:55.229726Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"BERT_architecture(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.2, inplace=False)\n  (relu): ReLU()\n  (fc1): Linear(in_features=768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=2, bias=True)\n  (softmax): LogSoftmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AdamW\noptimizer = AdamW(model.parameters(),lr = 1e-3)  ## learning rate needs to be adjusted.","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:16:59.426920Z","iopub.execute_input":"2024-08-18T06:16:59.427276Z","iopub.status.idle":"2024-08-18T06:16:59.446852Z","shell.execute_reply.started":"2024-08-18T06:16:59.427246Z","shell.execute_reply":"2024-08-18T06:16:59.445887Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport torch\nimport torch.nn as nn\n\n# Compute the class weights for two classes\nclass_weights = compute_class_weight(class_weight=\"balanced\",\n                                     classes=np.unique(train_labels),\n                                     y=train_labels)\nprint(\"Class weights are {} for {}\".format(class_weights, np.unique(train_labels)))","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:17:02.032796Z","iopub.execute_input":"2024-08-18T06:17:02.033153Z","iopub.status.idle":"2024-08-18T06:17:02.042360Z","shell.execute_reply.started":"2024-08-18T06:17:02.033124Z","shell.execute_reply":"2024-08-18T06:17:02.041149Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"Class weights are [1.02225755 0.97869102] for [0 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.value_counts(train_labels)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:17:04.261612Z","iopub.execute_input":"2024-08-18T06:17:04.262031Z","iopub.status.idle":"2024-08-18T06:17:04.271856Z","shell.execute_reply.started":"2024-08-18T06:17:04.262004Z","shell.execute_reply":"2024-08-18T06:17:04.270680Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/883685092.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n  pd.value_counts(train_labels)\n","output_type":"stream"},{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"class\n1    1314\n0    1258\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"weights = torch.tensor(class_weights, dtype=torch.float)\n\nweights = weights.to(device)\ncross_entropy = nn.CrossEntropyLoss()\n\nepochs = 50 ##increase the no of epochs for better accuracy \n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:17:07.361009Z","iopub.execute_input":"2024-08-18T06:17:07.361393Z","iopub.status.idle":"2024-08-18T06:17:07.368622Z","shell.execute_reply.started":"2024-08-18T06:17:07.361363Z","shell.execute_reply":"2024-08-18T06:17:07.367743Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def train():\n    model.train()\n    total_loss, total_accuracy = 0, 0\n    total_preds, total_labels = [], []\n\n    for step, batch in enumerate(train_dataloader):\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n        batch = [r.to(device) for r in batch]\n        sent_id, mask, labels = batch\n        model.zero_grad()\n        preds = model(sent_id, mask)\n        loss = cross_entropy(preds, labels)\n        total_loss = total_loss + loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        preds = preds.detach().cpu().numpy()\n        total_preds.append(preds)\n        total_labels.append(labels.detach().cpu().numpy())\n\n    avg_loss = total_loss / len(train_dataloader)\n    total_preds = np.concatenate(total_preds, axis=0)\n    total_labels = np.concatenate(total_labels, axis=0)\n    return avg_loss, total_preds, total_labels\n\ndef evaluate():\n    print(\"\\nEvaluating after the test: \")\n    model.eval()\n    total_loss, total_accuracy = 0, 0\n    total_preds, total_labels = [], []\n\n    for step, batch in enumerate(val_dataloader):\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        batch = [t.to(device) for t in batch]\n        sent_id, mask, labels = batch\n\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            loss = cross_entropy(preds, labels)\n            total_loss = total_loss + loss.item()\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n            total_labels.append(labels.detach().cpu().numpy())\n\n    avg_loss = total_loss / len(val_dataloader)\n    total_preds = np.concatenate(total_preds, axis=0)\n    total_labels = np.concatenate(total_labels, axis=0)\n    return avg_loss, total_preds, total_labels\ndef calculate_accuracy(preds, labels):\n\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    accuracy = np.sum(preds_flat == labels_flat) / len(labels_flat)\n    return accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:17:10.414900Z","iopub.execute_input":"2024-08-18T06:17:10.415253Z","iopub.status.idle":"2024-08-18T06:17:10.432387Z","shell.execute_reply.started":"2024-08-18T06:17:10.415225Z","shell.execute_reply":"2024-08-18T06:17:10.431195Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"best_valid_loss = float('inf')\ntrain_losses = []\nvalid_losses = []\ntrain_accuracies = []\nvalid_accuracies = []\n\nfor epoch in range(epochs):\n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    train_loss, train_preds, train_labels = train()\n    valid_loss, valid_preds, valid_labels = evaluate()\n    train_accuracy = calculate_accuracy(train_preds, train_labels)\n    valid_accuracy = calculate_accuracy(valid_preds, valid_labels)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accuracies.append(train_accuracy)\n    valid_accuracies.append(valid_accuracy)\n    \n    print('\\nTraining Loss: {}'.format(train_loss))\n    print('Validation Loss: {}'.format(valid_loss))\n    print('Training Accuracy: {:.2f}%'.format(train_accuracy * 100))\n    print('Validation Accuracy: {:.2f}%'.format(valid_accuracy * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:17:24.827960Z","iopub.execute_input":"2024-08-18T06:17:24.828586Z","iopub.status.idle":"2024-08-18T06:21:12.896100Z","shell.execute_reply.started":"2024-08-18T06:17:24.828552Z","shell.execute_reply":"2024-08-18T06:21:12.895163Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"\n Epoch 1 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.7003437149815444\nValidation Loss: 0.6739660600821177\nTraining Accuracy: 54.39%\nValidation Accuracy: 58.57%\n\n Epoch 2 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.6417871059440985\nValidation Loss: 0.6448143720626831\nTraining Accuracy: 63.22%\nValidation Accuracy: 62.31%\n\n Epoch 3 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.6044809745579232\nValidation Loss: 0.5178181727727255\nTraining Accuracy: 68.04%\nValidation Accuracy: 73.21%\n\n Epoch 4 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5920353008479606\nValidation Loss: 0.5160391281048456\nTraining Accuracy: 68.08%\nValidation Accuracy: 73.21%\n\n Epoch 5 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5692223041522794\nValidation Loss: 0.5262759625911713\nTraining Accuracy: 71.42%\nValidation Accuracy: 71.96%\n\n Epoch 6 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5791351715239083\nValidation Loss: 0.5234638924400011\nTraining Accuracy: 69.56%\nValidation Accuracy: 66.36%\n\n Epoch 7 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5558932300021009\nValidation Loss: 0.49765467022856075\nTraining Accuracy: 71.81%\nValidation Accuracy: 70.72%\n\n Epoch 8 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5341397022328725\nValidation Loss: 0.4684268608689308\nTraining Accuracy: 73.52%\nValidation Accuracy: 76.01%\n\n Epoch 9 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5385527421788472\nValidation Loss: 0.45305442810058594\nTraining Accuracy: 72.40%\nValidation Accuracy: 78.50%\n\n Epoch 10 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5375926937998795\nValidation Loss: 0.46583059678475064\nTraining Accuracy: 73.06%\nValidation Accuracy: 74.45%\n\n Epoch 11 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5378546169618281\nValidation Loss: 0.4528324802716573\nTraining Accuracy: 73.17%\nValidation Accuracy: 77.26%\n\n Epoch 12 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5348041479180499\nValidation Loss: 0.4679103270173073\nTraining Accuracy: 72.71%\nValidation Accuracy: 73.83%\n\n Epoch 13 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5498634504108895\nValidation Loss: 0.4568720708290736\nTraining Accuracy: 71.66%\nValidation Accuracy: 78.50%\n\n Epoch 14 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5245833193383566\nValidation Loss: 0.508253779883186\nTraining Accuracy: 74.38%\nValidation Accuracy: 68.22%\n\n Epoch 15 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5378086486967598\nValidation Loss: 0.469437117377917\nTraining Accuracy: 72.16%\nValidation Accuracy: 76.64%\n\n Epoch 16 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5269744134530788\nValidation Loss: 0.43246353417634964\nTraining Accuracy: 73.95%\nValidation Accuracy: 79.75%\n\n Epoch 17 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4978836651255445\nValidation Loss: 0.45860403776168823\nTraining Accuracy: 76.40%\nValidation Accuracy: 75.70%\n\n Epoch 18 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5171074300277524\nValidation Loss: 0.45632683485746384\nTraining Accuracy: 74.18%\nValidation Accuracy: 74.45%\n\n Epoch 19 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5115585036394072\nValidation Loss: 0.47924312949180603\nTraining Accuracy: 74.30%\nValidation Accuracy: 72.90%\n\n Epoch 20 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5353802311711195\nValidation Loss: 0.44377172489960987\nTraining Accuracy: 74.34%\nValidation Accuracy: 77.26%\n\n Epoch 21 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5225073564343337\nValidation Loss: 0.5019421974817911\nTraining Accuracy: 73.87%\nValidation Accuracy: 72.90%\n\n Epoch 22 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5004378382752581\nValidation Loss: 0.4919462203979492\nTraining Accuracy: 75.00%\nValidation Accuracy: 75.08%\n\n Epoch 23 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4981224769499244\nValidation Loss: 0.43945768972237903\nTraining Accuracy: 76.79%\nValidation Accuracy: 77.57%\n\n Epoch 24 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5088076191704448\nValidation Loss: 0.4352020223935445\nTraining Accuracy: 74.88%\nValidation Accuracy: 78.19%\n\n Epoch 25 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4747827478298327\nValidation Loss: 0.5248923649390539\nTraining Accuracy: 77.68%\nValidation Accuracy: 70.40%\n\n Epoch 26 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5171063171654213\nValidation Loss: 0.43562233944733936\nTraining Accuracy: 74.81%\nValidation Accuracy: 78.50%\n\n Epoch 27 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5030673633261424\nValidation Loss: 0.6800976792971293\nTraining Accuracy: 75.66%\nValidation Accuracy: 64.17%\n\n Epoch 28 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4985543147819798\nValidation Loss: 0.4164273999631405\nTraining Accuracy: 76.28%\nValidation Accuracy: 78.50%\n\n Epoch 29 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.49505328405194166\nValidation Loss: 0.5005609293778738\nTraining Accuracy: 76.48%\nValidation Accuracy: 71.65%\n\n Epoch 30 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4941301098683985\nValidation Loss: 0.41318927705287933\nTraining Accuracy: 76.59%\nValidation Accuracy: 78.82%\n\n Epoch 31 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5296399658773003\nValidation Loss: 0.4292522706091404\nTraining Accuracy: 73.87%\nValidation Accuracy: 77.88%\n\n Epoch 32 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.493723724673434\nValidation Loss: 0.4163267140587171\nTraining Accuracy: 76.63%\nValidation Accuracy: 80.06%\n\n Epoch 33 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4854268598847273\nValidation Loss: 0.4300309469302495\nTraining Accuracy: 76.59%\nValidation Accuracy: 78.19%\n\n Epoch 34 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.49729760790743477\nValidation Loss: 0.5418036133050919\nTraining Accuracy: 75.82%\nValidation Accuracy: 69.78%\n\n Epoch 35 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.480495678942378\nValidation Loss: 0.40893928458293277\nTraining Accuracy: 77.49%\nValidation Accuracy: 79.13%\n\n Epoch 36 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4756331581894944\nValidation Loss: 0.4132826253771782\nTraining Accuracy: 77.06%\nValidation Accuracy: 79.13%\n\n Epoch 37 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.48865047169894704\nValidation Loss: 0.43851719548304874\nTraining Accuracy: 76.83%\nValidation Accuracy: 78.19%\n\n Epoch 38 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.49952823795923373\nValidation Loss: 0.4058397610982259\nTraining Accuracy: 76.05%\nValidation Accuracy: 79.44%\n\n Epoch 39 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.48376514780812147\nValidation Loss: 0.4220850244164467\nTraining Accuracy: 77.68%\nValidation Accuracy: 79.13%\n\n Epoch 40 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4822501525646303\nValidation Loss: 0.43570824960867566\nTraining Accuracy: 76.05%\nValidation Accuracy: 76.64%\n\n Epoch 41 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4807001679408841\nValidation Loss: 0.4904633288582166\nTraining Accuracy: 77.68%\nValidation Accuracy: 70.40%\n\n Epoch 42 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5004452096252907\nValidation Loss: 0.4107462465763092\nTraining Accuracy: 75.93%\nValidation Accuracy: 80.69%\n\n Epoch 43 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4789603788678239\nValidation Loss: 0.44074010848999023\nTraining Accuracy: 77.45%\nValidation Accuracy: 76.64%\n\n Epoch 44 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.49218278905240503\nValidation Loss: 0.4691962202390035\nTraining Accuracy: 76.09%\nValidation Accuracy: 74.77%\n\n Epoch 45 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.49775809994558007\nValidation Loss: 0.42867840081453323\nTraining Accuracy: 74.46%\nValidation Accuracy: 79.75%\n\n Epoch 46 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.5156504591790642\nValidation Loss: 0.4442758485674858\nTraining Accuracy: 75.19%\nValidation Accuracy: 77.57%\n\n Epoch 47 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.49155116662746523\nValidation Loss: 0.40596579511960346\nTraining Accuracy: 75.62%\nValidation Accuracy: 80.69%\n\n Epoch 48 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.48843632020601413\nValidation Loss: 0.45179719229539234\nTraining Accuracy: 75.89%\nValidation Accuracy: 76.95%\n\n Epoch 49 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.4791688628313018\nValidation Loss: 0.4317593326171239\nTraining Accuracy: 77.02%\nValidation Accuracy: 77.57%\n\n Epoch 50 / 50\n\nEvaluating after the test: \n\nTraining Loss: 0.48270031737118235\nValidation Loss: 0.4202681630849838\nTraining Accuracy: 76.98%\nValidation Accuracy: 79.13%\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()\npred = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, pred))","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:24:17.946632Z","iopub.execute_input":"2024-08-18T06:24:17.947024Z","iopub.status.idle":"2024-08-18T06:24:18.445180Z","shell.execute_reply.started":"2024-08-18T06:24:17.946996Z","shell.execute_reply":"2024-08-18T06:24:18.443889Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.73      0.85      0.79       157\n           1       0.83      0.70      0.76       165\n\n    accuracy                           0.77       322\n   macro avg       0.78      0.78      0.77       322\nweighted avg       0.78      0.77      0.77       322\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_user_input(input_text, model, tokenizer, device):\n    inputs = tokenizer.encode_plus(\n        input_text,\n        add_special_tokens=True,\n        max_length=max_seq_len,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask)\n        preds = torch.argmax(outputs, dim=1)\n    \n    return preds.item()\nmax_seq_len=25\ninput_text = \"I been told you that yo folks dont care about you . Im starving and my dad deadass didnt make me a plate\"\npredicted_class = predict_user_input(input_text, model, tokenizer, device)\nif(predicted_class==0):\n    print('Predicted sentiment: Negative')\nelse:\n    print('Predicted sentiment: Positive')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:24:51.884916Z","iopub.execute_input":"2024-08-18T06:24:51.885774Z","iopub.status.idle":"2024-08-18T06:24:51.925043Z","shell.execute_reply.started":"2024-08-18T06:24:51.885723Z","shell.execute_reply":"2024-08-18T06:24:51.923302Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Predicted sentiment: Negative\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]}]}