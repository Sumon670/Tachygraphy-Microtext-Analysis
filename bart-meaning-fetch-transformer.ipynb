{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9420547,"sourceType":"datasetVersion","datasetId":5721840}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-17T15:19:11.999588Z","iopub.execute_input":"2024-09-17T15:19:12.000481Z","iopub.status.idle":"2024-09-17T15:19:12.015724Z","shell.execute_reply.started":"2024-09-17T15:19:12.000438Z","shell.execute_reply":"2024-09-17T15:19:12.014781Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"/kaggle/input/informal-text-dataset/tachygraphy_dataset_v2.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"import emoji\nfrom bs4 import BeautifulSoup\nimport os\nimport re\nimport string\nimport json\n\n'''For emoji cleaning'''","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.017135Z","iopub.execute_input":"2024-09-17T15:19:12.017427Z","iopub.status.idle":"2024-09-17T15:19:12.024060Z","shell.execute_reply.started":"2024-09-17T15:19:12.017390Z","shell.execute_reply":"2024-09-17T15:19:12.022946Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'For emoji cleaning'"},"metadata":{}}]},{"cell_type":"code","source":"dataset = pd.read_excel('/kaggle/input/informal-text-dataset/tachygraphy_dataset_v2.xlsx')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.025405Z","iopub.execute_input":"2024-09-17T15:19:12.026109Z","iopub.status.idle":"2024-09-17T15:19:12.915616Z","shell.execute_reply.started":"2024-09-17T15:19:12.026077Z","shell.execute_reply":"2024-09-17T15:19:12.914698Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df=dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.918140Z","iopub.execute_input":"2024-09-17T15:19:12.918516Z","iopub.status.idle":"2024-09-17T15:19:12.923109Z","shell.execute_reply.started":"2024-09-17T15:19:12.918477Z","shell.execute_reply":"2024-09-17T15:19:12.922058Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n\npunct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n                'demonetisation': 'demonetization'}","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.925072Z","iopub.execute_input":"2024-09-17T15:19:12.925582Z","iopub.status.idle":"2024-09-17T15:19:12.954037Z","shell.execute_reply.started":"2024-09-17T15:19:12.925530Z","shell.execute_reply":"2024-09-17T15:19:12.953137Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = emoji.demojize(text)\n    text = re.sub(r'\\:(.*?)\\:','',text)\n    text = str(text).lower()    #Making Text Lowercase\n    text = re.sub('\\[.*?\\]', '', text)\n    #The next 2 lines remove html text\n    text = BeautifulSoup(text, 'lxml').get_text()\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n    return text\n\ndef clean_contractions(text, mapping):\n    '''Clean contraction using contraction mapping'''    \n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    for word in mapping.keys():\n        if \"\"+word+\"\" in text:\n            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n    #Remove Punctuations\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    return text\n\ndef clean_special_chars(text, punct, mapping):\n    '''Cleans special characters present(if any)'''   \n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text\n\ndef correct_spelling(x, dic):\n    '''Corrects common spelling errors'''   \n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndef remove_space(text):\n    '''Removes awkward spaces'''   \n    #Removes awkward spaces \n    text = text.strip()\n    text = text.split()\n    return \" \".join(text)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.955435Z","iopub.execute_input":"2024-09-17T15:19:12.955783Z","iopub.status.idle":"2024-09-17T15:19:12.969612Z","shell.execute_reply.started":"2024-09-17T15:19:12.955719Z","shell.execute_reply":"2024-09-17T15:19:12.968680Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing_pipeline(text):\n    '''Cleaning and parsing the text.'''\n    text = clean_contractions(text, contraction_mapping)\n    text = clean_text(text)\n    text = clean_contractions(text, contraction_mapping)\n    text = clean_special_chars(text, punct, punct_mapping)\n#     text = correct_spelling(text, mispell_dict)\n    text = remove_space(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.970901Z","iopub.execute_input":"2024-09-17T15:19:12.971217Z","iopub.status.idle":"2024-09-17T15:19:12.981129Z","shell.execute_reply.started":"2024-09-17T15:19:12.971187Z","shell.execute_reply":"2024-09-17T15:19:12.980399Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df['Expanded Meaning']","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.982171Z","iopub.execute_input":"2024-09-17T15:19:12.982474Z","iopub.status.idle":"2024-09-17T15:19:12.996109Z","shell.execute_reply.started":"2024-09-17T15:19:12.982444Z","shell.execute_reply":"2024-09-17T15:19:12.994928Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0        Oh my god, Joint Entrance Examination preparat...\n1        Are you up for a break before Union Public Ser...\n2        Talk to you later, finishing the Common Admiss...\n3        No, that Graduate Aptitude Test in Engineering...\n4        What's up? Are you done with your Indian Insti...\n                               ...                        \n10275    Brother, power cuts in Uttar Pradesh are ongoi...\n10276    What the fuck, new tax laws are increasing, ca...\n10277    Yo, Adani’s stock is dropping, with more probl...\n10278    Brother, new technology IPOs are performing we...\n10279    What the fuck, Delhi’s air quality index has h...\nName: Expanded Meaning, Length: 10280, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:12.997265Z","iopub.execute_input":"2024-09-17T15:19:12.997599Z","iopub.status.idle":"2024-09-17T15:19:13.008771Z","shell.execute_reply.started":"2024-09-17T15:19:12.997563Z","shell.execute_reply":"2024-09-17T15:19:13.007923Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Informal Text       2\nExpanded Meaning    2\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df=df.fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:13.012419Z","iopub.execute_input":"2024-09-17T15:19:13.012793Z","iopub.status.idle":"2024-09-17T15:19:13.020395Z","shell.execute_reply.started":"2024-09-17T15:19:13.012749Z","shell.execute_reply":"2024-09-17T15:19:13.019548Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df['Expanded Meaning'] = df['Expanded Meaning'].astype(str)\ndf['Expanded Meaning'] = df['Expanded Meaning'].apply(lambda x: text_preprocessing_pipeline(x))","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:13.021470Z","iopub.execute_input":"2024-09-17T15:19:13.021823Z","iopub.status.idle":"2024-09-17T15:19:18.108900Z","shell.execute_reply.started":"2024-09-17T15:19:13.021790Z","shell.execute_reply":"2024-09-17T15:19:18.108015Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:18.110032Z","iopub.execute_input":"2024-09-17T15:19:18.110346Z","iopub.status.idle":"2024-09-17T15:19:18.119827Z","shell.execute_reply.started":"2024-09-17T15:19:18.110312Z","shell.execute_reply":"2024-09-17T15:19:18.118808Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                                           Informal Text  \\\n10275        bruh, UP’s power cuts, no relief in sight 🔌   \n10276  wtf, new tax laws hitting hard, prices gonna j...   \n10277  yo, Adani’s stock falling, more issues surfaci...   \n10278    bruh, tech IPOs doing well, investors excited 📉   \n10279  wtf, Delhi’s AQI at record highs, air quality ...   \n\n                                        Expanded Meaning  \n10275  brother power cuts in uttar pradesh are ongoin...  \n10276  what the fuck new tax laws are increasing caus...  \n10277  yo adanis stock is dropping with more problems...  \n10278  brother new technology ipos are performing wel...  \n10279  what the fuck delhis air quality index has hit...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Informal Text</th>\n      <th>Expanded Meaning</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10275</th>\n      <td>bruh, UP’s power cuts, no relief in sight 🔌</td>\n      <td>brother power cuts in uttar pradesh are ongoin...</td>\n    </tr>\n    <tr>\n      <th>10276</th>\n      <td>wtf, new tax laws hitting hard, prices gonna j...</td>\n      <td>what the fuck new tax laws are increasing caus...</td>\n    </tr>\n    <tr>\n      <th>10277</th>\n      <td>yo, Adani’s stock falling, more issues surfaci...</td>\n      <td>yo adanis stock is dropping with more problems...</td>\n    </tr>\n    <tr>\n      <th>10278</th>\n      <td>bruh, tech IPOs doing well, investors excited 📉</td>\n      <td>brother new technology ipos are performing wel...</td>\n    </tr>\n    <tr>\n      <th>10279</th>\n      <td>wtf, Delhi’s AQI at record highs, air quality ...</td>\n      <td>what the fuck delhis air quality index has hit...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset = pd.DataFrame(df)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:18.121355Z","iopub.execute_input":"2024-09-17T15:19:18.121761Z","iopub.status.idle":"2024-09-17T15:19:18.128122Z","shell.execute_reply.started":"2024-09-17T15:19:18.121699Z","shell.execute_reply":"2024-09-17T15:19:18.127250Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\n\n# Load a sentiment analysis pipeline with a model that supports multiclass sentiment (e.g., 5-class)\nclassifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n\n# Function to map sentiment to three categories: Positive, Neutral, Negative\ndef map_sentiment(score):\n    if score in [4, 5]:\n        return 'positive'\n    elif score == 3:\n        return 'neutral'\n    else:\n        return 'negative'\n\n# Classify each text and map the scores to the three categories\ndataset['Polarity'] = dataset['Expanded Meaning'].apply(lambda x: map_sentiment(int(classifier(x)[0]['label'][0])))","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:19:18.129337Z","iopub.execute_input":"2024-09-17T15:19:18.129997Z","iopub.status.idle":"2024-09-17T15:29:29.153495Z","shell.execute_reply.started":"2024-09-17T15:19:18.129950Z","shell.execute_reply":"2024-09-17T15:29:29.152394Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the distribution of polarities\nprint(\"Polarity Distribution Before Balancing:\")\nprint(dataset['Polarity'].value_counts())\n\n# Polarity Leveling (Balancing)\n# You can choose oversampling or undersampling based on the dataset's imbalance\nmax_samples = dataset['Polarity'].value_counts().max()\n\n# Oversample the minority classes to balance the dataset\nbalanced_dataset = pd.concat([\n    dataset[dataset['Polarity'] == 'positive'],\n    dataset[dataset['Polarity'] == 'neutral'].sample(max_samples, replace=True),\n    dataset[dataset['Polarity'] == 'negative'].sample(max_samples, replace=True)\n])\n\nprint(\"\\nPolarity Distribution After Balancing:\")\nprint(balanced_dataset['Polarity'].value_counts())\n\n# Display the balanced dataset\nprint(\"\\nBalanced Dataset:\")\nprint(balanced_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:29:29.154848Z","iopub.execute_input":"2024-09-17T15:29:29.155166Z","iopub.status.idle":"2024-09-17T15:29:29.187018Z","shell.execute_reply.started":"2024-09-17T15:29:29.155133Z","shell.execute_reply":"2024-09-17T15:29:29.186193Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Polarity Distribution Before Balancing:\nPolarity\nnegative    5252\npositive    3657\nneutral     1371\nName: count, dtype: int64\n\nPolarity Distribution After Balancing:\nPolarity\nneutral     5252\nnegative    5252\npositive    3657\nName: count, dtype: int64\n\nBalanced Dataset:\n                                          Informal Text  \\\n0                        omg, JEE prep is killing me rn   \n2                           ttyl, finishing da CAT mock   \n4                     sup? u done w/ ur IIT assignment?   \n5                       fyi, da UPSC date changed again   \n7                     gtg, hv to prep 4 da JEE mock tmr   \n...                                                 ...   \n6838   no u dont cause u will wake up at 9am for sure .   \n4840                                   I lafff the rain   \n2452           bruh, da Crab Nebula is cray af up close   \n4008  no. my school will start on June1. two days to...   \n3199  evryone come here http://www.blogtv.com/People...   \n\n                                       Expanded Meaning  Polarity  \n0     oh my god joint entrance examination preparati...  positive  \n2     talk to you later finishing the common admissi...  positive  \n4     whats up are you done with your indian institu...  positive  \n5     for your information the union public service ...  positive  \n7     got to go have to prepare for the joint entran...  positive  \n...                                                 ...       ...  \n6838  no you do not because you will wake up at for ...  negative  \n4840                                   i lafff the rain  negative  \n2452     bruh the crab nebula is crazy as fuck up close  negative  \n4008  no my school will start on two days to go i st...  negative  \n3199  evryone come here were spamming so much and bl...  negative  \n\n[14161 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset.to_csv('polarity_leveled_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-17T15:29:29.188290Z","iopub.execute_input":"2024-09-17T15:29:29.188764Z","iopub.status.idle":"2024-09-17T15:29:29.272531Z","shell.execute_reply.started":"2024-09-17T15:29:29.188684Z","shell.execute_reply":"2024-09-17T15:29:29.271468Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\n\n# Load the pretrained BART model and tokenizer\nmodel_name = 'facebook/bart-base'  # You can also use 'bart-large' for better performance\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T16:04:48.695706Z","iopub.execute_input":"2024-09-17T16:04:48.696674Z","iopub.status.idle":"2024-09-17T16:04:54.061173Z","shell.execute_reply.started":"2024-09-17T16:04:48.696628Z","shell.execute_reply":"2024-09-17T16:04:54.059991Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d438d76d26d4d97b89221c8a78c2d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3447707ca68b4fe58912614261d5cbcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a8ab4f464f6422dabf10e8ef0f0715b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f083dd44e9d94531a22b2f365be422a2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea9d9ad630649dfa4680d243e4fa23d"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\ndataset = Dataset.from_pandas(dataset)\n\n# Tokenization function\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples['Informal Text'], max_length=128, truncation=True, padding='max_length')\n    \n    # Tokenize the target (expanded meaning)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples['Expanded Meaning'], max_length=128, truncation=True, padding='max_length')\n\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\n# Apply tokenization\ntokenized_dataset = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T16:07:06.638009Z","iopub.execute_input":"2024-09-17T16:07:06.638997Z","iopub.status.idle":"2024-09-17T16:07:14.323674Z","shell.execute_reply.started":"2024-09-17T16:07:06.638944Z","shell.execute_reply":"2024-09-17T16:07:14.322700Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10280 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b477c777954442f0b6e192c93062f2c6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define training arguments without evaluation strategy\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_steps=500,\n    evaluation_strategy='no',  # No evaluation during training\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,  # Only training dataset\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-17T16:18:29.273535Z","iopub.execute_input":"2024-09-17T16:18:29.274376Z","iopub.status.idle":"2024-09-17T16:38:13.373222Z","shell.execute_reply.started":"2024-09-17T16:18:29.274333Z","shell.execute_reply":"2024-09-17T16:38:13.372424Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3855' max='3855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3855/3855 19:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.073700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.078800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.067100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.070400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.068300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.058900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.037700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.047600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.044300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.040200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.044100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.050300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.031000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.042600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.033100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.050400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.022800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.034900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.036100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.049200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.041300</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.053800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.045600</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.043700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.034100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.029000</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.035200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.025800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.031300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.034500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.034200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.043900</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.029100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.037700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.028100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.033800</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.031800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.032500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.029900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.035900</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.044800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.040100</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.037700</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.030700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.030100</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.035500</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.034800</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.025500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.036500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.039600</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.039100</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.026500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.036800</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.026000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.038600</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.038300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.035900</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.034800</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.030500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.046300</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.030400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.034400</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.032800</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.032200</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.030700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.034000</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.031100</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.033500</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.033400</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.037200</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.052100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.039400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.040100</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.032700</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.047900</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.035700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.037400</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.034700</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.022000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.041000</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.028000</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.039500</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.038700</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.035300</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.024400</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.030100</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.036500</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.028800</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.033800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.027700</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.027600</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.045900</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.049600</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.048600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.045200</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.045200</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.043600</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.040800</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.045200</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.036600</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.045300</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.036000</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.047300</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.034400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.039600</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.042000</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.033200</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.043600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.050200</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.041000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.041400</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.048400</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.037900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.046500</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.044600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.043500</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.045800</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.038900</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.041200</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.028600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.048400</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.040200</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.040900</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.030700</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.037900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.046200</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.036400</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.052700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.040100</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.038200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.028700</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.042600</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.037900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.047000</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.038900</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.034400</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.039600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.035800</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.034000</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.031500</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.049700</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.042800</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.049200</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.039900</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.029900</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.034700</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.040600</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.043200</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.028900</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.050200</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.044900</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.034600</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.042600</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.030500</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.032900</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.039500</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.044400</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.033200</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.042100</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.032700</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.038600</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.050700</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.044200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.046500</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.035600</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.029600</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.034800</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.044400</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.040200</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.050800</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.038200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.038100</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.032800</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.043600</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.033600</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.035000</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.031600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.040600</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.028400</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.033000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.038500</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.044600</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.028300</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.033900</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.043100</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.039100</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.036400</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.038300</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.033000</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.041500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.037000</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.035700</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.034100</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.047300</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.031300</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.034000</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.033300</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.042700</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.033900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.045500</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.029500</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.040600</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.039400</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.040300</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.028700</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.035700</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.018900</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.022800</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.025400</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.028900</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.029400</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.033000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.022200</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.032300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.027800</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.021200</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.024700</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.017100</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.027100</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.025600</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.025100</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.025300</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.028100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.025300</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.023800</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.030600</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.027500</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.024900</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.025800</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.021900</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.027900</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.022800</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.024700</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.032500</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.017600</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.027500</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.026300</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.022200</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.022100</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.017400</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.022300</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.026400</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.021500</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.016700</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.021800</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.026600</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.028900</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.018500</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.022400</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.027200</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.023700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3855, training_loss=0.03291883580531592, metrics={'train_runtime': 1183.5149, 'train_samples_per_second': 26.058, 'train_steps_per_second': 3.257, 'total_flos': 2350534046515200.0, 'train_loss': 0.03291883580531592, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# key-wandb.ai: d8b8084404747be261e7c5db146361882b71e7e6","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef generate_expanded_meaning(informal_text):\n    # Check if a GPU is available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Move the model to the device (GPU or CPU)\n    model.to(device)\n    \n    # Tokenize the input informal text\n    inputs = tokenizer([informal_text], return_tensors='pt', max_length=128, truncation=True)\n    \n    # Move the inputs to the same device as the model\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Generate the output (expanded meaning)\n    output_ids = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n\n    # Decode the generated output\n    expanded_meaning = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    return expanded_meaning\n\n# Example usage\ninformal_text = \"do it ASAP!\"\nexpanded_meaning = generate_expanded_meaning(informal_text)\nprint(\"Expanded Meaning:\", expanded_meaning)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-17T16:50:18.828978Z","iopub.execute_input":"2024-09-17T16:50:18.829322Z","iopub.status.idle":"2024-09-17T16:50:18.938461Z","shell.execute_reply.started":"2024-09-17T16:50:18.829290Z","shell.execute_reply":"2024-09-17T16:50:18.937411Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Expanded Meaning: do it as soon as possible\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}