{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a4b825",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:29.139996Z",
     "iopub.status.busy": "2024-11-29T13:48:29.139712Z",
     "iopub.status.idle": "2024-11-29T13:48:29.837892Z",
     "shell.execute_reply": "2024-11-29T13:48:29.837021Z"
    },
    "papermill": {
     "duration": 0.722279,
     "end_time": "2024-11-29T13:48:29.839731",
     "exception": false,
     "start_time": "2024-11-29T13:48:29.117452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_dataset_main.csv\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_MicroText-AIO-V2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f82df",
   "metadata": {
    "papermill": {
     "duration": 0.019095,
     "end_time": "2024-11-29T13:48:29.879420",
     "exception": false,
     "start_time": "2024-11-29T13:48:29.860325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6064a50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:29.918863Z",
     "iopub.status.busy": "2024-11-29T13:48:29.918489Z",
     "iopub.status.idle": "2024-11-29T13:48:30.107996Z",
     "shell.execute_reply": "2024-11-29T13:48:30.107175Z"
    },
    "papermill": {
     "duration": 0.211368,
     "end_time": "2024-11-29T13:48:30.109752",
     "exception": false,
     "start_time": "2024-11-29T13:48:29.898384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For emoji cleaning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "'''For emoji cleaning'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10b210c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.149776Z",
     "iopub.status.busy": "2024-11-29T13:48:30.149401Z",
     "iopub.status.idle": "2024-11-29T13:48:30.194614Z",
     "shell.execute_reply": "2024-11-29T13:48:30.194048Z"
    },
    "papermill": {
     "duration": 0.066925,
     "end_time": "2024-11-29T13:48:30.196306",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.129381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7db0f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.236415Z",
     "iopub.status.busy": "2024-11-29T13:48:30.236178Z",
     "iopub.status.idle": "2024-11-29T13:48:30.239532Z",
     "shell.execute_reply": "2024-11-29T13:48:30.238876Z"
    },
    "papermill": {
     "duration": 0.024831,
     "end_time": "2024-11-29T13:48:30.241091",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.216260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a800713c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.282606Z",
     "iopub.status.busy": "2024-11-29T13:48:30.282365Z",
     "iopub.status.idle": "2024-11-29T13:48:30.303207Z",
     "shell.execute_reply": "2024-11-29T13:48:30.302553Z"
    },
    "papermill": {
     "duration": 0.043373,
     "end_time": "2024-11-29T13:48:30.304748",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.261375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "#     text = emoji.demojize(text)\n",
    "#     text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "#     text = str(text).lower()    #Making Text Lowercase\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, 0-9, \"%\", \".\", \"&\", \",\", \"'\", \"?\", \"!\", \",\", \"'\", \";\", \"-\")\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!,¿'%&,';-]+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "#     text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_text(text)\n",
    "#     text = clean_contractions(text, contraction_mapping)\n",
    "#     text = clean_special_chars(text, punct, punct_mapping)\n",
    "#     text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbed35ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.345532Z",
     "iopub.status.busy": "2024-11-29T13:48:30.345310Z",
     "iopub.status.idle": "2024-11-29T13:48:30.356587Z",
     "shell.execute_reply": "2024-11-29T13:48:30.355752Z"
    },
    "papermill": {
     "duration": 0.033504,
     "end_time": "2024-11-29T13:48:30.358231",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.324727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                last session of the day \n",
       "1       shanghai is also really exciting precisely  sk...\n",
       "2                                  submit the report asap\n",
       "3                                              happy bday\n",
       "4                                      the ogs  i like it\n",
       "                              ...                        \n",
       "4953      make a pet face  wtf wrong with me tonight haha\n",
       "4954         i dnt care anymore  boyz aint worth d drama \n",
       "4955    no relationship is perfect tho me bae goo from...\n",
       "4956    over here tryna get my nail polishes and shit lol\n",
       "4957                       no one was loved d way i luv u\n",
       "Name: text, Length: 4958, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21b6160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.397535Z",
     "iopub.status.busy": "2024-11-29T13:48:30.397318Z",
     "iopub.status.idle": "2024-11-29T13:48:30.408125Z",
     "shell.execute_reply": "2024-11-29T13:48:30.407567Z"
    },
    "papermill": {
     "duration": 0.032278,
     "end_time": "2024-11-29T13:48:30.409617",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.377339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "df[emotion_columns] = df[emotion_columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57568291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.449085Z",
     "iopub.status.busy": "2024-11-29T13:48:30.448828Z",
     "iopub.status.idle": "2024-11-29T13:48:30.453089Z",
     "shell.execute_reply": "2024-11-29T13:48:30.452287Z"
    },
    "papermill": {
     "duration": 0.025682,
     "end_time": "2024-11-29T13:48:30.454712",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.429030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "# df['anger'] = df['anger'].astype(float)\n",
    "# df['disgust'] = df['disgust'].astype(float)\n",
    "# df['fear'] = df['fear'].astype(float)\n",
    "# df['joy'] = df['joy'].astype(float)\n",
    "# df['neutral'] = df['neutral'].astype(float)\n",
    "# df['sadness'] = df['sadness'].astype(float)\n",
    "# df['surprise'] = df['surprise'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e781b738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:30.494233Z",
     "iopub.status.busy": "2024-11-29T13:48:30.494015Z",
     "iopub.status.idle": "2024-11-29T13:48:31.217001Z",
     "shell.execute_reply": "2024-11-29T13:48:31.216349Z"
    },
    "papermill": {
     "duration": 0.745026,
     "end_time": "2024-11-29T13:48:31.218984",
     "exception": false,
     "start_time": "2024-11-29T13:48:30.473958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: text_preprocessing_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc9aa52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.258660Z",
     "iopub.status.busy": "2024-11-29T13:48:31.258425Z",
     "iopub.status.idle": "2024-11-29T13:48:31.264630Z",
     "shell.execute_reply": "2024-11-29T13:48:31.263836Z"
    },
    "papermill": {
     "duration": 0.027797,
     "end_time": "2024-11-29T13:48:31.266293",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.238496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 last session of the day\n",
       "1       shanghai is also really exciting precisely sky...\n",
       "2                                  submit the report asap\n",
       "3                                              happy bday\n",
       "4                                       the ogs i like it\n",
       "                              ...                        \n",
       "4953       make a pet face wtf wrong with me tonight haha\n",
       "4954           i dnt care anymore boyz aint worth d drama\n",
       "4955    no relationship is perfect tho me bae goo from...\n",
       "4956    over here tryna get my nail polishes and shit lol\n",
       "4957                       no one was loved d way i luv u\n",
       "Name: text, Length: 4958, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e7378e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.305981Z",
     "iopub.status.busy": "2024-11-29T13:48:31.305730Z",
     "iopub.status.idle": "2024-11-29T13:48:31.311645Z",
     "shell.execute_reply": "2024-11-29T13:48:31.310857Z"
    },
    "papermill": {
     "duration": 0.027619,
     "end_time": "2024-11-29T13:48:31.313227",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.285608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['serial', 'pred', 'label', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97b1b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.353312Z",
     "iopub.status.busy": "2024-11-29T13:48:31.353097Z",
     "iopub.status.idle": "2024-11-29T13:48:31.356830Z",
     "shell.execute_reply": "2024-11-29T13:48:31.356037Z"
    },
    "papermill": {
     "duration": 0.025501,
     "end_time": "2024-11-29T13:48:31.358400",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.332899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_label_mapping = {\n",
    "    0: \"anger\", 1: \"disgust\", 2: \"fear\", 3: \"joy\", 4: \"neutral\",\n",
    "    5: \"sadness\", 6: \"surprise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb97d3ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.398072Z",
     "iopub.status.busy": "2024-11-29T13:48:31.397798Z",
     "iopub.status.idle": "2024-11-29T13:48:31.401164Z",
     "shell.execute_reply": "2024-11-29T13:48:31.400529Z"
    },
    "papermill": {
     "duration": 0.02481,
     "end_time": "2024-11-29T13:48:31.402694",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.377884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMOTION_LABELS = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\",\n",
    "    \"sadness\", \"surprise\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34c7a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.441959Z",
     "iopub.status.busy": "2024-11-29T13:48:31.441709Z",
     "iopub.status.idle": "2024-11-29T13:48:31.445645Z",
     "shell.execute_reply": "2024-11-29T13:48:31.444867Z"
    },
    "papermill": {
     "duration": 0.025239,
     "end_time": "2024-11-29T13:48:31.447168",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.421929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_train_split(dataset, test_ratio=0.1):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1582a8f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.519964Z",
     "iopub.status.busy": "2024-11-29T13:48:31.519659Z",
     "iopub.status.idle": "2024-11-29T13:48:31.526629Z",
     "shell.execute_reply": "2024-11-29T13:48:31.525878Z"
    },
    "papermill": {
     "duration": 0.061994,
     "end_time": "2024-11-29T13:48:31.528201",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.466207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4516 examples in training, 442 examples in testing.\n"
     ]
    }
   ],
   "source": [
    "train_ds_pd, validation_ds_pd = test_train_split(df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(validation_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1449203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.567639Z",
     "iopub.status.busy": "2024-11-29T13:48:31.567383Z",
     "iopub.status.idle": "2024-11-29T13:48:31.572374Z",
     "shell.execute_reply": "2024-11-29T13:48:31.571754Z"
    },
    "papermill": {
     "duration": 0.026548,
     "end_time": "2024-11-29T13:48:31.574043",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.547495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_pd = train_ds_pd.reset_index(drop=True)\n",
    "validation_ds_pd = validation_ds_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8916d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:31.615074Z",
     "iopub.status.busy": "2024-11-29T13:48:31.614562Z",
     "iopub.status.idle": "2024-11-29T13:48:34.739075Z",
     "shell.execute_reply": "2024-11-29T13:48:34.738376Z"
    },
    "papermill": {
     "duration": 3.146673,
     "end_time": "2024-11-29T13:48:34.741094",
     "exception": false,
     "start_time": "2024-11-29T13:48:31.594421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a2d67",
   "metadata": {
    "papermill": {
     "duration": 0.019277,
     "end_time": "2024-11-29T13:48:34.780410",
     "exception": false,
     "start_time": "2024-11-29T13:48:34.761133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SETTING CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ee31c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:34.820754Z",
     "iopub.status.busy": "2024-11-29T13:48:34.820329Z",
     "iopub.status.idle": "2024-11-29T13:48:34.903068Z",
     "shell.execute_reply": "2024-11-29T13:48:34.902335Z"
    },
    "papermill": {
     "duration": 0.104944,
     "end_time": "2024-11-29T13:48:34.904728",
     "exception": false,
     "start_time": "2024-11-29T13:48:34.799784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f07255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:34.945216Z",
     "iopub.status.busy": "2024-11-29T13:48:34.944943Z",
     "iopub.status.idle": "2024-11-29T13:48:34.948679Z",
     "shell.execute_reply": "2024-11-29T13:48:34.948049Z"
    },
    "papermill": {
     "duration": 0.025743,
     "end_time": "2024-11-29T13:48:34.950283",
     "exception": false,
     "start_time": "2024-11-29T13:48:34.924540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "576134ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:34.990103Z",
     "iopub.status.busy": "2024-11-29T13:48:34.989834Z",
     "iopub.status.idle": "2024-11-29T13:48:36.897960Z",
     "shell.execute_reply": "2024-11-29T13:48:36.897214Z"
    },
    "papermill": {
     "duration": 1.930025,
     "end_time": "2024-11-29T13:48:36.899768",
     "exception": false,
     "start_time": "2024-11-29T13:48:34.969743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DebertaV2Model, DebertaV2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91701547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:36.941113Z",
     "iopub.status.busy": "2024-11-29T13:48:36.940676Z",
     "iopub.status.idle": "2024-11-29T13:48:38.458328Z",
     "shell.execute_reply": "2024-11-29T13:48:38.457379Z"
    },
    "papermill": {
     "duration": 1.539987,
     "end_time": "2024-11-29T13:48:38.460254",
     "exception": false,
     "start_time": "2024-11-29T13:48:36.920267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27bfe67bb0743d4968c07a6d8c6e737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91343e06f8124833aac7683f490e0c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0571fb07024fd1a3da6034907b502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f92c80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:38.502629Z",
     "iopub.status.busy": "2024-11-29T13:48:38.502345Z",
     "iopub.status.idle": "2024-11-29T13:48:38.505969Z",
     "shell.execute_reply": "2024-11-29T13:48:38.505175Z"
    },
    "papermill": {
     "duration": 0.02635,
     "end_time": "2024-11-29T13:48:38.507500",
     "exception": false,
     "start_time": "2024-11-29T13:48:38.481150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cfc661",
   "metadata": {
    "papermill": {
     "duration": 0.019596,
     "end_time": "2024-11-29T13:48:38.547056",
     "exception": false,
     "start_time": "2024-11-29T13:48:38.527460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TORCH IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc7fcb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:38.587643Z",
     "iopub.status.busy": "2024-11-29T13:48:38.587406Z",
     "iopub.status.idle": "2024-11-29T13:48:52.891047Z",
     "shell.execute_reply": "2024-11-29T13:48:52.890303Z"
    },
    "papermill": {
     "duration": 14.326396,
     "end_time": "2024-11-29T13:48:52.893105",
     "exception": false,
     "start_time": "2024-11-29T13:48:38.566709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 13:48:40,466\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-29 13:48:40,999\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# from optuna.integration import PyTorchLightningPruner\n",
    "from ray import tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.integration.pytorch import TuneReportCallback\n",
    "from torch.amp import GradScaler, autocast\n",
    "# from ray.tune.integration.optuna import OptunaSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from torch import autocast\n",
    "# from ray import tune\n",
    "# from ray.tune.integration.tensorboard import TensorBoardReporter\n",
    "from ray.tune.logger import TBXLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ray.train import report\n",
    "# from ray.tune.integration.jupyter import JupyterNotebookReporter\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from ray.tune.schedulers import HyperBandScheduler, AsyncHyperBandScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33fdb0",
   "metadata": {
    "papermill": {
     "duration": 0.01948,
     "end_time": "2024-11-29T13:48:52.933384",
     "exception": false,
     "start_time": "2024-11-29T13:48:52.913904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET CONFIG & ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ba5bfc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:52.974624Z",
     "iopub.status.busy": "2024-11-29T13:48:52.973984Z",
     "iopub.status.idle": "2024-11-29T13:48:52.980747Z",
     "shell.execute_reply": "2024-11-29T13:48:52.980080Z"
    },
    "papermill": {
     "duration": 0.028892,
     "end_time": "2024-11-29T13:48:52.982216",
     "exception": false,
     "start_time": "2024-11-29T13:48:52.953324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         text = self.data.iloc[index, 0]\n",
    "#         labels = self.data.iloc[index, 1:].values.astype(float)\n",
    "        text = str(self.data.iloc[index]['text'])\n",
    "        labels = self.data.iloc[index][['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].values.astype(np.float32)\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ca162",
   "metadata": {
    "papermill": {
     "duration": 0.019734,
     "end_time": "2024-11-29T13:48:53.022013",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.002279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac948e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.063804Z",
     "iopub.status.busy": "2024-11-29T13:48:53.063549Z",
     "iopub.status.idle": "2024-11-29T13:48:53.069347Z",
     "shell.execute_reply": "2024-11-29T13:48:53.068605Z"
    },
    "papermill": {
     "duration": 0.029051,
     "end_time": "2024-11-29T13:48:53.070821",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.041770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.roberta = roberta_model\n",
    "        self.drop = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.out = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_states = output.last_hidden_state\n",
    "        \n",
    "        # Extract the [CLS] token representation (first token in the sequence)\n",
    "        cls_token_state = output.last_hidden_state[:, 0, :]\n",
    "        output = self.drop(cls_token_state)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.drop(output)\n",
    "        output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64daa7",
   "metadata": {
    "papermill": {
     "duration": 0.019497,
     "end_time": "2024-11-29T13:48:53.109934",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.090437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Other Models to try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0684e829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.152703Z",
     "iopub.status.busy": "2024-11-29T13:48:53.152441Z",
     "iopub.status.idle": "2024-11-29T13:48:53.155854Z",
     "shell.execute_reply": "2024-11-29T13:48:53.155226Z"
    },
    "papermill": {
     "duration": 0.026541,
     "end_time": "2024-11-29T13:48:53.157418",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.130877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         output = self.drop(output.pooler_output)\n",
    "#         return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e1da77d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.197860Z",
     "iopub.status.busy": "2024-11-29T13:48:53.197602Z",
     "iopub.status.idle": "2024-11-29T13:48:53.201280Z",
     "shell.execute_reply": "2024-11-29T13:48:53.200589Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025733,
     "end_time": "2024-11-29T13:48:53.202792",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.177059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.out = nn.Linear(256, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Use the pooled output for classification tasks\n",
    "#         pooled_output = output.pooler_output\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a71b339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.243217Z",
     "iopub.status.busy": "2024-11-29T13:48:53.242982Z",
     "iopub.status.idle": "2024-11-29T13:48:53.246622Z",
     "shell.execute_reply": "2024-11-29T13:48:53.245916Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025719,
     "end_time": "2024-11-29T13:48:53.248262",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.222543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate, hidden_size):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.out = nn.Linear(hidden_size // 2, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Use the pooled output for classification tasks\n",
    "#         pooled_output = output.pooler_output\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f18a40a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.289033Z",
     "iopub.status.busy": "2024-11-29T13:48:53.288775Z",
     "iopub.status.idle": "2024-11-29T13:48:53.292357Z",
     "shell.execute_reply": "2024-11-29T13:48:53.291584Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025491,
     "end_time": "2024-11-29T13:48:53.293972",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.268481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class RoBERTaEmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model_name, num_emotions=7):\n",
    "#         super(RoBERTaEmotionModel, self).__init__()\n",
    "#         self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "#         self.drop = nn.Dropout(p=0.3)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.roberta(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         pooled_output = outputs[1]  # CLS token\n",
    "#         output = self.drop(pooled_output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0969644",
   "metadata": {
    "papermill": {
     "duration": 0.019674,
     "end_time": "2024-11-29T13:48:53.333354",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.313680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN & VALIDATION \n",
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75a0e8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.374408Z",
     "iopub.status.busy": "2024-11-29T13:48:53.374126Z",
     "iopub.status.idle": "2024-11-29T13:48:53.378409Z",
     "shell.execute_reply": "2024-11-29T13:48:53.377593Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026886,
     "end_time": "2024-11-29T13:48:53.380119",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.353233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, n_epochs):\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for data in train_loader:\n",
    "#             input_ids = data['input_ids'].to(device)\n",
    "#             attention_mask = data['attention_mask'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             with autocast(device_type=device.type):\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             if scheduler:\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "#         train_accuracy = correct / total\n",
    "#         val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a870841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.421882Z",
     "iopub.status.busy": "2024-11-29T13:48:53.421165Z",
     "iopub.status.idle": "2024-11-29T13:48:53.425078Z",
     "shell.execute_reply": "2024-11-29T13:48:53.424257Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026558,
     "end_time": "2024-11-29T13:48:53.426640",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.400082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def eval_model(model, val_loader, loss_fn, device):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             input_ids = data['input_ids'].to(device)\n",
    "#             attention_mask = data['attention_mask'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "#     val_accuracy = correct / total\n",
    "#     return val_loss / len(val_loader), val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "047f3b58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.467438Z",
     "iopub.status.busy": "2024-11-29T13:48:53.467219Z",
     "iopub.status.idle": "2024-11-29T13:48:53.471321Z",
     "shell.execute_reply": "2024-11-29T13:48:53.470596Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026548,
     "end_time": "2024-11-29T13:48:53.472875",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.446327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
    "#     dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [2, 4, 8, 16])\n",
    "#     learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     epochs = 5  # Adjust as needed\n",
    "\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "#     # Load your data here\n",
    "# #     data = df  # Replace with your data loading logic\n",
    "# #     train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "# #         data.iloc[:, 0],  # Assuming text is in the first column\n",
    "# #         data.iloc[:, 1:],  # Labels are in the remaining columns\n",
    "# #         test_size=0.2\n",
    "# #     )\n",
    "\n",
    "\n",
    "# #     train_dataset = EmotionDataset(pd.DataFrame({'text': train_texts, **pd.DataFrame(train_labels)}), tokenizer, max_len=128)\n",
    "# #     val_dataset = EmotionDataset(pd.DataFrame({'text': val_texts, **pd.DataFrame(val_labels)}), tokenizer, max_len=128)\n",
    "    \n",
    "    \n",
    "#     train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#     val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     model = EmotionClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "#     loss_fn = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, epochs)\n",
    "#     val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "    \n",
    "#     return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f428e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.513917Z",
     "iopub.status.busy": "2024-11-29T13:48:53.513635Z",
     "iopub.status.idle": "2024-11-29T13:48:53.517857Z",
     "shell.execute_reply": "2024-11-29T13:48:53.517072Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026821,
     "end_time": "2024-11-29T13:48:53.519501",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.492680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_fn(config):\n",
    "#     try:\n",
    "#         hidden_size = config['hidden_size']\n",
    "#         dropout_rate = config['dropout_rate']\n",
    "#         batch_size = config['batch_size']\n",
    "#         learning_rate = config['lr']\n",
    "#         epochs = 5\n",
    "\n",
    "#         tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#         # Load your data here\n",
    "# #         data = pd.read_csv('path_to_your_data.csv')  # Replace with your data loading logic\n",
    "# #         train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "# #             data.iloc[:, 0],  # Assuming text is in the first column\n",
    "# #             data.iloc[:, 1:],  # Labels are in the remaining columns\n",
    "# #             test_size=0.2\n",
    "# #         )\n",
    "\n",
    "#         train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#         val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         model = EmotionClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "#         loss_fn = nn.BCEWithLogitsLoss()\n",
    "#         optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#         scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "#         scaler = GradScaler()\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, epochs)\n",
    "#             val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "#             tune.report(loss=val_loss, accuracy=val_accuracy)\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47e5c8e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.560471Z",
     "iopub.status.busy": "2024-11-29T13:48:53.560254Z",
     "iopub.status.idle": "2024-11-29T13:48:53.564169Z",
     "shell.execute_reply": "2024-11-29T13:48:53.563362Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026263,
     "end_time": "2024-11-29T13:48:53.565721",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.539458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tune_model(config):\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#     train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#     train_dataset = EmotionDataset(train_df, tokenizer, config['max_len'])\n",
    "#     val_dataset = EmotionDataset(val_df, tokenizer, config['max_len'])\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = RoBERTaEmotionModel('roberta-base').to(device)\n",
    "\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "#     loss_fn = nn.MSELoss().to(device)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['lr'], steps_per_epoch=len(train_loader), epochs=config['epochs'])\n",
    "\n",
    "#     train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, config['epochs'])\n",
    "\n",
    "#     val_loss = eval_model(model, val_loader, loss_fn, device)\n",
    "#     tune.report(val_loss=val_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a9970a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.606315Z",
     "iopub.status.busy": "2024-11-29T13:48:53.606086Z",
     "iopub.status.idle": "2024-11-29T13:48:53.609637Z",
     "shell.execute_reply": "2024-11-29T13:48:53.608901Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.02579,
     "end_time": "2024-11-29T13:48:53.611199",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.585409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'max_len': tune.choice([128, 192, 256]),\n",
    "#     'batch_size': tune.choice([8, 16, 32]),\n",
    "#     'lr': tune.loguniform(1e-5, 5e-5),\n",
    "#     'epochs': tune.choice([3, 5, 7])\n",
    "# }\n",
    "\n",
    "# scheduler = ASHAScheduler(\n",
    "#     metric='val_loss',\n",
    "#     mode='min',\n",
    "#     max_t=10,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )\n",
    "\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=['val_loss', 'training_iteration']\n",
    "# )\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     tune_model,\n",
    "#     resources_per_trial={'cpu': 2, 'gpu': 1},\n",
    "#     config=config,\n",
    "#     num_samples=20,\n",
    "#     scheduler=scheduler,\n",
    "#     progress_reporter=reporter\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8854794a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.652018Z",
     "iopub.status.busy": "2024-11-29T13:48:53.651589Z",
     "iopub.status.idle": "2024-11-29T13:48:53.656521Z",
     "shell.execute_reply": "2024-11-29T13:48:53.655872Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026887,
     "end_time": "2024-11-29T13:48:53.657989",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.631102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "#     model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "#     model.train()\n",
    "#     for epoch in range(config[\"epochs\"]):\n",
    "#         total_train_loss = 0.0\n",
    "#         correct_train_preds = 0\n",
    "#         total_train_preds = 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "        \n",
    "#         avg_train_loss = total_train_loss / len(train_loader)\n",
    "#         train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         correct_val_preds = 0\n",
    "#         total_val_preds = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 input_ids = batch[\"input_ids\"].to(device)\n",
    "#                 attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#                 labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "\n",
    "#         avg_val_loss = total_val_loss / len(val_loader)\n",
    "#         val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "#         tune.report(loss=avg_val_loss, accuracy=val_accuracy, train_loss=avg_train_loss, train_accuracy=train_accuracy)\n",
    "#         model_save_path = os.path.join(tune.get_trial_dir(), \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def train_fn(config):\n",
    "#     # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#     train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#     val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     train_model(config, train_dataset, val_dataset, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8ffad",
   "metadata": {
    "papermill": {
     "duration": 0.01961,
     "end_time": "2024-11-29T13:48:53.697400",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.677790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NEW TRAIN & VALIDATION 2.0\n",
    "### Custom metric is a metric for determining the best model free from any overfitting, underfitting. ```custom_metric = (avg_val_loss−val_accuracy) + α × ∣avg_train_loss−avg_val_loss∣ + β × ∣val_accuracy-train_accuracy∣``` This metric balances between low validation loss, high validation accuracy, and penalizing large discrepancies between training and validation loss. Note that, in the model selection we have used ```α = β = 0.5```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2c316",
   "metadata": {
    "papermill": {
     "duration": 0.01964,
     "end_time": "2024-11-29T13:48:53.736691",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.717051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### But since, we have probabilistic values as input and output we cannot directly calculate the accuracy, infact we need to rely on loss only. ```custom_metric = avg_val_loss + α × ∣avg_train_loss−avg_val_loss∣```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ee76d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.777042Z",
     "iopub.status.busy": "2024-11-29T13:48:53.776799Z",
     "iopub.status.idle": "2024-11-29T13:48:53.780592Z",
     "shell.execute_reply": "2024-11-29T13:48:53.779886Z"
    },
    "papermill": {
     "duration": 0.025585,
     "end_time": "2024-11-29T13:48:53.782196",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.756611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom metric calculation function\n",
    "def calculate_custom_metric(avg_val_loss, avg_train_loss, alpha=0.5):\n",
    "    loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "    custom_metric = avg_val_loss + alpha * loss_diff\n",
    "    return custom_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9498517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.823111Z",
     "iopub.status.busy": "2024-11-29T13:48:53.822357Z",
     "iopub.status.idle": "2024-11-29T13:48:53.826063Z",
     "shell.execute_reply": "2024-11-29T13:48:53.825436Z"
    },
    "papermill": {
     "duration": 0.025747,
     "end_time": "2024-11-29T13:48:53.827652",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.801905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Custom metric calculation function\n",
    "# def calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy, alpha=0.5, beta=0.5):\n",
    "#     loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "#     accuracy_diff = abs(train_accuracy - val_accuracy)\n",
    "#     custom_metric = avg_val_loss - val_accuracy + alpha * loss_diff + beta * accuracy_diff\n",
    "#     return custom_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecc86d",
   "metadata": {
    "papermill": {
     "duration": 0.019794,
     "end_time": "2024-11-29T13:48:53.867064",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.847270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27e39c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.907283Z",
     "iopub.status.busy": "2024-11-29T13:48:53.907049Z",
     "iopub.status.idle": "2024-11-29T13:48:53.912237Z",
     "shell.execute_reply": "2024-11-29T13:48:53.911530Z"
    },
    "papermill": {
     "duration": 0.027104,
     "end_time": "2024-11-29T13:48:53.913759",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.886655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        EarlyStopping to stop training when a metric has stopped improving.\n",
    "\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss improved to {val_loss:.4f}')\n",
    "#             torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss did not improve. Patience: {self.patience}, Counter: {self.counter}')\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "024f5397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:53.954605Z",
     "iopub.status.busy": "2024-11-29T13:48:53.954385Z",
     "iopub.status.idle": "2024-11-29T13:48:53.968179Z",
     "shell.execute_reply": "2024-11-29T13:48:53.967578Z"
    },
    "papermill": {
     "duration": 0.036485,
     "end_time": "2024-11-29T13:48:53.970171",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.933686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training function with variable parameters\n",
    "def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = EmotionModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=7,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()  # Initialize GradScaler\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):  # Ensure epochs are passed from config\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):  # Mixed precision\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "            correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "                correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "#         custom_metric = avg_val_loss - val_accuracy\n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)\n",
    "\n",
    "        # Report metrics using ray.train.report\n",
    "        report({\n",
    "            \"loss\": avg_val_loss,\n",
    "            \"accuracy\": val_accuracy,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"custom_metric\": custom_metric,\n",
    "            \"early_stopping_epoch\": epoch + 1,\n",
    "        })\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "#         trial_dir = os.path.join(os.environ[\"TUNE_TRIAL_DIR\"], \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "#         checkpoint_path = f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_fn(config):\n",
    "    # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "    train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(config, train_dataset, val_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be9603",
   "metadata": {
    "papermill": {
     "duration": 0.019972,
     "end_time": "2024-11-29T13:48:54.014775",
     "exception": false,
     "start_time": "2024-11-29T13:48:53.994803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fa98a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.056374Z",
     "iopub.status.busy": "2024-11-29T13:48:54.056113Z",
     "iopub.status.idle": "2024-11-29T13:48:54.060552Z",
     "shell.execute_reply": "2024-11-29T13:48:54.059902Z"
    },
    "papermill": {
     "duration": 0.027568,
     "end_time": "2024-11-29T13:48:54.062095",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.034527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'dropout_rate': tune.choice([0.2, 0.25, 0.27, 0.3, 0.35, 0.4]),\n",
    "    'batch_size': tune.choice([32, 64]),\n",
    "    'weight_decay': tune.choice([1e-6, 2e-6, 1e-5, 2e-5, 1e-4, 2e-4, 5e-5, 5e-6, 1e-2, 1e-3]),\n",
    "    'lr': tune.choice([1e-6, 1e-5, 2e-5, 1e-4, 2e-4, 2e-6, 3e-6, 5e-6, 3e-5, 5e-5, 2e-7, 1e-7, 5e-7, 1e-3]),\n",
    "    'epochs': tune.choice([5, 7, 10, 12, 15, 20])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0570f98a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.102531Z",
     "iopub.status.busy": "2024-11-29T13:48:54.101890Z",
     "iopub.status.idle": "2024-11-29T13:48:54.106066Z",
     "shell.execute_reply": "2024-11-29T13:48:54.105163Z"
    },
    "papermill": {
     "duration": 0.026657,
     "end_time": "2024-11-29T13:48:54.108307",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.081650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([32, 64, 128, 256]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([3, 5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "160ad350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.175777Z",
     "iopub.status.busy": "2024-11-29T13:48:54.174993Z",
     "iopub.status.idle": "2024-11-29T13:48:54.178899Z",
     "shell.execute_reply": "2024-11-29T13:48:54.178094Z"
    },
    "papermill": {
     "duration": 0.035863,
     "end_time": "2024-11-29T13:48:54.180486",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.144623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([3, 5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a1bfd93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.221528Z",
     "iopub.status.busy": "2024-11-29T13:48:54.221272Z",
     "iopub.status.idle": "2024-11-29T13:48:54.224573Z",
     "shell.execute_reply": "2024-11-29T13:48:54.223889Z"
    },
    "papermill": {
     "duration": 0.025282,
     "end_time": "2024-11-29T13:48:54.226136",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.200854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([32, 64, 128, 256]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.randint(3, 21)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bce2da02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.266593Z",
     "iopub.status.busy": "2024-11-29T13:48:54.266352Z",
     "iopub.status.idle": "2024-11-29T13:48:54.269781Z",
     "shell.execute_reply": "2024-11-29T13:48:54.269140Z"
    },
    "papermill": {
     "duration": 0.025543,
     "end_time": "2024-11-29T13:48:54.271359",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.245816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([256, 512]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([4, 8, 16]),\n",
    "#     'lr': tune.loguniform(1e-5, 1e-2)\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0f9fd",
   "metadata": {
    "papermill": {
     "duration": 0.019607,
     "end_time": "2024-11-29T13:48:54.310655",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.291048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fef072b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.352229Z",
     "iopub.status.busy": "2024-11-29T13:48:54.351983Z",
     "iopub.status.idle": "2024-11-29T13:48:54.355697Z",
     "shell.execute_reply": "2024-11-29T13:48:54.354883Z"
    },
    "papermill": {
     "duration": 0.026345,
     "end_time": "2024-11-29T13:48:54.357207",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.330862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optuna_search = OptunaSearch(\n",
    "    metric=\"accuracy\",\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df520a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.397939Z",
     "iopub.status.busy": "2024-11-29T13:48:54.397279Z",
     "iopub.status.idle": "2024-11-29T13:48:54.400742Z",
     "shell.execute_reply": "2024-11-29T13:48:54.400077Z"
    },
    "papermill": {
     "duration": 0.025499,
     "end_time": "2024-11-29T13:48:54.402301",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.376802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optuna_search = OptunaSearch(\n",
    "#     metric=[\n",
    "#         tune.MultiObjective(\"loss\", \"min\"),\n",
    "#         tune.MultiObjective(\"custom_metric\", \"min\"),\n",
    "#         tune.MultiObjective(\"accuracy\", \"max\")\n",
    "#     ],\n",
    "#     mode=[\n",
    "#         \"min\",  # for loss\n",
    "#         \"min\",  # for custom metric\n",
    "#         \"max\"   # for accuracy\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "678c612b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.443216Z",
     "iopub.status.busy": "2024-11-29T13:48:54.442564Z",
     "iopub.status.idle": "2024-11-29T13:48:54.446192Z",
     "shell.execute_reply": "2024-11-29T13:48:54.445267Z"
    },
    "papermill": {
     "duration": 0.025543,
     "end_time": "2024-11-29T13:48:54.447802",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.422259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optuna_search = OptunaSearch(\n",
    "#     metric=\"custom_metric\",\n",
    "#     mode=\"min\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05dda16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.488466Z",
     "iopub.status.busy": "2024-11-29T13:48:54.488003Z",
     "iopub.status.idle": "2024-11-29T13:48:54.491221Z",
     "shell.execute_reply": "2024-11-29T13:48:54.490667Z"
    },
    "papermill": {
     "duration": 0.025133,
     "end_time": "2024-11-29T13:48:54.492765",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.467632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Setup Optuna for hyperparameter optimization\n",
    "# optuna_search = OptunaSearch(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43b5457e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.532934Z",
     "iopub.status.busy": "2024-11-29T13:48:54.532661Z",
     "iopub.status.idle": "2024-11-29T13:48:54.535705Z",
     "shell.execute_reply": "2024-11-29T13:48:54.535079Z"
    },
    "papermill": {
     "duration": 0.024884,
     "end_time": "2024-11-29T13:48:54.537273",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.512389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=15,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f6e9ba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.577336Z",
     "iopub.status.busy": "2024-11-29T13:48:54.577096Z",
     "iopub.status.idle": "2024-11-29T13:48:54.580476Z",
     "shell.execute_reply": "2024-11-29T13:48:54.579671Z"
    },
    "papermill": {
     "duration": 0.02532,
     "end_time": "2024-11-29T13:48:54.582062",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.556742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"accuracy\",\n",
    "#     mode=\"max\",\n",
    "#     max_t=15,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2af074",
   "metadata": {
    "papermill": {
     "duration": 0.019624,
     "end_time": "2024-11-29T13:48:54.621600",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.601976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SCHEDULER ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da16be0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.662193Z",
     "iopub.status.busy": "2024-11-29T13:48:54.661939Z",
     "iopub.status.idle": "2024-11-29T13:48:54.665574Z",
     "shell.execute_reply": "2024-11-29T13:48:54.664875Z"
    },
    "papermill": {
     "duration": 0.025929,
     "end_time": "2024-11-29T13:48:54.667247",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.641318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric='accuracy',\n",
    "    mode='max',\n",
    "    max_t=22,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88966735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.708749Z",
     "iopub.status.busy": "2024-11-29T13:48:54.708512Z",
     "iopub.status.idle": "2024-11-29T13:48:54.711691Z",
     "shell.execute_reply": "2024-11-29T13:48:54.711069Z"
    },
    "papermill": {
     "duration": 0.025159,
     "end_time": "2024-11-29T13:48:54.713183",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.688024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = AsyncHyperBandScheduler(\n",
    "#     metric='accuracy',\n",
    "#     mode='max',\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee5e68a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.753471Z",
     "iopub.status.busy": "2024-11-29T13:48:54.753227Z",
     "iopub.status.idle": "2024-11-29T13:48:54.756471Z",
     "shell.execute_reply": "2024-11-29T13:48:54.755899Z"
    },
    "papermill": {
     "duration": 0.025109,
     "end_time": "2024-11-29T13:48:54.757930",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.732821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=[\n",
    "#         tune.MultiObjective(\"loss\", \"min\"),\n",
    "#         tune.MultiObjective(\"custom_metric\", \"min\"),\n",
    "#         tune.MultiObjective(\"accuracy\", \"max\")\n",
    "#     ],\n",
    "#     mode=[\n",
    "#         \"min\",  # for loss\n",
    "#         \"min\",  # for custom metric\n",
    "#         \"max\"   # for accuracy\n",
    "#     ],\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "521ed23f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.798873Z",
     "iopub.status.busy": "2024-11-29T13:48:54.798160Z",
     "iopub.status.idle": "2024-11-29T13:48:54.801376Z",
     "shell.execute_reply": "2024-11-29T13:48:54.800761Z"
    },
    "papermill": {
     "duration": 0.025346,
     "end_time": "2024-11-29T13:48:54.802926",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.777580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"custom_metric\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "385daa72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.844346Z",
     "iopub.status.busy": "2024-11-29T13:48:54.844093Z",
     "iopub.status.idle": "2024-11-29T13:48:54.847611Z",
     "shell.execute_reply": "2024-11-29T13:48:54.846814Z"
    },
    "papermill": {
     "duration": 0.02645,
     "end_time": "2024-11-29T13:48:54.849250",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.822800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = HyperBandScheduler(\n",
    "#     time_attr='training_iteration',  # The attribute to use for the time dimension (similar to ASHA's `max_t`)\n",
    "#     metric='custom_metric',          # The metric to optimize (similar to ASHA's `metric`)\n",
    "#     mode='min',                      # Optimization mode (minimize `custom_metric`)\n",
    "#     max_t=22,                        # Maximum number of iterations per trial (similar to ASHA's `max_t`)\n",
    "#     reduction_factor=2               # Reduction factor (similar to ASHA)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b8346c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.890471Z",
     "iopub.status.busy": "2024-11-29T13:48:54.889742Z",
     "iopub.status.idle": "2024-11-29T13:48:54.893700Z",
     "shell.execute_reply": "2024-11-29T13:48:54.892912Z"
    },
    "papermill": {
     "duration": 0.026145,
     "end_time": "2024-11-29T13:48:54.895335",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.869190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False\n",
    ")\n",
    "reporter.verbosity = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eefa9d",
   "metadata": {
    "papermill": {
     "duration": 0.019741,
     "end_time": "2024-11-29T13:48:54.935627",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.915886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom CLI Reporter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "320ce977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:54.976173Z",
     "iopub.status.busy": "2024-11-29T13:48:54.975919Z",
     "iopub.status.idle": "2024-11-29T13:48:54.979791Z",
     "shell.execute_reply": "2024-11-29T13:48:54.978994Z"
    },
    "papermill": {
     "duration": 0.02583,
     "end_time": "2024-11-29T13:48:54.981340",
     "exception": false,
     "start_time": "2024-11-29T13:48:54.955510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FinalTableCLIReporter(CLIReporter):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.results = []\n",
    "\n",
    "#     def _update(self, *args, **kwargs):\n",
    "#         # Collect results without printing intermediate updates\n",
    "#         self.results.append(kwargs)\n",
    "\n",
    "#     def print_table(self):\n",
    "#         # Print only the final results\n",
    "#         if self.results:\n",
    "#             final_results = [result for result in self.results]\n",
    "#             headers = list(final_results[0].keys()) if final_results else []\n",
    "#             table = [headers] + [list(result.values()) for result in final_results]\n",
    "#             for row in table:\n",
    "#                 print(\" | \".join(str(cell) for cell in row))\n",
    "\n",
    "#     def _report(self, *args, **kwargs):\n",
    "#         # Override this to prevent intermediate print\n",
    "#         self._update(*args, **kwargs)\n",
    "\n",
    "#     def _finalize(self):\n",
    "#         # Call this to print the final table\n",
    "#         self.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212059f",
   "metadata": {
    "papermill": {
     "duration": 0.019797,
     "end_time": "2024-11-29T13:48:55.021051",
     "exception": false,
     "start_time": "2024-11-29T13:48:55.001254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16f44c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:55.064329Z",
     "iopub.status.busy": "2024-11-29T13:48:55.064082Z",
     "iopub.status.idle": "2024-11-29T13:48:55.067767Z",
     "shell.execute_reply": "2024-11-29T13:48:55.066968Z"
    },
    "papermill": {
     "duration": 0.028303,
     "end_time": "2024-11-29T13:48:55.069534",
     "exception": false,
     "start_time": "2024-11-29T13:48:55.041231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To ignore warinings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c77b8fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T13:48:55.115867Z",
     "iopub.status.busy": "2024-11-29T13:48:55.115404Z",
     "iopub.status.idle": "2024-11-29T18:43:42.251234Z",
     "shell.execute_reply": "2024-11-29T18:43:42.250218Z"
    },
    "papermill": {
     "duration": 17687.158952,
     "end_time": "2024-11-29T18:43:42.253566",
     "exception": false,
     "start_time": "2024-11-29T13:48:55.094614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-29 18:43:42</td></tr>\n",
       "<tr><td>Running for: </td><td>04:54:34.75        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.2/31.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=60<br>Bracket: Iter 12.000: 0.9149967679379444 | Iter 6.000: 0.9143503555268261 | Iter 3.000: 0.9044925662572721<br>Logical resource usage: 2.0/4 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  early_stopping_epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_de540f6e</td><td>TERMINATED</td><td>172.19.2.2:341 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0350333</td><td style=\"text-align: right;\">  0.913381</td><td style=\"text-align: right;\">      0.0378744</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.029351  </td><td style=\"text-align: right;\">        0.928255</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5ccc8042</td><td>TERMINATED</td><td>172.19.2.2:376 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0511788</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0529625</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0547463 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_f4f11c61</td><td>TERMINATED</td><td>172.19.2.2:482 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.050289 </td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.052501 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.054713  </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_cc7b2bbd</td><td>TERMINATED</td><td>172.19.2.2:560 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0518973</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0531873</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0544773 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_af324419</td><td>TERMINATED</td><td>172.19.2.2:641 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0314786</td><td style=\"text-align: right;\">  0.915966</td><td style=\"text-align: right;\">      0.0423468</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00974222</td><td style=\"text-align: right;\">        0.969663</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_109ac776</td><td>TERMINATED</td><td>172.19.2.2:728 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0505464</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526129</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0546793 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_01ed111c</td><td>TERMINATED</td><td>172.19.2.2:829 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0508442</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0545136</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0581831 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_908f2e6c</td><td>TERMINATED</td><td>172.19.2.2:922 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0368132</td><td style=\"text-align: right;\">  0.912088</td><td style=\"text-align: right;\">      0.0455325</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0193747 </td><td style=\"text-align: right;\">        0.948659</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_f727ef99</td><td>TERMINATED</td><td>172.19.2.2:1001</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0796937</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0907037</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.101714  </td><td style=\"text-align: right;\">        0.887701</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_16b7cbd1</td><td>TERMINATED</td><td>172.19.2.2:1098</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0323154</td><td style=\"text-align: right;\">  0.918552</td><td style=\"text-align: right;\">      0.0378706</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0212051 </td><td style=\"text-align: right;\">        0.943945</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_6c3e629c</td><td>TERMINATED</td><td>172.19.2.2:1169</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0758959</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0855704</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.095245  </td><td style=\"text-align: right;\">        0.884474</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_34225abb</td><td>TERMINATED</td><td>172.19.2.2:1269</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.175278 </td><td style=\"text-align: right;\">  0.738203</td><td style=\"text-align: right;\">      0.178973 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.182668  </td><td style=\"text-align: right;\">        0.655479</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_48613c3c</td><td>TERMINATED</td><td>172.19.2.2:1346</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0385429</td><td style=\"text-align: right;\">  0.910795</td><td style=\"text-align: right;\">      0.0470659</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0214969 </td><td style=\"text-align: right;\">        0.945464</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_d4ef5ae7</td><td>TERMINATED</td><td>172.19.2.2:1428</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0300592</td><td style=\"text-align: right;\">  0.914027</td><td style=\"text-align: right;\">      0.04111  </td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.00795752</td><td style=\"text-align: right;\">        0.973175</td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_f8e72998</td><td>TERMINATED</td><td>172.19.2.2:1556</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0300058</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0416037</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00681015</td><td style=\"text-align: right;\">        0.974756</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_2a405bc4</td><td>TERMINATED</td><td>172.19.2.2:1631</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0299406</td><td style=\"text-align: right;\">  0.916936</td><td style=\"text-align: right;\">      0.0410155</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.00779094</td><td style=\"text-align: right;\">        0.973554</td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_69e9fd94</td><td>TERMINATED</td><td>172.19.2.2:1766</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.183209 </td><td style=\"text-align: right;\">  0.460246</td><td style=\"text-align: right;\">      0.189404 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.195599  </td><td style=\"text-align: right;\">        0.387859</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_49e25293</td><td>TERMINATED</td><td>172.19.2.2:1802</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0333446</td><td style=\"text-align: right;\">  0.910795</td><td style=\"text-align: right;\">      0.043279 </td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0134758 </td><td style=\"text-align: right;\">        0.962672</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_f90ab6f6</td><td>TERMINATED</td><td>172.19.2.2:1898</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0503676</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0524108</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0544539 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_81042903</td><td>TERMINATED</td><td>172.19.2.2:1997</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.033786 </td><td style=\"text-align: right;\">  0.912411</td><td style=\"text-align: right;\">      0.0437104</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0139371 </td><td style=\"text-align: right;\">        0.961249</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_9c3780ff</td><td>TERMINATED</td><td>172.19.2.2:2090</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0400354</td><td style=\"text-align: right;\">  0.904977</td><td style=\"text-align: right;\">      0.0473455</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0254151 </td><td style=\"text-align: right;\">        0.935151</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_90248808</td><td>TERMINATED</td><td>172.19.2.2:2196</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0487009</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0522138</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0557266 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_fb6accfc</td><td>TERMINATED</td><td>172.19.2.2:2278</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0504637</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0542055</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0579473 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_9c022d45</td><td>TERMINATED</td><td>172.19.2.2:2372</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0503695</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0542905</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0582115 </td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_46b518e6</td><td>TERMINATED</td><td>172.19.2.2:2448</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.037888 </td><td style=\"text-align: right;\">  0.905624</td><td style=\"text-align: right;\">      0.0506012</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0124617 </td><td style=\"text-align: right;\">        0.964887</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_698a65bc</td><td>TERMINATED</td><td>172.19.2.2:2548</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0303713</td><td style=\"text-align: right;\">  0.917582</td><td style=\"text-align: right;\">      0.0402177</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0106785 </td><td style=\"text-align: right;\">        0.970043</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_fe5b5527</td><td>TERMINATED</td><td>172.19.2.2:2648</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0313541</td><td style=\"text-align: right;\">  0.91532 </td><td style=\"text-align: right;\">      0.0430757</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00791094</td><td style=\"text-align: right;\">        0.974219</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_29835677</td><td>TERMINATED</td><td>172.19.2.2:2758</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0334814</td><td style=\"text-align: right;\">  0.911765</td><td style=\"text-align: right;\">      0.0439654</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0125135 </td><td style=\"text-align: right;\">        0.963843</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_e33e54ea</td><td>TERMINATED</td><td>172.19.2.2:2857</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0319665</td><td style=\"text-align: right;\">  0.916613</td><td style=\"text-align: right;\">      0.0411225</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">  0.0136544 </td><td style=\"text-align: right;\">        0.960743</td><td style=\"text-align: right;\">                     9</td></tr>\n",
       "<tr><td>train_fn_f54a9490</td><td>TERMINATED</td><td>172.19.2.2:2968</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0410251</td><td style=\"text-align: right;\">  0.902069</td><td style=\"text-align: right;\">      0.0503268</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0224218 </td><td style=\"text-align: right;\">        0.941858</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_503ad4c5</td><td>TERMINATED</td><td>172.19.2.2:3052</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0322252</td><td style=\"text-align: right;\">  0.919845</td><td style=\"text-align: right;\">      0.0376601</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0213554 </td><td style=\"text-align: right;\">        0.94404 </td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_fed786e8</td><td>TERMINATED</td><td>172.19.2.2:3140</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.033461 </td><td style=\"text-align: right;\">  0.915643</td><td style=\"text-align: right;\">      0.0365665</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0272499 </td><td style=\"text-align: right;\">        0.931672</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_ae2bc71f</td><td>TERMINATED</td><td>172.19.2.2:3222</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0373205</td><td style=\"text-align: right;\">  0.911441</td><td style=\"text-align: right;\">      0.0417144</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0285326 </td><td style=\"text-align: right;\">        0.929236</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_6681de2f</td><td>TERMINATED</td><td>172.19.2.2:3307</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.032462 </td><td style=\"text-align: right;\">  0.917906</td><td style=\"text-align: right;\">      0.0385028</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0203802 </td><td style=\"text-align: right;\">        0.945274</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_da2e9004</td><td>TERMINATED</td><td>172.19.2.2:3390</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0342558</td><td style=\"text-align: right;\">  0.916613</td><td style=\"text-align: right;\">      0.0417494</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0192687 </td><td style=\"text-align: right;\">        0.948248</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_552a4070</td><td>TERMINATED</td><td>172.19.2.2:3478</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0369745</td><td style=\"text-align: right;\">  0.902715</td><td style=\"text-align: right;\">      0.0446231</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0216771 </td><td style=\"text-align: right;\">        0.941193</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_b20e2045</td><td>TERMINATED</td><td>172.19.2.2:3561</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0366559</td><td style=\"text-align: right;\">  0.913058</td><td style=\"text-align: right;\">      0.0447022</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0205632 </td><td style=\"text-align: right;\">        0.946634</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_92270e1f</td><td>TERMINATED</td><td>172.19.2.2:3649</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0332854</td><td style=\"text-align: right;\">  0.911765</td><td style=\"text-align: right;\">      0.0390837</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0216887 </td><td style=\"text-align: right;\">        0.942901</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_95f7641c</td><td>TERMINATED</td><td>172.19.2.2:3731</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0353802</td><td style=\"text-align: right;\">  0.907563</td><td style=\"text-align: right;\">      0.0417099</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0227207 </td><td style=\"text-align: right;\">        0.941351</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_3c6e9283</td><td>TERMINATED</td><td>172.19.2.2:3820</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0502097</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526491</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0550886 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_389be36f</td><td>TERMINATED</td><td>172.19.2.2:3902</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0504594</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0527196</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0549799 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_6cc479ef</td><td>TERMINATED</td><td>172.19.2.2:3978</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0582743</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0678617</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0774491 </td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_53fb32b2</td><td>TERMINATED</td><td>172.19.2.2:4062</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.060927 </td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0721981</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0834691 </td><td style=\"text-align: right;\">        0.887479</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_5f388356</td><td>TERMINATED</td><td>172.19.2.2:4136</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0505731</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0563868</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0622004 </td><td style=\"text-align: right;\">        0.88865 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_6d6c1b59</td><td>TERMINATED</td><td>172.19.2.2:4220</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0797273</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0949112</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.110095  </td><td style=\"text-align: right;\">        0.874415</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_c65f797e</td><td>TERMINATED</td><td>172.19.2.2:4296</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0742184</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0857546</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0972908 </td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_82a633fc</td><td>TERMINATED</td><td>172.19.2.2:4379</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.116065 </td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.129645 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.143225  </td><td style=\"text-align: right;\">        0.824244</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_826f9e17</td><td>TERMINATED</td><td>172.19.2.2:4455</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0506621</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0527283</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0547944 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_7e91e652</td><td>TERMINATED</td><td>172.19.2.2:4539</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0511426</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.052896 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0546493 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_9c379fb2</td><td>TERMINATED</td><td>172.19.2.2:4615</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0505771</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526241</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0546712 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_d1c65aca</td><td>TERMINATED</td><td>172.19.2.2:4698</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.179929 </td><td style=\"text-align: right;\">  0.782482</td><td style=\"text-align: right;\">      0.182062 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.184195  </td><td style=\"text-align: right;\">        0.670537</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_02af8ae3</td><td>TERMINATED</td><td>172.19.2.2:4774</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.184704 </td><td style=\"text-align: right;\">  0.473174</td><td style=\"text-align: right;\">      0.188709 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.192714  </td><td style=\"text-align: right;\">        0.438979</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_47fe5fd4</td><td>TERMINATED</td><td>172.19.2.2:4858</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.192361 </td><td style=\"text-align: right;\">  0.348739</td><td style=\"text-align: right;\">      0.195502 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.198644  </td><td style=\"text-align: right;\">        0.33636 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_c2590652</td><td>TERMINATED</td><td>172.19.2.2:4935</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0306743</td><td style=\"text-align: right;\">  0.916936</td><td style=\"text-align: right;\">      0.0414976</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0090277 </td><td style=\"text-align: right;\">        0.96979 </td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_e41060ac</td><td>TERMINATED</td><td>172.19.2.2:5016</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0332299</td><td style=\"text-align: right;\">  0.912734</td><td style=\"text-align: right;\">      0.0402099</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0192698 </td><td style=\"text-align: right;\">        0.948754</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_458f3eff</td><td>TERMINATED</td><td>172.19.2.2:5121</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0344052</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0429152</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0173852 </td><td style=\"text-align: right;\">        0.95527 </td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_a3aa277b</td><td>TERMINATED</td><td>172.19.2.2:5215</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0319244</td><td style=\"text-align: right;\">  0.91435 </td><td style=\"text-align: right;\">      0.042559 </td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0106552 </td><td style=\"text-align: right;\">        0.968335</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_20ede2c3</td><td>TERMINATED</td><td>172.19.2.2:5303</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0492642</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526206</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.055977  </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_06ee8403</td><td>TERMINATED</td><td>172.19.2.2:5391</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0504541</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0525395</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0546249 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8740179d</td><td>TERMINATED</td><td>172.19.2.2:5479</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.165938 </td><td style=\"text-align: right;\">  0.617647</td><td style=\"text-align: right;\">      0.17098  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.176023  </td><td style=\"text-align: right;\">        0.619638</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_96617005</td><td>TERMINATED</td><td>172.19.2.2:5567</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.173896 </td><td style=\"text-align: right;\">  0.683581</td><td style=\"text-align: right;\">      0.178364 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.182833  </td><td style=\"text-align: right;\">        0.602208</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e1f9289e</td><td>TERMINATED</td><td>172.19.2.2:5637</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.180237 </td><td style=\"text-align: right;\">  0.669037</td><td style=\"text-align: right;\">      0.18407  </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.187902  </td><td style=\"text-align: right;\">        0.54027 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_9bf381ec</td><td>TERMINATED</td><td>172.19.2.2:5727</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0375738</td><td style=\"text-align: right;\">  0.908209</td><td style=\"text-align: right;\">      0.0448638</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0229937 </td><td style=\"text-align: right;\">        0.939517</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_752b526f</td><td>TERMINATED</td><td>172.19.2.2:5797</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0335976</td><td style=\"text-align: right;\">  0.906917</td><td style=\"text-align: right;\">      0.0403624</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.020068  </td><td style=\"text-align: right;\">        0.946444</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_8f4423f1</td><td>TERMINATED</td><td>172.19.2.2:5903</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0303013</td><td style=\"text-align: right;\">  0.914674</td><td style=\"text-align: right;\">      0.0413572</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00818952</td><td style=\"text-align: right;\">        0.971878</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_798b884e</td><td>TERMINATED</td><td>172.19.2.2:5973</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0363925</td><td style=\"text-align: right;\">  0.909825</td><td style=\"text-align: right;\">      0.0440184</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0211408 </td><td style=\"text-align: right;\">        0.944167</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_89a266a4</td><td>TERMINATED</td><td>172.19.2.2:6081</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0321174</td><td style=\"text-align: right;\">  0.910149</td><td style=\"text-align: right;\">      0.0432112</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00992985</td><td style=\"text-align: right;\">        0.969031</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_8a2c6b66</td><td>TERMINATED</td><td>172.19.2.2:6185</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0298463</td><td style=\"text-align: right;\">  0.915966</td><td style=\"text-align: right;\">      0.0408968</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00774546</td><td style=\"text-align: right;\">        0.973206</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_9f492ec8</td><td>TERMINATED</td><td>172.19.2.2:6291</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0292649</td><td style=\"text-align: right;\">  0.919522</td><td style=\"text-align: right;\">      0.0395046</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.00878543</td><td style=\"text-align: right;\">        0.971498</td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_0bceb13d</td><td>TERMINATED</td><td>172.19.2.2:6395</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0381824</td><td style=\"text-align: right;\">  0.909179</td><td style=\"text-align: right;\">      0.0437851</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.026977  </td><td style=\"text-align: right;\">        0.930786</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_145d0de8</td><td>TERMINATED</td><td>172.19.2.2:6491</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0397822</td><td style=\"text-align: right;\">  0.898836</td><td style=\"text-align: right;\">      0.0412599</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0427376 </td><td style=\"text-align: right;\">        0.90241 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_2e014c67</td><td>TERMINATED</td><td>172.19.2.2:6565</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0329717</td><td style=\"text-align: right;\">  0.916613</td><td style=\"text-align: right;\">      0.0398331</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0192488 </td><td style=\"text-align: right;\">        0.947646</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_6929203f</td><td>TERMINATED</td><td>172.19.2.2:6649</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0362603</td><td style=\"text-align: right;\">  0.913381</td><td style=\"text-align: right;\">      0.0418776</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0250257 </td><td style=\"text-align: right;\">        0.936069</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_c2817ead</td><td>TERMINATED</td><td>172.19.2.2:6737</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.02941  </td><td style=\"text-align: right;\">  0.920168</td><td style=\"text-align: right;\">      0.0407794</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0066714 </td><td style=\"text-align: right;\">        0.97618 </td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_d4b242b7</td><td>TERMINATED</td><td>172.19.2.2:6819</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0287138</td><td style=\"text-align: right;\">  0.9234  </td><td style=\"text-align: right;\">      0.0393177</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0075058 </td><td style=\"text-align: right;\">        0.973839</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_914959a0</td><td>TERMINATED</td><td>172.19.2.2:6946</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0280731</td><td style=\"text-align: right;\">  0.91629 </td><td style=\"text-align: right;\">      0.0387016</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00681608</td><td style=\"text-align: right;\">        0.97637 </td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_9a44fe65</td><td>TERMINATED</td><td>172.19.2.2:7029</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0502192</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0524912</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0547631 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_7e5865c1</td><td>TERMINATED</td><td>172.19.2.2:7118</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0295725</td><td style=\"text-align: right;\">  0.91435 </td><td style=\"text-align: right;\">      0.0407399</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00723747</td><td style=\"text-align: right;\">        0.972827</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_eba5b8f9</td><td>TERMINATED</td><td>172.19.2.2:7227</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0297765</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0400712</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.00918716</td><td style=\"text-align: right;\">        0.970929</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_2b6b2342</td><td>TERMINATED</td><td>172.19.2.2:7328</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0339783</td><td style=\"text-align: right;\">  0.911765</td><td style=\"text-align: right;\">      0.041056 </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0198229 </td><td style=\"text-align: right;\">        0.947045</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_9f6a9839</td><td>TERMINATED</td><td>172.19.2.2:7427</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0329649</td><td style=\"text-align: right;\">  0.913381</td><td style=\"text-align: right;\">      0.0406532</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0175884 </td><td style=\"text-align: right;\">        0.954732</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_29d71365</td><td>TERMINATED</td><td>172.19.2.2:7505</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.111515 </td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.127684 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.143853  </td><td style=\"text-align: right;\">        0.858566</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_a8a0f427</td><td>TERMINATED</td><td>172.19.2.2:7594</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0358254</td><td style=\"text-align: right;\">  0.906917</td><td style=\"text-align: right;\">      0.0404488</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0265785 </td><td style=\"text-align: right;\">        0.933664</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_01a33d3f</td><td>TERMINATED</td><td>172.19.2.2:7674</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0353014</td><td style=\"text-align: right;\">  0.910795</td><td style=\"text-align: right;\">      0.045574 </td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.014756  </td><td style=\"text-align: right;\">        0.960142</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_848bacfa</td><td>TERMINATED</td><td>172.19.2.2:7770</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0395372</td><td style=\"text-align: right;\">  0.912411</td><td style=\"text-align: right;\">      0.046006 </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0265996 </td><td style=\"text-align: right;\">        0.933981</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_82ab1f61</td><td>TERMINATED</td><td>172.19.2.2:7875</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0331978</td><td style=\"text-align: right;\">  0.912088</td><td style=\"text-align: right;\">      0.0408057</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0179821 </td><td style=\"text-align: right;\">        0.952708</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_02a53601</td><td>TERMINATED</td><td>172.19.2.2:7955</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0535997</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0608401</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0680806 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_b51b1859</td><td>TERMINATED</td><td>172.19.2.2:8044</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0547537</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0626595</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0705653 </td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_d65ce0c6</td><td>TERMINATED</td><td>172.19.2.2:8123</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0506597</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0527322</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0548048 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_db7fb288</td><td>TERMINATED</td><td>172.19.2.2:8203</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0498755</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.054428 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0589805 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_bc37b902</td><td>TERMINATED</td><td>172.19.2.2:8282</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0500147</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0538655</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0577163 </td><td style=\"text-align: right;\">        0.888523</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_efcb6b91</td><td>TERMINATED</td><td>172.19.2.2:8363</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0481083</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0519877</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.055867  </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_ebf013a7</td><td>TERMINATED</td><td>172.19.2.2:8442</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0404782</td><td style=\"text-align: right;\">  0.904008</td><td style=\"text-align: right;\">      0.0425597</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0446412 </td><td style=\"text-align: right;\">        0.896368</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_0ba1ccdb</td><td>TERMINATED</td><td>172.19.2.2:8523</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.027122 </td><td style=\"text-align: right;\">  0.923723</td><td style=\"text-align: right;\">      0.0371339</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.00709802</td><td style=\"text-align: right;\">        0.975452</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_40ecab1f</td><td>TERMINATED</td><td>172.19.2.2:8602</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0503289</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0523273</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0543257 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_ff7091ec</td><td>TERMINATED</td><td>172.19.2.2:8691</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0335598</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0452415</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.0101963 </td><td style=\"text-align: right;\">        0.970391</td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_f5ee23f8</td><td>TERMINATED</td><td>172.19.2.2:8805</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0315285</td><td style=\"text-align: right;\">  0.920814</td><td style=\"text-align: right;\">      0.0412147</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">  0.012156  </td><td style=\"text-align: right;\">        0.965298</td><td style=\"text-align: right;\">                     9</td></tr>\n",
       "<tr><td>train_fn_277dbe25</td><td>TERMINATED</td><td>172.19.2.2:8895</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0479035</td><td style=\"text-align: right;\">  0.89819 </td><td style=\"text-align: right;\">      0.0499326</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0519618 </td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_5420120b</td><td>TERMINATED</td><td>172.19.2.2:8984</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0357222</td><td style=\"text-align: right;\">  0.905947</td><td style=\"text-align: right;\">      0.0438642</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.019438  </td><td style=\"text-align: right;\">        0.949639</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_5c80e3d6</td><td>TERMINATED</td><td>172.19.2.2:9069</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.125828 </td><td style=\"text-align: right;\">  0.833872</td><td style=\"text-align: right;\">      0.141865 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.157902  </td><td style=\"text-align: right;\">        0.763001</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 13:48:59,034\tINFO worker.py:1753 -- Started a local Ray instance.\n",
      "2024-11-29 13:49:00,153\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-11-29 13:49:00,157\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2024-11-29 13:49:00,198] A new study created in memory with name: optuna\n",
      "\u001b[36m(train_fn pid=341)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=341)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  early_stopping_epoch</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_01a33d3f</td><td style=\"text-align: right;\">  0.910795</td><td style=\"text-align: right;\">      0.045574 </td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0353014</td><td style=\"text-align: right;\">        0.960142</td><td style=\"text-align: right;\">  0.014756  </td></tr>\n",
       "<tr><td>train_fn_01ed111c</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0545136</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0508442</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0581831 </td></tr>\n",
       "<tr><td>train_fn_02a53601</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0608401</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0535997</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0680806 </td></tr>\n",
       "<tr><td>train_fn_02af8ae3</td><td style=\"text-align: right;\">  0.473174</td><td style=\"text-align: right;\">      0.188709 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.184704 </td><td style=\"text-align: right;\">        0.438979</td><td style=\"text-align: right;\">  0.192714  </td></tr>\n",
       "<tr><td>train_fn_06ee8403</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0525395</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0504541</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0546249 </td></tr>\n",
       "<tr><td>train_fn_0ba1ccdb</td><td style=\"text-align: right;\">  0.923723</td><td style=\"text-align: right;\">      0.0371339</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.027122 </td><td style=\"text-align: right;\">        0.975452</td><td style=\"text-align: right;\">  0.00709802</td></tr>\n",
       "<tr><td>train_fn_0bceb13d</td><td style=\"text-align: right;\">  0.909179</td><td style=\"text-align: right;\">      0.0437851</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0381824</td><td style=\"text-align: right;\">        0.930786</td><td style=\"text-align: right;\">  0.026977  </td></tr>\n",
       "<tr><td>train_fn_109ac776</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526129</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0505464</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0546793 </td></tr>\n",
       "<tr><td>train_fn_145d0de8</td><td style=\"text-align: right;\">  0.898836</td><td style=\"text-align: right;\">      0.0412599</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0397822</td><td style=\"text-align: right;\">        0.90241 </td><td style=\"text-align: right;\">  0.0427376 </td></tr>\n",
       "<tr><td>train_fn_16b7cbd1</td><td style=\"text-align: right;\">  0.918552</td><td style=\"text-align: right;\">      0.0378706</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0323154</td><td style=\"text-align: right;\">        0.943945</td><td style=\"text-align: right;\">  0.0212051 </td></tr>\n",
       "<tr><td>train_fn_20ede2c3</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526206</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0492642</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.055977  </td></tr>\n",
       "<tr><td>train_fn_277dbe25</td><td style=\"text-align: right;\">  0.89819 </td><td style=\"text-align: right;\">      0.0499326</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0479035</td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">  0.0519618 </td></tr>\n",
       "<tr><td>train_fn_29835677</td><td style=\"text-align: right;\">  0.911765</td><td style=\"text-align: right;\">      0.0439654</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0334814</td><td style=\"text-align: right;\">        0.963843</td><td style=\"text-align: right;\">  0.0125135 </td></tr>\n",
       "<tr><td>train_fn_29d71365</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.127684 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.111515 </td><td style=\"text-align: right;\">        0.858566</td><td style=\"text-align: right;\">  0.143853  </td></tr>\n",
       "<tr><td>train_fn_2a405bc4</td><td style=\"text-align: right;\">  0.916936</td><td style=\"text-align: right;\">      0.0410155</td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0299406</td><td style=\"text-align: right;\">        0.973554</td><td style=\"text-align: right;\">  0.00779094</td></tr>\n",
       "<tr><td>train_fn_2b6b2342</td><td style=\"text-align: right;\">  0.911765</td><td style=\"text-align: right;\">      0.041056 </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0339783</td><td style=\"text-align: right;\">        0.947045</td><td style=\"text-align: right;\">  0.0198229 </td></tr>\n",
       "<tr><td>train_fn_2e014c67</td><td style=\"text-align: right;\">  0.916613</td><td style=\"text-align: right;\">      0.0398331</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0329717</td><td style=\"text-align: right;\">        0.947646</td><td style=\"text-align: right;\">  0.0192488 </td></tr>\n",
       "<tr><td>train_fn_34225abb</td><td style=\"text-align: right;\">  0.738203</td><td style=\"text-align: right;\">      0.178973 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.175278 </td><td style=\"text-align: right;\">        0.655479</td><td style=\"text-align: right;\">  0.182668  </td></tr>\n",
       "<tr><td>train_fn_389be36f</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0527196</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0504594</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0549799 </td></tr>\n",
       "<tr><td>train_fn_3c6e9283</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526491</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0502097</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0550886 </td></tr>\n",
       "<tr><td>train_fn_40ecab1f</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0523273</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0503289</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0543257 </td></tr>\n",
       "<tr><td>train_fn_458f3eff</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0429152</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0344052</td><td style=\"text-align: right;\">        0.95527 </td><td style=\"text-align: right;\">  0.0173852 </td></tr>\n",
       "<tr><td>train_fn_46b518e6</td><td style=\"text-align: right;\">  0.905624</td><td style=\"text-align: right;\">      0.0506012</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.037888 </td><td style=\"text-align: right;\">        0.964887</td><td style=\"text-align: right;\">  0.0124617 </td></tr>\n",
       "<tr><td>train_fn_47fe5fd4</td><td style=\"text-align: right;\">  0.348739</td><td style=\"text-align: right;\">      0.195502 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.192361 </td><td style=\"text-align: right;\">        0.33636 </td><td style=\"text-align: right;\">  0.198644  </td></tr>\n",
       "<tr><td>train_fn_48613c3c</td><td style=\"text-align: right;\">  0.910795</td><td style=\"text-align: right;\">      0.0470659</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0385429</td><td style=\"text-align: right;\">        0.945464</td><td style=\"text-align: right;\">  0.0214969 </td></tr>\n",
       "<tr><td>train_fn_49e25293</td><td style=\"text-align: right;\">  0.910795</td><td style=\"text-align: right;\">      0.043279 </td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0333446</td><td style=\"text-align: right;\">        0.962672</td><td style=\"text-align: right;\">  0.0134758 </td></tr>\n",
       "<tr><td>train_fn_503ad4c5</td><td style=\"text-align: right;\">  0.919845</td><td style=\"text-align: right;\">      0.0376601</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0322252</td><td style=\"text-align: right;\">        0.94404 </td><td style=\"text-align: right;\">  0.0213554 </td></tr>\n",
       "<tr><td>train_fn_53fb32b2</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0721981</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.060927 </td><td style=\"text-align: right;\">        0.887479</td><td style=\"text-align: right;\">  0.0834691 </td></tr>\n",
       "<tr><td>train_fn_5420120b</td><td style=\"text-align: right;\">  0.905947</td><td style=\"text-align: right;\">      0.0438642</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0357222</td><td style=\"text-align: right;\">        0.949639</td><td style=\"text-align: right;\">  0.019438  </td></tr>\n",
       "<tr><td>train_fn_552a4070</td><td style=\"text-align: right;\">  0.902715</td><td style=\"text-align: right;\">      0.0446231</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0369745</td><td style=\"text-align: right;\">        0.941193</td><td style=\"text-align: right;\">  0.0216771 </td></tr>\n",
       "<tr><td>train_fn_5c80e3d6</td><td style=\"text-align: right;\">  0.833872</td><td style=\"text-align: right;\">      0.141865 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.125828 </td><td style=\"text-align: right;\">        0.763001</td><td style=\"text-align: right;\">  0.157902  </td></tr>\n",
       "<tr><td>train_fn_5ccc8042</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0529625</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0511788</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0547463 </td></tr>\n",
       "<tr><td>train_fn_5f388356</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0563868</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0505731</td><td style=\"text-align: right;\">        0.88865 </td><td style=\"text-align: right;\">  0.0622004 </td></tr>\n",
       "<tr><td>train_fn_6681de2f</td><td style=\"text-align: right;\">  0.917906</td><td style=\"text-align: right;\">      0.0385028</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.032462 </td><td style=\"text-align: right;\">        0.945274</td><td style=\"text-align: right;\">  0.0203802 </td></tr>\n",
       "<tr><td>train_fn_6929203f</td><td style=\"text-align: right;\">  0.913381</td><td style=\"text-align: right;\">      0.0418776</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0362603</td><td style=\"text-align: right;\">        0.936069</td><td style=\"text-align: right;\">  0.0250257 </td></tr>\n",
       "<tr><td>train_fn_698a65bc</td><td style=\"text-align: right;\">  0.917582</td><td style=\"text-align: right;\">      0.0402177</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0303713</td><td style=\"text-align: right;\">        0.970043</td><td style=\"text-align: right;\">  0.0106785 </td></tr>\n",
       "<tr><td>train_fn_69e9fd94</td><td style=\"text-align: right;\">  0.460246</td><td style=\"text-align: right;\">      0.189404 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.183209 </td><td style=\"text-align: right;\">        0.387859</td><td style=\"text-align: right;\">  0.195599  </td></tr>\n",
       "<tr><td>train_fn_6c3e629c</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0855704</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0758959</td><td style=\"text-align: right;\">        0.884474</td><td style=\"text-align: right;\">  0.095245  </td></tr>\n",
       "<tr><td>train_fn_6cc479ef</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0678617</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0582743</td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">  0.0774491 </td></tr>\n",
       "<tr><td>train_fn_6d6c1b59</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0949112</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0797273</td><td style=\"text-align: right;\">        0.874415</td><td style=\"text-align: right;\">  0.110095  </td></tr>\n",
       "<tr><td>train_fn_752b526f</td><td style=\"text-align: right;\">  0.906917</td><td style=\"text-align: right;\">      0.0403624</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0335976</td><td style=\"text-align: right;\">        0.946444</td><td style=\"text-align: right;\">  0.020068  </td></tr>\n",
       "<tr><td>train_fn_798b884e</td><td style=\"text-align: right;\">  0.909825</td><td style=\"text-align: right;\">      0.0440184</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0363925</td><td style=\"text-align: right;\">        0.944167</td><td style=\"text-align: right;\">  0.0211408 </td></tr>\n",
       "<tr><td>train_fn_7e5865c1</td><td style=\"text-align: right;\">  0.91435 </td><td style=\"text-align: right;\">      0.0407399</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0295725</td><td style=\"text-align: right;\">        0.972827</td><td style=\"text-align: right;\">  0.00723747</td></tr>\n",
       "<tr><td>train_fn_7e91e652</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.052896 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0511426</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0546493 </td></tr>\n",
       "<tr><td>train_fn_81042903</td><td style=\"text-align: right;\">  0.912411</td><td style=\"text-align: right;\">      0.0437104</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.033786 </td><td style=\"text-align: right;\">        0.961249</td><td style=\"text-align: right;\">  0.0139371 </td></tr>\n",
       "<tr><td>train_fn_826f9e17</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0527283</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0506621</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0547944 </td></tr>\n",
       "<tr><td>train_fn_82a633fc</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.129645 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.116065 </td><td style=\"text-align: right;\">        0.824244</td><td style=\"text-align: right;\">  0.143225  </td></tr>\n",
       "<tr><td>train_fn_82ab1f61</td><td style=\"text-align: right;\">  0.912088</td><td style=\"text-align: right;\">      0.0408057</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0331978</td><td style=\"text-align: right;\">        0.952708</td><td style=\"text-align: right;\">  0.0179821 </td></tr>\n",
       "<tr><td>train_fn_848bacfa</td><td style=\"text-align: right;\">  0.912411</td><td style=\"text-align: right;\">      0.046006 </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0395372</td><td style=\"text-align: right;\">        0.933981</td><td style=\"text-align: right;\">  0.0265996 </td></tr>\n",
       "<tr><td>train_fn_8740179d</td><td style=\"text-align: right;\">  0.617647</td><td style=\"text-align: right;\">      0.17098  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.165938 </td><td style=\"text-align: right;\">        0.619638</td><td style=\"text-align: right;\">  0.176023  </td></tr>\n",
       "<tr><td>train_fn_89a266a4</td><td style=\"text-align: right;\">  0.910149</td><td style=\"text-align: right;\">      0.0432112</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0321174</td><td style=\"text-align: right;\">        0.969031</td><td style=\"text-align: right;\">  0.00992985</td></tr>\n",
       "<tr><td>train_fn_8a2c6b66</td><td style=\"text-align: right;\">  0.915966</td><td style=\"text-align: right;\">      0.0408968</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0298463</td><td style=\"text-align: right;\">        0.973206</td><td style=\"text-align: right;\">  0.00774546</td></tr>\n",
       "<tr><td>train_fn_8f4423f1</td><td style=\"text-align: right;\">  0.914674</td><td style=\"text-align: right;\">      0.0413572</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0303013</td><td style=\"text-align: right;\">        0.971878</td><td style=\"text-align: right;\">  0.00818952</td></tr>\n",
       "<tr><td>train_fn_90248808</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0522138</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0487009</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0557266 </td></tr>\n",
       "<tr><td>train_fn_908f2e6c</td><td style=\"text-align: right;\">  0.912088</td><td style=\"text-align: right;\">      0.0455325</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0368132</td><td style=\"text-align: right;\">        0.948659</td><td style=\"text-align: right;\">  0.0193747 </td></tr>\n",
       "<tr><td>train_fn_914959a0</td><td style=\"text-align: right;\">  0.91629 </td><td style=\"text-align: right;\">      0.0387016</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0280731</td><td style=\"text-align: right;\">        0.97637 </td><td style=\"text-align: right;\">  0.00681608</td></tr>\n",
       "<tr><td>train_fn_92270e1f</td><td style=\"text-align: right;\">  0.911765</td><td style=\"text-align: right;\">      0.0390837</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0332854</td><td style=\"text-align: right;\">        0.942901</td><td style=\"text-align: right;\">  0.0216887 </td></tr>\n",
       "<tr><td>train_fn_95f7641c</td><td style=\"text-align: right;\">  0.907563</td><td style=\"text-align: right;\">      0.0417099</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0353802</td><td style=\"text-align: right;\">        0.941351</td><td style=\"text-align: right;\">  0.0227207 </td></tr>\n",
       "<tr><td>train_fn_96617005</td><td style=\"text-align: right;\">  0.683581</td><td style=\"text-align: right;\">      0.178364 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.173896 </td><td style=\"text-align: right;\">        0.602208</td><td style=\"text-align: right;\">  0.182833  </td></tr>\n",
       "<tr><td>train_fn_9a44fe65</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0524912</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0502192</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0547631 </td></tr>\n",
       "<tr><td>train_fn_9bf381ec</td><td style=\"text-align: right;\">  0.908209</td><td style=\"text-align: right;\">      0.0448638</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0375738</td><td style=\"text-align: right;\">        0.939517</td><td style=\"text-align: right;\">  0.0229937 </td></tr>\n",
       "<tr><td>train_fn_9c022d45</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0542905</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0503695</td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">  0.0582115 </td></tr>\n",
       "<tr><td>train_fn_9c3780ff</td><td style=\"text-align: right;\">  0.904977</td><td style=\"text-align: right;\">      0.0473455</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0400354</td><td style=\"text-align: right;\">        0.935151</td><td style=\"text-align: right;\">  0.0254151 </td></tr>\n",
       "<tr><td>train_fn_9c379fb2</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0526241</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0505771</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0546712 </td></tr>\n",
       "<tr><td>train_fn_9f492ec8</td><td style=\"text-align: right;\">  0.919522</td><td style=\"text-align: right;\">      0.0395046</td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0292649</td><td style=\"text-align: right;\">        0.971498</td><td style=\"text-align: right;\">  0.00878543</td></tr>\n",
       "<tr><td>train_fn_9f6a9839</td><td style=\"text-align: right;\">  0.913381</td><td style=\"text-align: right;\">      0.0406532</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0329649</td><td style=\"text-align: right;\">        0.954732</td><td style=\"text-align: right;\">  0.0175884 </td></tr>\n",
       "<tr><td>train_fn_a3aa277b</td><td style=\"text-align: right;\">  0.91435 </td><td style=\"text-align: right;\">      0.042559 </td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0319244</td><td style=\"text-align: right;\">        0.968335</td><td style=\"text-align: right;\">  0.0106552 </td></tr>\n",
       "<tr><td>train_fn_a8a0f427</td><td style=\"text-align: right;\">  0.906917</td><td style=\"text-align: right;\">      0.0404488</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0358254</td><td style=\"text-align: right;\">        0.933664</td><td style=\"text-align: right;\">  0.0265785 </td></tr>\n",
       "<tr><td>train_fn_ae2bc71f</td><td style=\"text-align: right;\">  0.911441</td><td style=\"text-align: right;\">      0.0417144</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0373205</td><td style=\"text-align: right;\">        0.929236</td><td style=\"text-align: right;\">  0.0285326 </td></tr>\n",
       "<tr><td>train_fn_af324419</td><td style=\"text-align: right;\">  0.915966</td><td style=\"text-align: right;\">      0.0423468</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0314786</td><td style=\"text-align: right;\">        0.969663</td><td style=\"text-align: right;\">  0.00974222</td></tr>\n",
       "<tr><td>train_fn_b20e2045</td><td style=\"text-align: right;\">  0.913058</td><td style=\"text-align: right;\">      0.0447022</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0366559</td><td style=\"text-align: right;\">        0.946634</td><td style=\"text-align: right;\">  0.0205632 </td></tr>\n",
       "<tr><td>train_fn_b51b1859</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0626595</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0547537</td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">  0.0705653 </td></tr>\n",
       "<tr><td>train_fn_bc37b902</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0538655</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0500147</td><td style=\"text-align: right;\">        0.888523</td><td style=\"text-align: right;\">  0.0577163 </td></tr>\n",
       "<tr><td>train_fn_c2590652</td><td style=\"text-align: right;\">  0.916936</td><td style=\"text-align: right;\">      0.0414976</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0306743</td><td style=\"text-align: right;\">        0.96979 </td><td style=\"text-align: right;\">  0.0090277 </td></tr>\n",
       "<tr><td>train_fn_c2817ead</td><td style=\"text-align: right;\">  0.920168</td><td style=\"text-align: right;\">      0.0407794</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.02941  </td><td style=\"text-align: right;\">        0.97618 </td><td style=\"text-align: right;\">  0.0066714 </td></tr>\n",
       "<tr><td>train_fn_c65f797e</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0857546</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0742184</td><td style=\"text-align: right;\">        0.888587</td><td style=\"text-align: right;\">  0.0972908 </td></tr>\n",
       "<tr><td>train_fn_cc7b2bbd</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0531873</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0518973</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0544773 </td></tr>\n",
       "<tr><td>train_fn_d1c65aca</td><td style=\"text-align: right;\">  0.782482</td><td style=\"text-align: right;\">      0.182062 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.179929 </td><td style=\"text-align: right;\">        0.670537</td><td style=\"text-align: right;\">  0.184195  </td></tr>\n",
       "<tr><td>train_fn_d4b242b7</td><td style=\"text-align: right;\">  0.9234  </td><td style=\"text-align: right;\">      0.0393177</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0287138</td><td style=\"text-align: right;\">        0.973839</td><td style=\"text-align: right;\">  0.0075058 </td></tr>\n",
       "<tr><td>train_fn_d4ef5ae7</td><td style=\"text-align: right;\">  0.914027</td><td style=\"text-align: right;\">      0.04111  </td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0300592</td><td style=\"text-align: right;\">        0.973175</td><td style=\"text-align: right;\">  0.00795752</td></tr>\n",
       "<tr><td>train_fn_d65ce0c6</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0527322</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0506597</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0548048 </td></tr>\n",
       "<tr><td>train_fn_da2e9004</td><td style=\"text-align: right;\">  0.916613</td><td style=\"text-align: right;\">      0.0417494</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0342558</td><td style=\"text-align: right;\">        0.948248</td><td style=\"text-align: right;\">  0.0192687 </td></tr>\n",
       "<tr><td>train_fn_db7fb288</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.054428 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0498755</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0589805 </td></tr>\n",
       "<tr><td>train_fn_de540f6e</td><td style=\"text-align: right;\">  0.913381</td><td style=\"text-align: right;\">      0.0378744</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0350333</td><td style=\"text-align: right;\">        0.928255</td><td style=\"text-align: right;\">  0.029351  </td></tr>\n",
       "<tr><td>train_fn_e1f9289e</td><td style=\"text-align: right;\">  0.669037</td><td style=\"text-align: right;\">      0.18407  </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.180237 </td><td style=\"text-align: right;\">        0.54027 </td><td style=\"text-align: right;\">  0.187902  </td></tr>\n",
       "<tr><td>train_fn_e33e54ea</td><td style=\"text-align: right;\">  0.916613</td><td style=\"text-align: right;\">      0.0411225</td><td style=\"text-align: right;\">                     9</td><td style=\"text-align: right;\">0.0319665</td><td style=\"text-align: right;\">        0.960743</td><td style=\"text-align: right;\">  0.0136544 </td></tr>\n",
       "<tr><td>train_fn_e41060ac</td><td style=\"text-align: right;\">  0.912734</td><td style=\"text-align: right;\">      0.0402099</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0332299</td><td style=\"text-align: right;\">        0.948754</td><td style=\"text-align: right;\">  0.0192698 </td></tr>\n",
       "<tr><td>train_fn_eba5b8f9</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0400712</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0297765</td><td style=\"text-align: right;\">        0.970929</td><td style=\"text-align: right;\">  0.00918716</td></tr>\n",
       "<tr><td>train_fn_ebf013a7</td><td style=\"text-align: right;\">  0.904008</td><td style=\"text-align: right;\">      0.0425597</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0404782</td><td style=\"text-align: right;\">        0.896368</td><td style=\"text-align: right;\">  0.0446412 </td></tr>\n",
       "<tr><td>train_fn_efcb6b91</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0519877</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0481083</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.055867  </td></tr>\n",
       "<tr><td>train_fn_f4f11c61</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.052501 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.050289 </td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.054713  </td></tr>\n",
       "<tr><td>train_fn_f54a9490</td><td style=\"text-align: right;\">  0.902069</td><td style=\"text-align: right;\">      0.0503268</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0410251</td><td style=\"text-align: right;\">        0.941858</td><td style=\"text-align: right;\">  0.0224218 </td></tr>\n",
       "<tr><td>train_fn_f5ee23f8</td><td style=\"text-align: right;\">  0.920814</td><td style=\"text-align: right;\">      0.0412147</td><td style=\"text-align: right;\">                     9</td><td style=\"text-align: right;\">0.0315285</td><td style=\"text-align: right;\">        0.965298</td><td style=\"text-align: right;\">  0.012156  </td></tr>\n",
       "<tr><td>train_fn_f727ef99</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0907037</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0796937</td><td style=\"text-align: right;\">        0.887701</td><td style=\"text-align: right;\">  0.101714  </td></tr>\n",
       "<tr><td>train_fn_f8e72998</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0416037</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0300058</td><td style=\"text-align: right;\">        0.974756</td><td style=\"text-align: right;\">  0.00681015</td></tr>\n",
       "<tr><td>train_fn_f90ab6f6</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0524108</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0503676</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0544539 </td></tr>\n",
       "<tr><td>train_fn_fb6accfc</td><td style=\"text-align: right;\">  0.896251</td><td style=\"text-align: right;\">      0.0542055</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0504637</td><td style=\"text-align: right;\">        0.888618</td><td style=\"text-align: right;\">  0.0579473 </td></tr>\n",
       "<tr><td>train_fn_fe5b5527</td><td style=\"text-align: right;\">  0.91532 </td><td style=\"text-align: right;\">      0.0430757</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0313541</td><td style=\"text-align: right;\">        0.974219</td><td style=\"text-align: right;\">  0.00791094</td></tr>\n",
       "<tr><td>train_fn_fed786e8</td><td style=\"text-align: right;\">  0.915643</td><td style=\"text-align: right;\">      0.0365665</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.033461 </td><td style=\"text-align: right;\">        0.931672</td><td style=\"text-align: right;\">  0.0272499 </td></tr>\n",
       "<tr><td>train_fn_ff7091ec</td><td style=\"text-align: right;\">  0.914997</td><td style=\"text-align: right;\">      0.0452415</td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0335598</td><td style=\"text-align: right;\">        0.970391</td><td style=\"text-align: right;\">  0.0101963 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=482)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_fn pid=482)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_fn pid=560)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=560)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=641)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=641)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=728)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=728)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=829)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=829)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=922)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=922)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1001)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1001)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1098)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1098)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1169)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1169)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1269)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1269)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1346)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1346)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1428)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1428)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1556)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1556)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1631)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1631)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1766)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1766)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1802)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1802)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1898)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1898)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1997)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1997)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2090)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2090)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2196)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2196)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2278)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2278)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2372)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2372)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2448)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2448)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2548)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2548)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2648)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2648)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2758)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2758)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2857)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2857)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2968)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2968)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3052)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3052)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3140)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3140)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3222)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3222)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3307)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3307)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3390)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3390)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3478)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3478)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3561)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3561)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3649)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3649)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3731)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3731)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3820)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3820)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3902)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3902)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3978)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3978)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4062)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4062)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4136)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4136)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4220)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4220)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4296)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4296)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4379)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4379)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4455)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4455)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4539)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4539)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4615)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4615)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4698)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4698)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4774)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4774)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4858)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4858)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4935)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4935)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5016)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5016)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5121)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5121)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5215)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5215)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5303)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5303)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5391)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5391)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5479)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5479)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5567)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5567)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5637)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5637)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5727)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5727)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5797)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5797)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5903)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5903)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5973)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5973)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6081)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6081)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6185)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6185)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6291)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6291)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6395)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6395)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6491)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6491)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6565)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6565)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6649)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6649)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6737)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6737)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6819)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6819)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6946)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6946)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7029)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7029)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7118)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7118)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7227)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7227)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7328)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7328)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7427)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7427)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7505)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7505)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7594)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7594)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7674)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7674)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7770)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7770)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7875)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7875)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7955)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7955)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8044)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8044)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8123)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8123)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8203)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8203)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8282)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8282)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8363)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8363)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8442)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8442)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8523)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8523)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8602)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8602)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8691)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8691)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8805)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8805)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8895)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8895)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8984)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8984)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=9069)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=9069)\u001b[0m   warnings.warn(\n",
      "2024-11-29 18:43:42,092\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_fn_2024-11-29_13-49-00' in 0.0488s.\n",
      "2024-11-29 18:43:42,123\tINFO tune.py:1041 -- Total run time: 17681.97 seconds (17674.70 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /kaggle/working/tensorboard_logs\n",
    "\n",
    "\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "#     print_intermediate_tables=False\n",
    "# )\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False,\n",
    ")\n",
    "\n",
    "# tuner = tune.run(\n",
    "#     train_fn,\n",
    "#     config=search_space,\n",
    "#     num_samples=25,\n",
    "#     progress_reporter=reporter,\n",
    "#     resources_per_trial={\"cpu\": 4, \"gpu\": 2},  # Adjust resources as needed\n",
    "#     scheduler=ASHAScheduler(\n",
    "#         metric=\"accuracy\",\n",
    "#         mode=\"max\",\n",
    "#         max_t=15,\n",
    "#         grace_period=1,\n",
    "#         reduction_factor=2\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# reporter = TensorBoardReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"training_iteration\", \"train_loss\", \"train_accuracy\"]\n",
    "# )\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    train_fn,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    search_alg=optuna_search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891fecf",
   "metadata": {
    "papermill": {
     "duration": 0.051568,
     "end_time": "2024-11-29T18:43:42.334071",
     "exception": false,
     "start_time": "2024-11-29T18:43:42.282503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "331944d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T18:43:42.390717Z",
     "iopub.status.busy": "2024-11-29T18:43:42.389732Z",
     "iopub.status.idle": "2024-11-29T18:43:42.397110Z",
     "shell.execute_reply": "2024-11-29T18:43:42.396298Z"
    },
    "papermill": {
     "duration": 0.03624,
     "end_time": "2024-11-29T18:43:42.398770",
     "exception": false,
     "start_time": "2024-11-29T18:43:42.362530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'dropout_rate': 0.27, 'batch_size': 32, 'weight_decay': 2e-06, 'lr': 5e-05, 'epochs': 12}\n",
      "Best trial final validation loss: 0.02712196870041745\n",
      "Best trial final validation accuracy: 0.9237233354880414\n",
      "Best trial final training loss: 0.007098021731764393\n",
      "Best trial final training accuracy: 0.975452359863343\n",
      "Best trial final custom_metric: 0.03713394218474397\n",
      "Best trial final Early Stopping Epoch: 12\n",
      "NOTE: Both the accuracy are based on converting values > 0.5 to 1 and values < 0.5 to 0, hence, rely on the loss, MSE here.\n"
     ]
    }
   ],
   "source": [
    "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "# best_checkpoint_dir = best_trial.checkpoint.value\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "print(f\"Best trial final training loss: {best_trial.last_result['train_loss']}\")\n",
    "print(f\"Best trial final training accuracy: {best_trial.last_result['train_accuracy']}\")\n",
    "print(f\"Best trial final custom_metric: {best_trial.last_result['custom_metric']}\")\n",
    "print(f\"Best trial final Early Stopping Epoch: {best_trial.last_result['early_stopping_epoch']}\")\n",
    "print(f\"NOTE: Both the accuracy are based on converting values > 0.5 to 1 and values < 0.5 to 0, hence, rely on the loss, MSE here.\")\n",
    "\n",
    "\n",
    "# print(\"\\nModel Parameters:\")\n",
    "# for name, param in best_model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffc20b",
   "metadata": {
    "papermill": {
     "duration": 0.02591,
     "end_time": "2024-11-29T18:43:42.450862",
     "exception": false,
     "start_time": "2024-11-29T18:43:42.424952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Train the Model with the Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e41ac48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T18:43:42.505118Z",
     "iopub.status.busy": "2024-11-29T18:43:42.504790Z",
     "iopub.status.idle": "2024-11-29T18:43:42.520340Z",
     "shell.execute_reply": "2024-11-29T18:43:42.519734Z"
    },
    "papermill": {
     "duration": 0.044895,
     "end_time": "2024-11-29T18:43:42.521830",
     "exception": false,
     "start_time": "2024-11-29T18:43:42.476935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_best_model(config, device, train_dataset = train_ds_pd, val_dataset = validation_ds_pd):\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Tuning Model with:\n",
    "    {config}\n",
    "    \"\"\")\n",
    "    \n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = EmotionModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=7,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    ## DATASET ENCODING\n",
    "    \n",
    "    train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_path = None\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    \n",
    "    ### Running for config epochs +patience+1 for introducing noise. \n",
    "    \n",
    "    for epoch in range((config[\"epochs\"]+patience+1)):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "            correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "                correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "        checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "#         # Save the best model based on validation loss\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             best_model_path = checkpoint_path\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_since_improvement = 0\n",
    "            best_model_path = checkpoint_path\n",
    "            # Save the model checkpoint with the best validation loss\n",
    "#             checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config  # Save configuration\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            print(f\"Best Model Epoch Saved: {epoch - patience+1}\")\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "#         if epochs_since_improvement >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "#             print(f\"Best Model Epoch Saved: {epoch - patience}\")\n",
    "#             break\n",
    "            \n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)        \n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        print(f\"\"\"\n",
    "        Validation Loss: {avg_val_loss},\n",
    "        Training Loss: {avg_train_loss},\n",
    "        Argmax Binary Validation Accuracy: {val_accuracy},\n",
    "        Argmax Binary Training Accuracy: {train_accuracy},\n",
    "        Custom Metric: {custom_metric},\n",
    "        Epochs: {epoch+1}\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"Best Validation Loss: {best_val_loss}, Best Validation accuracy: {val_accuracy}\")\n",
    "    \n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9955c3",
   "metadata": {
    "papermill": {
     "duration": 0.025975,
     "end_time": "2024-11-29T18:43:42.574238",
     "exception": false,
     "start_time": "2024-11-29T18:43:42.548263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BEST MODEL AFTER FINAL TRAINING & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58c7ec39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T18:43:42.628759Z",
     "iopub.status.busy": "2024-11-29T18:43:42.628512Z",
     "iopub.status.idle": "2024-11-29T19:00:59.477554Z",
     "shell.execute_reply": "2024-11-29T19:00:59.476663Z"
    },
    "papermill": {
     "duration": 1036.905708,
     "end_time": "2024-11-29T19:00:59.507025",
     "exception": false,
     "start_time": "2024-11-29T18:43:42.601317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Tuning Model with:\n",
      "    {'dropout_rate': 0.27, 'batch_size': 32, 'weight_decay': 2e-06, 'lr': 5e-05, 'epochs': 12}\n",
      "    \n",
      "\n",
      "        Validation Loss: 0.04935679371867861,\n",
      "        Training Loss: 0.06670709872539614,\n",
      "        Argmax Binary Validation Accuracy: 0.8962508080155139,\n",
      "        Argmax Binary Training Accuracy: 0.8784638744780463,\n",
      "        Custom Metric: 0.058031946222037376,\n",
      "        Epochs: 1\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.04059630020388535,\n",
      "        Training Loss: 0.052316734698456775,\n",
      "        Argmax Binary Validation Accuracy: 0.9033613445378151,\n",
      "        Argmax Binary Training Accuracy: 0.8883335442237125,\n",
      "        Custom Metric: 0.04645651745117106,\n",
      "        Epochs: 2\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03736577129789761,\n",
      "        Training Loss: 0.03861217776602003,\n",
      "        Argmax Binary Validation Accuracy: 0.9124111182934712,\n",
      "        Argmax Binary Training Accuracy: 0.9083892192838162,\n",
      "        Custom Metric: 0.03798897453195882,\n",
      "        Epochs: 3\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.04248645236449582,\n",
      "        Training Loss: 0.030159671043216343,\n",
      "        Argmax Binary Validation Accuracy: 0.9010989010989011,\n",
      "        Argmax Binary Training Accuracy: 0.9241110970517525,\n",
      "        Custom Metric: 0.048649843025135565,\n",
      "        Epochs: 4\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.036504060828260014,\n",
      "        Training Loss: 0.02436392218477919,\n",
      "        Argmax Binary Validation Accuracy: 0.9072398190045249,\n",
      "        Argmax Binary Training Accuracy: 0.9361002151081868,\n",
      "        Custom Metric: 0.042574130150000424,\n",
      "        Epochs: 5\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03418291453272104,\n",
      "        Training Loss: 0.020710302811955482,\n",
      "        Argmax Binary Validation Accuracy: 0.9111182934712346,\n",
      "        Argmax Binary Training Accuracy: 0.9461913197519929,\n",
      "        Custom Metric: 0.04091922039310382,\n",
      "        Epochs: 6\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03831439305629049,\n",
      "        Training Loss: 0.016622903196036185,\n",
      "        Argmax Binary Validation Accuracy: 0.9069166127989657,\n",
      "        Argmax Binary Training Accuracy: 0.9544160445400481,\n",
      "        Custom Metric: 0.04916013798641764,\n",
      "        Epochs: 7\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.030385308366801058,\n",
      "        Training Loss: 0.014329497076072534,\n",
      "        Argmax Binary Validation Accuracy: 0.9169360051712993,\n",
      "        Argmax Binary Training Accuracy: 0.9601417183348095,\n",
      "        Custom Metric: 0.03841321401216532,\n",
      "        Epochs: 8\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03131072090140411,\n",
      "        Training Loss: 0.011785985718310719,\n",
      "        Argmax Binary Validation Accuracy: 0.9159663865546218,\n",
      "        Argmax Binary Training Accuracy: 0.9665000632671138,\n",
      "        Custom Metric: 0.041073088492950806,\n",
      "        Epochs: 9\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03082786327494042,\n",
      "        Training Loss: 0.010848343634689358,\n",
      "        Argmax Binary Validation Accuracy: 0.9169360051712993,\n",
      "        Argmax Binary Training Accuracy: 0.9681766417816019,\n",
      "        Custom Metric: 0.04081762309506595,\n",
      "        Epochs: 10\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.0287569242396525,\n",
      "        Training Loss: 0.00878706822191483,\n",
      "        Argmax Binary Validation Accuracy: 0.9195216548157724,\n",
      "        Argmax Binary Training Accuracy: 0.9724155384031381,\n",
      "        Custom Metric: 0.038741852248521336,\n",
      "        Epochs: 11\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028545369261077473,\n",
      "        Training Loss: 0.007610115307894811,\n",
      "        Argmax Binary Validation Accuracy: 0.9195216548157724,\n",
      "        Argmax Binary Training Accuracy: 0.9739972162469948,\n",
      "        Custom Metric: 0.0390129962376688,\n",
      "        Epochs: 12\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028920617486749376,\n",
      "        Training Loss: 0.0068851636466898125,\n",
      "        Argmax Binary Validation Accuracy: 0.9195216548157724,\n",
      "        Argmax Binary Training Accuracy: 0.9762748323421485,\n",
      "        Custom Metric: 0.03993834440677916,\n",
      "        Epochs: 13\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027106768052492822,\n",
      "        Training Loss: 0.006128876099467907,\n",
      "        Argmax Binary Validation Accuracy: 0.9201680672268907,\n",
      "        Argmax Binary Training Accuracy: 0.977065671264077,\n",
      "        Custom Metric: 0.03759571402900528,\n",
      "        Epochs: 14\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027051438045288836,\n",
      "        Training Loss: 0.005185029269392852,\n",
      "        Argmax Binary Validation Accuracy: 0.9214608920491273,\n",
      "        Argmax Binary Training Accuracy: 0.9801973933949133,\n",
      "        Custom Metric: 0.03798464243323683,\n",
      "        Epochs: 15\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027716866162206446,\n",
      "        Training Loss: 0.004669801304182431,\n",
      "        Argmax Binary Validation Accuracy: 0.9195216548157724,\n",
      "        Argmax Binary Training Accuracy: 0.979881057826142,\n",
      "        Custom Metric: 0.03924039859121845,\n",
      "        Epochs: 16\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.02609484296824251,\n",
      "        Training Loss: 0.004453688290980185,\n",
      "        Argmax Binary Validation Accuracy: 0.9217840982546864,\n",
      "        Argmax Binary Training Accuracy: 0.981747437681893,\n",
      "        Custom Metric: 0.03691542030687367,\n",
      "        Epochs: 17\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027342431247234344,\n",
      "        Training Loss: 0.004149098406729459,\n",
      "        Argmax Binary Validation Accuracy: 0.920814479638009,\n",
      "        Argmax Binary Training Accuracy: 0.9807984309755788,\n",
      "        Custom Metric: 0.03893909766748679,\n",
      "        Epochs: 18\n",
      "        \n",
      "Best Validation Loss: 0.02609484296824251, Best Validation accuracy: 0.920814479638009\n"
     ]
    }
   ],
   "source": [
    "best_config = best_trial.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model with the best configuration\n",
    "best_model, best_model_path = train_best_model(best_config, device, train_ds_pd, validation_ds_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196680d",
   "metadata": {
    "papermill": {
     "duration": 0.028719,
     "end_time": "2024-11-29T19:00:59.564219",
     "exception": false,
     "start_time": "2024-11-29T19:00:59.535500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "987dd702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:00:59.619821Z",
     "iopub.status.busy": "2024-11-29T19:00:59.619211Z",
     "iopub.status.idle": "2024-11-29T19:01:00.824350Z",
     "shell.execute_reply": "2024-11-29T19:01:00.823376Z"
    },
    "papermill": {
     "duration": 1.234623,
     "end_time": "2024-11-29T19:01:00.826021",
     "exception": false,
     "start_time": "2024-11-29T19:00:59.591398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /kaggle/working/final_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "# final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path) \n",
    "\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path)\n",
    "torch.save({'model_state_dict': best_model.state_dict(),'config': best_config}, final_model_path)\n",
    "\n",
    "print(f\"Best model saved at: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d28e8038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:00.883244Z",
     "iopub.status.busy": "2024-11-29T19:01:00.882955Z",
     "iopub.status.idle": "2024-11-29T19:01:00.886532Z",
     "shell.execute_reply": "2024-11-29T19:01:00.885772Z"
    },
    "papermill": {
     "duration": 0.033555,
     "end_time": "2024-11-29T19:01:00.888190",
     "exception": false,
     "start_time": "2024-11-29T19:01:00.854635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=best_trial.config[\"dropout_rate\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9877e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:00.944307Z",
     "iopub.status.busy": "2024-11-29T19:01:00.943482Z",
     "iopub.status.idle": "2024-11-29T19:01:00.947653Z",
     "shell.execute_reply": "2024-11-29T19:01:00.946817Z"
    },
    "papermill": {
     "duration": 0.033923,
     "end_time": "2024-11-29T19:01:00.949186",
     "exception": false,
     "start_time": "2024-11-29T19:01:00.915263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kaggle_working_dir = '/kaggle/working'\n",
    "# best_model_path = os.path.join(kaggle_working_dir, \"checkpoint.pt\")\n",
    "# best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30273eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:01.005991Z",
     "iopub.status.busy": "2024-11-29T19:01:01.005387Z",
     "iopub.status.idle": "2024-11-29T19:01:01.009039Z",
     "shell.execute_reply": "2024-11-29T19:01:01.008321Z"
    },
    "papermill": {
     "duration": 0.033628,
     "end_time": "2024-11-29T19:01:01.010552",
     "exception": false,
     "start_time": "2024-11-29T19:01:00.976924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # best_config = result.get_best_config(metric='accuracy', mode='max')\n",
    "# # best_trial = result.get_best_trial(metric='accuracy', mode='max')\n",
    "\n",
    "# best_config = result.get_best_config(metric='custom_metric', mode='min')\n",
    "# best_trial = result.get_best_trial(metric='custom_metric', mode='min')\n",
    "\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "# print(\"\\nModel Parameters:\")\n",
    "# for name, param in best_model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15f6c248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:01.067139Z",
     "iopub.status.busy": "2024-11-29T19:01:01.066510Z",
     "iopub.status.idle": "2024-11-29T19:01:01.070085Z",
     "shell.execute_reply": "2024-11-29T19:01:01.069315Z"
    },
    "papermill": {
     "duration": 0.033382,
     "end_time": "2024-11-29T19:01:01.071589",
     "exception": false,
     "start_time": "2024-11-29T19:01:01.038207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_trial = tuner.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0bb9c7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:01.127052Z",
     "iopub.status.busy": "2024-11-29T19:01:01.126784Z",
     "iopub.status.idle": "2024-11-29T19:01:01.130428Z",
     "shell.execute_reply": "2024-11-29T19:01:01.129623Z"
    },
    "papermill": {
     "duration": 0.032954,
     "end_time": "2024-11-29T19:01:01.131899",
     "exception": false,
     "start_time": "2024-11-29T19:01:01.098945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# best_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=best_trial.config[\"dropout_rate\"]\n",
    "# )\n",
    "\n",
    "# best_model_path = os.path.join(best_checkpoint_dir, \"checkpoint.pt\")\n",
    "# best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.eval()\n",
    "\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a331fe58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:01.188339Z",
     "iopub.status.busy": "2024-11-29T19:01:01.188093Z",
     "iopub.status.idle": "2024-11-29T19:01:01.191465Z",
     "shell.execute_reply": "2024-11-29T19:01:01.190714Z"
    },
    "papermill": {
     "duration": 0.033851,
     "end_time": "2024-11-29T19:01:01.193033",
     "exception": false,
     "start_time": "2024-11-29T19:01:01.159182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = EmotionClassifier(hidden_size=best_config['hidden_size'], dropout_rate=best_config['dropout_rate']).to(device)\n",
    "# checkpoint_path = \"best_model.pth\"\n",
    "# torch.save(best_model.state_dict(), checkpoint_path)\n",
    "# print(f\"Best model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a96d1095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:01.249911Z",
     "iopub.status.busy": "2024-11-29T19:01:01.249642Z",
     "iopub.status.idle": "2024-11-29T19:01:01.253181Z",
     "shell.execute_reply": "2024-11-29T19:01:01.252354Z"
    },
    "papermill": {
     "duration": 0.034274,
     "end_time": "2024-11-29T19:01:01.254735",
     "exception": false,
     "start_time": "2024-11-29T19:01:01.220461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = EmotionClassifier(hidden_size=best_config['hidden_size'], dropout_rate=best_config['dropout_rate']).to(device)\n",
    "# bestmodel_path = f\"/kaggle/working/bestmodel.pt\"\n",
    "# torch.save(best_model.state_dict(), checkpoint_path)\n",
    "# print(f\"Best model saved to {bestmodel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2846d",
   "metadata": {
    "papermill": {
     "duration": 0.028751,
     "end_time": "2024-11-29T19:01:01.312248",
     "exception": false,
     "start_time": "2024-11-29T19:01:01.283497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b59925f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:01.369064Z",
     "iopub.status.busy": "2024-11-29T19:01:01.368428Z",
     "iopub.status.idle": "2024-11-29T19:01:03.141673Z",
     "shell.execute_reply": "2024-11-29T19:01:03.140783Z"
    },
    "papermill": {
     "duration": 1.80301,
     "end_time": "2024-11-29T19:01:03.143298",
     "exception": false,
     "start_time": "2024-11-29T19:01:01.340288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /kaggle/working/final_best_model.pt for inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# final_model_path = \"/kaggle/input/tachygraphy-lv2-emotion-moodtags-v1/transformers/default/1/final_best_model.pt\"\n",
    "\n",
    "\n",
    "checkpoint = torch.load(final_model_path)\n",
    "config = checkpoint['config']\n",
    "\n",
    "# loaded_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=config[\"dropout_rate\"]\n",
    "# )\n",
    "loaded_model = EmotionModel(\n",
    "    roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "    n_classes=7,\n",
    "    dropout_rate=config[\"dropout_rate\"]\n",
    ")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Loaded model from {final_model_path} for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130d675",
   "metadata": {
    "papermill": {
     "duration": 0.027581,
     "end_time": "2024-11-29T19:01:03.199695",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.172114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PREDICT ON RANDOM INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdce6e",
   "metadata": {
    "papermill": {
     "duration": 0.028458,
     "end_time": "2024-11-29T19:01:03.255868",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.227410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a1042785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:03.312545Z",
     "iopub.status.busy": "2024-11-29T19:01:03.312238Z",
     "iopub.status.idle": "2024-11-29T19:01:03.317822Z",
     "shell.execute_reply": "2024-11-29T19:01:03.317059Z"
    },
    "papermill": {
     "duration": 0.035903,
     "end_time": "2024-11-29T19:01:03.319478",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.283575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_len=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    return torch.sigmoid(outputs).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4628507b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:03.376506Z",
     "iopub.status.busy": "2024-11-29T19:01:03.375815Z",
     "iopub.status.idle": "2024-11-29T19:01:03.383382Z",
     "shell.execute_reply": "2024-11-29T19:01:03.382548Z"
    },
    "papermill": {
     "duration": 0.037167,
     "end_time": "2024-11-29T19:01:03.385029",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.347862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = loaded_model\n",
    "best_model = best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2eca2217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:03.441505Z",
     "iopub.status.busy": "2024-11-29T19:01:03.440893Z",
     "iopub.status.idle": "2024-11-29T19:01:03.874218Z",
     "shell.execute_reply": "2024-11-29T19:01:03.873261Z"
    },
    "papermill": {
     "duration": 0.463728,
     "end_time": "2024-11-29T19:01:03.876152",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.412424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4907586",
   "metadata": {
    "papermill": {
     "duration": 0.027996,
     "end_time": "2024-11-29T19:01:03.933033",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.905037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Samples & Random Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84620691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:03.996444Z",
     "iopub.status.busy": "2024-11-29T19:01:03.995780Z",
     "iopub.status.idle": "2024-11-29T19:01:05.601594Z",
     "shell.execute_reply": "2024-11-29T19:01:05.600759Z"
    },
    "papermill": {
     "duration": 1.642226,
     "end_time": "2024-11-29T19:01:05.603362",
     "exception": false,
     "start_time": "2024-11-29T19:01:03.961136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'hey! hru, wanna ply valo toni8?': [[0.01522723 0.00806822 0.02226685 0.26780015 0.2287963  0.12956041\n",
      "  0.3424514 ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"51391c2b-6130-4059-9f29-a8b26cbc77f9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"51391c2b-6130-4059-9f29-a8b26cbc77f9\")) {                    Plotly.newPlot(                        \"51391c2b-6130-4059-9f29-a8b26cbc77f9\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.015227235,0.008068224,0.022266848,0.26780015,0.2287963,0.12956041,0.3424514,0.015227235],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('51391c2b-6130-4059-9f29-a8b26cbc77f9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa2c3e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:05.663538Z",
     "iopub.status.busy": "2024-11-29T19:01:05.663247Z",
     "iopub.status.idle": "2024-11-29T19:01:05.978082Z",
     "shell.execute_reply": "2024-11-29T19:01:05.977187Z"
    },
    "papermill": {
     "duration": 0.347109,
     "end_time": "2024-11-29T19:01:05.979969",
     "exception": false,
     "start_time": "2024-11-29T19:01:05.632860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'what a badass char is arthur, he is the best game char ever made, i luv rdr2': [[4.6330285e-03 6.1160052e-04 3.1421054e-03 9.5957112e-01 9.3753338e-03\n",
      "  1.2255819e-02 2.5350360e-02]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"674a837b-161a-49a3-8e58-46a2faf8d5ea\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"674a837b-161a-49a3-8e58-46a2faf8d5ea\")) {                    Plotly.newPlot(                        \"674a837b-161a-49a3-8e58-46a2faf8d5ea\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.0046330285,0.0006116005,0.0031421054,0.9595711,0.009375334,0.012255819,0.02535036,0.0046330285],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('674a837b-161a-49a3-8e58-46a2faf8d5ea');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"what a badass char is arthur, he is the best game char ever made, i luv rdr2\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "# fig.show()\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48c1c611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:06.043315Z",
     "iopub.status.busy": "2024-11-29T19:01:06.043013Z",
     "iopub.status.idle": "2024-11-29T19:01:06.285760Z",
     "shell.execute_reply": "2024-11-29T19:01:06.284678Z"
    },
    "papermill": {
     "duration": 0.27613,
     "end_time": "2024-11-29T19:01:06.288961",
     "exception": false,
     "start_time": "2024-11-29T19:01:06.012831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'I don't no fr y hes sooo sad.': [[0.01092991 0.00377975 0.00377853 0.00980998 0.01115605 0.96646357\n",
      "  0.01316012]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"b8197b13-696a-4da1-b0cf-469a7d3455ef\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b8197b13-696a-4da1-b0cf-469a7d3455ef\")) {                    Plotly.newPlot(                        \"b8197b13-696a-4da1-b0cf-469a7d3455ef\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.010929913,0.003779747,0.0037785303,0.009809984,0.011156048,0.96646357,0.013160117,0.010929913],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b8197b13-696a-4da1-b0cf-469a7d3455ef');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"I don't no fr y hes sooo sad.\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "# fig.show()\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "785c253c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:06.396047Z",
     "iopub.status.busy": "2024-11-29T19:01:06.395447Z",
     "iopub.status.idle": "2024-11-29T19:01:06.399395Z",
     "shell.execute_reply": "2024-11-29T19:01:06.398522Z"
    },
    "papermill": {
     "duration": 0.036722,
     "end_time": "2024-11-29T19:01:06.400956",
     "exception": false,
     "start_time": "2024-11-29T19:01:06.364234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# predictions = predict(sample_text, best_model, tokenizer, max_len=128, device=device)\n",
    "# print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e128afac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:06.462626Z",
     "iopub.status.busy": "2024-11-29T19:01:06.462352Z",
     "iopub.status.idle": "2024-11-29T19:01:06.465655Z",
     "shell.execute_reply": "2024-11-29T19:01:06.464922Z"
    },
    "papermill": {
     "duration": 0.035674,
     "end_time": "2024-11-29T19:01:06.467263",
     "exception": false,
     "start_time": "2024-11-29T19:01:06.431589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# predictions = predict(sample_text, best_model, tokenizer, max_len=128, device=device)\n",
    "# print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95bab602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-29T19:01:06.529019Z",
     "iopub.status.busy": "2024-11-29T19:01:06.528510Z",
     "iopub.status.idle": "2024-11-29T19:01:06.532167Z",
     "shell.execute_reply": "2024-11-29T19:01:06.531298Z"
    },
    "papermill": {
     "duration": 0.036518,
     "end_time": "2024-11-29T19:01:06.533689",
     "exception": false,
     "start_time": "2024-11-29T19:01:06.497171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# best_model = RoBERTaEmotionModel('roberta-base').to(device)\n",
    "# best_model.load_state_dict(torch.load(\"best_model.pth\"))  # Assuming best model is saved during training\n",
    "# predicted_emotions = predict_emotions(best_model, sample_text, tokenizer, best_params['max_len'], device)\n",
    "# print(predicted_emotions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5557640,
     "sourceId": 9500773,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18765.118187,
   "end_time": "2024-11-29T19:01:11.884670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-29T13:48:26.766483",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05cb6bbd56db48ddb5705477d08f8970": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f092c681db749728e5a96fe9a0e4a2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a7bbe9719c3948b18d14885c8fff0109",
       "placeholder": "​",
       "style": "IPY_MODEL_45648441641d4e04859aa3b4141c4023",
       "value": " 579/579 [00:00&lt;00:00, 60.4kB/s]"
      }
     },
     "157ec83f46e249cc9ff0cf80d51bb72a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_487131017c8842a98ec76662b7f480a6",
       "placeholder": "​",
       "style": "IPY_MODEL_794b6c917f5a4110b4ae34bb34cb36ce",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "176637c20ba143bc83f17b9ebd7e0778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f430b4863a049fd882e20c073d5e654",
       "placeholder": "​",
       "style": "IPY_MODEL_e4fcff00dc78461daa8be7f25ac29ae4",
       "value": "config.json: 100%"
      }
     },
     "313d466891354fbe927c93b95d0dc87e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3bdc49ad53ff44e9b9a77f7eb63cc837": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c0f40cdb37fc49f2b8c30e6a2c896c42",
       "placeholder": "​",
       "style": "IPY_MODEL_313d466891354fbe927c93b95d0dc87e",
       "value": "spm.model: 100%"
      }
     },
     "45648441641d4e04859aa3b4141c4023": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "487131017c8842a98ec76662b7f480a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f430b4863a049fd882e20c073d5e654": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6845965154514afebce9778049b8d311": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be78e7a15b5f4db6bc9326e6154627e5",
       "max": 2464616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eab1c90ae51848dc911908dfda737957",
       "value": 2464616.0
      }
     },
     "6d8ef6bcfa594878b0ac8dd15fe5ec15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fe73d2e1856c44269ea0157b545e39eb",
       "placeholder": "​",
       "style": "IPY_MODEL_dc2a9774443c482999f424e3435e1422",
       "value": " 52.0/52.0 [00:00&lt;00:00, 4.97kB/s]"
      }
     },
     "73740f0fa51241a88092f5d83ab90f90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "794b6c917f5a4110b4ae34bb34cb36ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7e0571fb07024fd1a3da6034907b502e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_176637c20ba143bc83f17b9ebd7e0778",
        "IPY_MODEL_888f4f6ae6c040e7bf3bc49417db56eb",
        "IPY_MODEL_0f092c681db749728e5a96fe9a0e4a2e"
       ],
       "layout": "IPY_MODEL_73740f0fa51241a88092f5d83ab90f90"
      }
     },
     "888f4f6ae6c040e7bf3bc49417db56eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b190c7d804f64519b7d866f822e0f8ec",
       "max": 579.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d67bba220e5f433693b6282bec2eba78",
       "value": 579.0
      }
     },
     "91343e06f8124833aac7683f490e0c00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3bdc49ad53ff44e9b9a77f7eb63cc837",
        "IPY_MODEL_6845965154514afebce9778049b8d311",
        "IPY_MODEL_e0e0c877d4e548fdbb3d85aff69fe778"
       ],
       "layout": "IPY_MODEL_05cb6bbd56db48ddb5705477d08f8970"
      }
     },
     "a7bbe9719c3948b18d14885c8fff0109": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af6790fcc2814fc79686aa915f774263": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b099215b968d4eadb54a083609d2ccc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b190c7d804f64519b7d866f822e0f8ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b39fb516d80240ca9744c498feba8e4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be78e7a15b5f4db6bc9326e6154627e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0f40cdb37fc49f2b8c30e6a2c896c42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c27bfe67bb0743d4968c07a6d8c6e737": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_157ec83f46e249cc9ff0cf80d51bb72a",
        "IPY_MODEL_d2ff87c7df6a499fb866640e9c41b119",
        "IPY_MODEL_6d8ef6bcfa594878b0ac8dd15fe5ec15"
       ],
       "layout": "IPY_MODEL_b099215b968d4eadb54a083609d2ccc1"
      }
     },
     "d2ff87c7df6a499fb866640e9c41b119": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af6790fcc2814fc79686aa915f774263",
       "max": 52.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d9904fe5a31044e8bb68e14a17d0c854",
       "value": 52.0
      }
     },
     "d67bba220e5f433693b6282bec2eba78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d9904fe5a31044e8bb68e14a17d0c854": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dc2a9774443c482999f424e3435e1422": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e0e0c877d4e548fdbb3d85aff69fe778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b39fb516d80240ca9744c498feba8e4f",
       "placeholder": "​",
       "style": "IPY_MODEL_f74b5168299545598181efe3d5281727",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 34.6MB/s]"
      }
     },
     "e4fcff00dc78461daa8be7f25ac29ae4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eab1c90ae51848dc911908dfda737957": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f74b5168299545598181efe3d5281727": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fe73d2e1856c44269ea0157b545e39eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
