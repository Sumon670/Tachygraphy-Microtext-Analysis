{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6316e44",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:45.188765Z",
     "iopub.status.busy": "2024-08-30T04:00:45.187912Z",
     "iopub.status.idle": "2024-08-30T04:00:46.067064Z",
     "shell.execute_reply": "2024-08-30T04:00:46.066136Z"
    },
    "papermill": {
     "duration": 0.917194,
     "end_time": "2024-08-30T04:00:46.069188",
     "exception": false,
     "start_time": "2024-08-30T04:00:45.151994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv\n",
      "/kaggle/input/dataset-tachygraphy/Tachygraphy_dataset_main.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2975663",
   "metadata": {
    "papermill": {
     "duration": 0.032834,
     "end_time": "2024-08-30T04:00:46.136043",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.103209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac493b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.202622Z",
     "iopub.status.busy": "2024-08-30T04:00:46.202147Z",
     "iopub.status.idle": "2024-08-30T04:00:46.465320Z",
     "shell.execute_reply": "2024-08-30T04:00:46.464465Z"
    },
    "papermill": {
     "duration": 0.298936,
     "end_time": "2024-08-30T04:00:46.467558",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.168622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For emoji cleaning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "'''For emoji cleaning'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ef2edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.536519Z",
     "iopub.status.busy": "2024-08-30T04:00:46.535503Z",
     "iopub.status.idle": "2024-08-30T04:00:46.582073Z",
     "shell.execute_reply": "2024-08-30T04:00:46.581138Z"
    },
    "papermill": {
     "duration": 0.083169,
     "end_time": "2024-08-30T04:00:46.584288",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.501119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/kaggle/input/dataset-tachygraphy/Tachygraphy_EmotionMoodtags_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebed191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.651089Z",
     "iopub.status.busy": "2024-08-30T04:00:46.650803Z",
     "iopub.status.idle": "2024-08-30T04:00:46.654821Z",
     "shell.execute_reply": "2024-08-30T04:00:46.653967Z"
    },
    "papermill": {
     "duration": 0.038967,
     "end_time": "2024-08-30T04:00:46.656750",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.617783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4891d6ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.722404Z",
     "iopub.status.busy": "2024-08-30T04:00:46.722069Z",
     "iopub.status.idle": "2024-08-30T04:00:46.744838Z",
     "shell.execute_reply": "2024-08-30T04:00:46.744032Z"
    },
    "papermill": {
     "duration": 0.058296,
     "end_time": "2024-08-30T04:00:46.746766",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.688470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a68805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.812026Z",
     "iopub.status.busy": "2024-08-30T04:00:46.811345Z",
     "iopub.status.idle": "2024-08-30T04:00:46.824173Z",
     "shell.execute_reply": "2024-08-30T04:00:46.823410Z"
    },
    "papermill": {
     "duration": 0.04725,
     "end_time": "2024-08-30T04:00:46.825985",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.778735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "945a8abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.889869Z",
     "iopub.status.busy": "2024-08-30T04:00:46.889607Z",
     "iopub.status.idle": "2024-08-30T04:00:46.894313Z",
     "shell.execute_reply": "2024-08-30T04:00:46.893571Z"
    },
    "papermill": {
     "duration": 0.038842,
     "end_time": "2024-08-30T04:00:46.896333",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.857491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "#     text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a196a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:46.961414Z",
     "iopub.status.busy": "2024-08-30T04:00:46.961127Z",
     "iopub.status.idle": "2024-08-30T04:00:46.974143Z",
     "shell.execute_reply": "2024-08-30T04:00:46.973332Z"
    },
    "papermill": {
     "duration": 0.047932,
     "end_time": "2024-08-30T04:00:46.976100",
     "exception": false,
     "start_time": "2024-08-30T04:00:46.928168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                last session of the day \n",
       "1       shanghai is also really exciting precisely  sk...\n",
       "2                                  submit the report asap\n",
       "3                                              happy bday\n",
       "4                                      the ogs  i like it\n",
       "                              ...                        \n",
       "4953      make a pet face  wtf wrong with me tonight haha\n",
       "4954         i dnt care anymore  boyz aint worth d drama \n",
       "4955    no relationship is perfect tho me bae goo from...\n",
       "4956    over here tryna get my nail polishes and shit lol\n",
       "4957                       no one was loved d way i luv u\n",
       "Name: text, Length: 4958, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a288c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:47.044731Z",
     "iopub.status.busy": "2024-08-30T04:00:47.044374Z",
     "iopub.status.idle": "2024-08-30T04:00:47.057077Z",
     "shell.execute_reply": "2024-08-30T04:00:47.056309Z"
    },
    "papermill": {
     "duration": 0.049324,
     "end_time": "2024-08-30T04:00:47.058984",
     "exception": false,
     "start_time": "2024-08-30T04:00:47.009660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "df[emotion_columns] = df[emotion_columns].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511291f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:47.124774Z",
     "iopub.status.busy": "2024-08-30T04:00:47.124471Z",
     "iopub.status.idle": "2024-08-30T04:00:47.129317Z",
     "shell.execute_reply": "2024-08-30T04:00:47.128428Z"
    },
    "papermill": {
     "duration": 0.040278,
     "end_time": "2024-08-30T04:00:47.131161",
     "exception": false,
     "start_time": "2024-08-30T04:00:47.090883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "# df['anger'] = df['anger'].astype(float)\n",
    "# df['disgust'] = df['disgust'].astype(float)\n",
    "# df['fear'] = df['fear'].astype(float)\n",
    "# df['joy'] = df['joy'].astype(float)\n",
    "# df['neutral'] = df['neutral'].astype(float)\n",
    "# df['sadness'] = df['sadness'].astype(float)\n",
    "# df['surprise'] = df['surprise'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff7e6e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:47.197421Z",
     "iopub.status.busy": "2024-08-30T04:00:47.196890Z",
     "iopub.status.idle": "2024-08-30T04:00:49.716583Z",
     "shell.execute_reply": "2024-08-30T04:00:49.715808Z"
    },
    "papermill": {
     "duration": 2.555046,
     "end_time": "2024-08-30T04:00:49.718981",
     "exception": false,
     "start_time": "2024-08-30T04:00:47.163935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: text_preprocessing_pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb7873b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:49.784570Z",
     "iopub.status.busy": "2024-08-30T04:00:49.784225Z",
     "iopub.status.idle": "2024-08-30T04:00:49.792041Z",
     "shell.execute_reply": "2024-08-30T04:00:49.791197Z"
    },
    "papermill": {
     "duration": 0.042837,
     "end_time": "2024-08-30T04:00:49.794071",
     "exception": false,
     "start_time": "2024-08-30T04:00:49.751234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 last session of the day\n",
       "1       shanghai is also really exciting precisely sky...\n",
       "2                                  submit the report asap\n",
       "3                                              happy bday\n",
       "4                                       the ogs i like it\n",
       "                              ...                        \n",
       "4953       make a pet face wtf wrong with me tonight haha\n",
       "4954           i dnt care anymore boyz aint worth d drama\n",
       "4955    no relationship is perfect tho me bae goo from...\n",
       "4956    over here tryna get my nail polishes and shit lol\n",
       "4957                       no one was loved d way i luv u\n",
       "Name: text, Length: 4958, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c336cd6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:49.859405Z",
     "iopub.status.busy": "2024-08-30T04:00:49.859125Z",
     "iopub.status.idle": "2024-08-30T04:00:49.866346Z",
     "shell.execute_reply": "2024-08-30T04:00:49.865476Z"
    },
    "papermill": {
     "duration": 0.041868,
     "end_time": "2024-08-30T04:00:49.868314",
     "exception": false,
     "start_time": "2024-08-30T04:00:49.826446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['serial', 'pred', 'label', 'score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d717b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:49.932681Z",
     "iopub.status.busy": "2024-08-30T04:00:49.932377Z",
     "iopub.status.idle": "2024-08-30T04:00:49.936604Z",
     "shell.execute_reply": "2024-08-30T04:00:49.935783Z"
    },
    "papermill": {
     "duration": 0.038683,
     "end_time": "2024-08-30T04:00:49.938647",
     "exception": false,
     "start_time": "2024-08-30T04:00:49.899964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotion_label_mapping = {\n",
    "    0: \"anger\", 1: \"disgust\", 2: \"fear\", 3: \"joy\", 4: \"neutral\",\n",
    "    5: \"sadness\", 6: \"surprise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca7dce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:50.005325Z",
     "iopub.status.busy": "2024-08-30T04:00:50.004802Z",
     "iopub.status.idle": "2024-08-30T04:00:50.009024Z",
     "shell.execute_reply": "2024-08-30T04:00:50.008166Z"
    },
    "papermill": {
     "duration": 0.039605,
     "end_time": "2024-08-30T04:00:50.010970",
     "exception": false,
     "start_time": "2024-08-30T04:00:49.971365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMOTION_LABELS = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\",\n",
    "    \"sadness\", \"surprise\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858c91d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:50.115915Z",
     "iopub.status.busy": "2024-08-30T04:00:50.115510Z",
     "iopub.status.idle": "2024-08-30T04:00:50.120428Z",
     "shell.execute_reply": "2024-08-30T04:00:50.119583Z"
    },
    "papermill": {
     "duration": 0.04133,
     "end_time": "2024-08-30T04:00:50.122385",
     "exception": false,
     "start_time": "2024-08-30T04:00:50.081055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_train_split(dataset, test_ratio=0.1):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aa8e8b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:50.188756Z",
     "iopub.status.busy": "2024-08-30T04:00:50.188450Z",
     "iopub.status.idle": "2024-08-30T04:00:50.197702Z",
     "shell.execute_reply": "2024-08-30T04:00:50.196493Z"
    },
    "papermill": {
     "duration": 0.044674,
     "end_time": "2024-08-30T04:00:50.199628",
     "exception": false,
     "start_time": "2024-08-30T04:00:50.154954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4489 examples in training, 469 examples in testing.\n"
     ]
    }
   ],
   "source": [
    "train_ds_pd, validation_ds_pd = test_train_split(df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(validation_ds_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b53889f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:50.264261Z",
     "iopub.status.busy": "2024-08-30T04:00:50.263963Z",
     "iopub.status.idle": "2024-08-30T04:00:50.270106Z",
     "shell.execute_reply": "2024-08-30T04:00:50.269389Z"
    },
    "papermill": {
     "duration": 0.040648,
     "end_time": "2024-08-30T04:00:50.271969",
     "exception": false,
     "start_time": "2024-08-30T04:00:50.231321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_pd = train_ds_pd.reset_index(drop=True)\n",
    "validation_ds_pd = validation_ds_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd8220a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:50.338253Z",
     "iopub.status.busy": "2024-08-30T04:00:50.337513Z",
     "iopub.status.idle": "2024-08-30T04:00:54.289369Z",
     "shell.execute_reply": "2024-08-30T04:00:54.288576Z"
    },
    "papermill": {
     "duration": 3.987967,
     "end_time": "2024-08-30T04:00:54.291730",
     "exception": false,
     "start_time": "2024-08-30T04:00:50.303763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b3fad",
   "metadata": {
    "papermill": {
     "duration": 0.032537,
     "end_time": "2024-08-30T04:00:54.357102",
     "exception": false,
     "start_time": "2024-08-30T04:00:54.324565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SETTING CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af9c9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:54.423380Z",
     "iopub.status.busy": "2024-08-30T04:00:54.422402Z",
     "iopub.status.idle": "2024-08-30T04:00:54.511685Z",
     "shell.execute_reply": "2024-08-30T04:00:54.510358Z"
    },
    "papermill": {
     "duration": 0.124844,
     "end_time": "2024-08-30T04:00:54.514296",
     "exception": false,
     "start_time": "2024-08-30T04:00:54.389452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a12e7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:54.580763Z",
     "iopub.status.busy": "2024-08-30T04:00:54.580389Z",
     "iopub.status.idle": "2024-08-30T04:00:54.584715Z",
     "shell.execute_reply": "2024-08-30T04:00:54.583913Z"
    },
    "papermill": {
     "duration": 0.039769,
     "end_time": "2024-08-30T04:00:54.586688",
     "exception": false,
     "start_time": "2024-08-30T04:00:54.546919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c64fd77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:54.652220Z",
     "iopub.status.busy": "2024-08-30T04:00:54.651941Z",
     "iopub.status.idle": "2024-08-30T04:00:57.136083Z",
     "shell.execute_reply": "2024-08-30T04:00:57.135312Z"
    },
    "papermill": {
     "duration": 2.519503,
     "end_time": "2024-08-30T04:00:57.138320",
     "exception": false,
     "start_time": "2024-08-30T04:00:54.618817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DebertaV2Model, DebertaV2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bfe83a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:57.206259Z",
     "iopub.status.busy": "2024-08-30T04:00:57.205170Z",
     "iopub.status.idle": "2024-08-30T04:00:58.872044Z",
     "shell.execute_reply": "2024-08-30T04:00:58.871086Z"
    },
    "papermill": {
     "duration": 1.703172,
     "end_time": "2024-08-30T04:00:58.874682",
     "exception": false,
     "start_time": "2024-08-30T04:00:57.171510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822e2843f6dd4d01a2e940af7d26ec71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eababcc8f664d65b0aae155db771c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16f39eefbe343b0b37c50827071f529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "977242af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:00:58.948852Z",
     "iopub.status.busy": "2024-08-30T04:00:58.947868Z",
     "iopub.status.idle": "2024-08-30T04:01:13.478525Z",
     "shell.execute_reply": "2024-08-30T04:01:13.477417Z"
    },
    "papermill": {
     "duration": 14.568138,
     "end_time": "2024-08-30T04:01:13.480985",
     "exception": false,
     "start_time": "2024-08-30T04:00:58.912847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.2)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.30)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.4)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.2)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.2)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5d4a3",
   "metadata": {
    "papermill": {
     "duration": 0.033469,
     "end_time": "2024-08-30T04:01:13.548328",
     "exception": false,
     "start_time": "2024-08-30T04:01:13.514859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TORCH IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a50346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:13.617465Z",
     "iopub.status.busy": "2024-08-30T04:01:13.617100Z",
     "iopub.status.idle": "2024-08-30T04:01:31.996334Z",
     "shell.execute_reply": "2024-08-30T04:01:31.995311Z"
    },
    "papermill": {
     "duration": 18.416411,
     "end_time": "2024-08-30T04:01:31.998798",
     "exception": false,
     "start_time": "2024-08-30T04:01:13.582387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 04:01:16,012\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-30 04:01:16,585\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "# from optuna.integration import PyTorchLightningPruner\n",
    "from ray import tune\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "# from ray.tune.integration.pytorch import TuneReportCallback\n",
    "from torch.amp import GradScaler, autocast\n",
    "# from ray.tune.integration.optuna import OptunaSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from torch import autocast\n",
    "# from ray import tune\n",
    "# from ray.tune.integration.tensorboard import TensorBoardReporter\n",
    "from ray.tune.logger import TBXLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ray.train import report\n",
    "# from ray.tune.integration.jupyter import JupyterNotebookReporter\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from ray.tune.schedulers import HyperBandScheduler, AsyncHyperBandScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c149b1",
   "metadata": {
    "papermill": {
     "duration": 0.033042,
     "end_time": "2024-08-30T04:01:32.066315",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.033273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DATASET CONFIG & ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8e0bf4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.135385Z",
     "iopub.status.busy": "2024-08-30T04:01:32.134247Z",
     "iopub.status.idle": "2024-08-30T04:01:32.143337Z",
     "shell.execute_reply": "2024-08-30T04:01:32.142400Z"
    },
    "papermill": {
     "duration": 0.045579,
     "end_time": "2024-08-30T04:01:32.145326",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.099747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         text = self.data.iloc[index, 0]\n",
    "#         labels = self.data.iloc[index, 1:].values.astype(float)\n",
    "        text = str(self.data.iloc[index]['text'])\n",
    "        labels = self.data.iloc[index][['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].values.astype(np.float32)\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86031fc5",
   "metadata": {
    "papermill": {
     "duration": 0.033161,
     "end_time": "2024-08-30T04:01:32.211746",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.178585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "237c2799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.280031Z",
     "iopub.status.busy": "2024-08-30T04:01:32.279250Z",
     "iopub.status.idle": "2024-08-30T04:01:32.287429Z",
     "shell.execute_reply": "2024-08-30T04:01:32.286589Z"
    },
    "papermill": {
     "duration": 0.044195,
     "end_time": "2024-08-30T04:01:32.289440",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.245245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.roberta = roberta_model\n",
    "        self.drop = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.out = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_states = output.last_hidden_state\n",
    "        \n",
    "        # Extract the [CLS] token representation (first token in the sequence)\n",
    "        cls_token_state = output.last_hidden_state[:, 0, :]\n",
    "        output = self.drop(cls_token_state)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.drop(output)\n",
    "        output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20820a32",
   "metadata": {
    "papermill": {
     "duration": 0.033264,
     "end_time": "2024-08-30T04:01:32.356359",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.323095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Other Models to try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e93b11b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.424331Z",
     "iopub.status.busy": "2024-08-30T04:01:32.424030Z",
     "iopub.status.idle": "2024-08-30T04:01:32.428245Z",
     "shell.execute_reply": "2024-08-30T04:01:32.427361Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040533,
     "end_time": "2024-08-30T04:01:32.430133",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.389600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         output = self.drop(output.pooler_output)\n",
    "#         return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af3a9089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.498697Z",
     "iopub.status.busy": "2024-08-30T04:01:32.498377Z",
     "iopub.status.idle": "2024-08-30T04:01:32.502913Z",
     "shell.execute_reply": "2024-08-30T04:01:32.501998Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040862,
     "end_time": "2024-08-30T04:01:32.504984",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.464122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.out = nn.Linear(256, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Use the pooled output for classification tasks\n",
    "#         pooled_output = output.pooler_output\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37f88726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.572799Z",
     "iopub.status.busy": "2024-08-30T04:01:32.572454Z",
     "iopub.status.idle": "2024-08-30T04:01:32.576937Z",
     "shell.execute_reply": "2024-08-30T04:01:32.576064Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040263,
     "end_time": "2024-08-30T04:01:32.578899",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.538636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model, n_classes, dropout_rate, hidden_size):\n",
    "#         super(EmotionModel, self).__init__()\n",
    "#         self.roberta = roberta_model\n",
    "#         self.drop = nn.Dropout(p=dropout_rate)\n",
    "#         self.fc1 = nn.Linear(self.roberta.config.hidden_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.out = nn.Linear(hidden_size // 2, n_classes)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         # Get the RoBERTa output\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Use the pooled output for classification tasks\n",
    "#         pooled_output = output.pooler_output\n",
    "#         # Pass through the custom layers\n",
    "#         output = self.drop(pooled_output)\n",
    "#         output = self.relu(self.fc1(output))\n",
    "#         output = self.drop(output)\n",
    "#         output = self.relu(self.fc2(output))\n",
    "#         output = self.drop(output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32b0883e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.647555Z",
     "iopub.status.busy": "2024-08-30T04:01:32.646656Z",
     "iopub.status.idle": "2024-08-30T04:01:32.651565Z",
     "shell.execute_reply": "2024-08-30T04:01:32.650706Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041498,
     "end_time": "2024-08-30T04:01:32.653476",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.611978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class RoBERTaEmotionModel(nn.Module):\n",
    "#     def __init__(self, roberta_model_name, num_emotions=7):\n",
    "#         super(RoBERTaEmotionModel, self).__init__()\n",
    "#         self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "#         self.drop = nn.Dropout(p=0.3)\n",
    "#         self.out = nn.Linear(self.roberta.config.hidden_size, num_emotions)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.roberta(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         pooled_output = outputs[1]  # CLS token\n",
    "#         output = self.drop(pooled_output)\n",
    "#         return self.out(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b6e33",
   "metadata": {
    "papermill": {
     "duration": 0.032915,
     "end_time": "2024-08-30T04:01:32.719984",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.687069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAIN & VALIDATION \n",
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5f3f3e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.788719Z",
     "iopub.status.busy": "2024-08-30T04:01:32.788389Z",
     "iopub.status.idle": "2024-08-30T04:01:32.793318Z",
     "shell.execute_reply": "2024-08-30T04:01:32.792510Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041343,
     "end_time": "2024-08-30T04:01:32.795147",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.753804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, n_epochs):\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for data in train_loader:\n",
    "#             input_ids = data['input_ids'].to(device)\n",
    "#             attention_mask = data['attention_mask'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             with autocast(device_type=device.type):\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "#             if scheduler:\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "#         train_accuracy = correct / total\n",
    "#         val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd76deb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.862166Z",
     "iopub.status.busy": "2024-08-30T04:01:32.861861Z",
     "iopub.status.idle": "2024-08-30T04:01:32.866241Z",
     "shell.execute_reply": "2024-08-30T04:01:32.865373Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040099,
     "end_time": "2024-08-30T04:01:32.868144",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.828045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def eval_model(model, val_loader, loss_fn, device):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             input_ids = data['input_ids'].to(device)\n",
    "#             attention_mask = data['attention_mask'].to(device)\n",
    "#             labels = data['labels'].to(device)\n",
    "\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "#     val_accuracy = correct / total\n",
    "#     return val_loss / len(val_loader), val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50c022e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:32.935773Z",
     "iopub.status.busy": "2024-08-30T04:01:32.935472Z",
     "iopub.status.idle": "2024-08-30T04:01:32.940862Z",
     "shell.execute_reply": "2024-08-30T04:01:32.939991Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041547,
     "end_time": "2024-08-30T04:01:32.942944",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.901397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     hidden_size = trial.suggest_int('hidden_size', 64, 256)\n",
    "#     dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [2, 4, 8, 16])\n",
    "#     learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "#     epochs = 5  # Adjust as needed\n",
    "\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    \n",
    "#     # Load your data here\n",
    "# #     data = df  # Replace with your data loading logic\n",
    "# #     train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "# #         data.iloc[:, 0],  # Assuming text is in the first column\n",
    "# #         data.iloc[:, 1:],  # Labels are in the remaining columns\n",
    "# #         test_size=0.2\n",
    "# #     )\n",
    "\n",
    "\n",
    "# #     train_dataset = EmotionDataset(pd.DataFrame({'text': train_texts, **pd.DataFrame(train_labels)}), tokenizer, max_len=128)\n",
    "# #     val_dataset = EmotionDataset(pd.DataFrame({'text': val_texts, **pd.DataFrame(val_labels)}), tokenizer, max_len=128)\n",
    "    \n",
    "    \n",
    "#     train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#     val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     model = EmotionClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "#     loss_fn = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, epochs)\n",
    "#     val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "    \n",
    "#     return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7a3e6e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.011434Z",
     "iopub.status.busy": "2024-08-30T04:01:33.011091Z",
     "iopub.status.idle": "2024-08-30T04:01:33.016417Z",
     "shell.execute_reply": "2024-08-30T04:01:33.015584Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.042417,
     "end_time": "2024-08-30T04:01:33.018485",
     "exception": false,
     "start_time": "2024-08-30T04:01:32.976068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_fn(config):\n",
    "#     try:\n",
    "#         hidden_size = config['hidden_size']\n",
    "#         dropout_rate = config['dropout_rate']\n",
    "#         batch_size = config['batch_size']\n",
    "#         learning_rate = config['lr']\n",
    "#         epochs = 5\n",
    "\n",
    "#         tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#         # Load your data here\n",
    "# #         data = pd.read_csv('path_to_your_data.csv')  # Replace with your data loading logic\n",
    "# #         train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "# #             data.iloc[:, 0],  # Assuming text is in the first column\n",
    "# #             data.iloc[:, 1:],  # Labels are in the remaining columns\n",
    "# #             test_size=0.2\n",
    "# #         )\n",
    "\n",
    "#         train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#         val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         model = EmotionClassifier(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
    "#         loss_fn = nn.BCEWithLogitsLoss()\n",
    "#         optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#         scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "#         scaler = GradScaler()\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, epochs)\n",
    "#             val_loss, val_accuracy = eval_model(model, val_loader, loss_fn, device)\n",
    "#             tune.report(loss=val_loss, accuracy=val_accuracy)\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d4b4bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.086521Z",
     "iopub.status.busy": "2024-08-30T04:01:33.086217Z",
     "iopub.status.idle": "2024-08-30T04:01:33.090783Z",
     "shell.execute_reply": "2024-08-30T04:01:33.090043Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040841,
     "end_time": "2024-08-30T04:01:33.092895",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.052054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tune_model(config):\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#     train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#     train_dataset = EmotionDataset(train_df, tokenizer, config['max_len'])\n",
    "#     val_dataset = EmotionDataset(val_df, tokenizer, config['max_len'])\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = RoBERTaEmotionModel('roberta-base').to(device)\n",
    "\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "#     loss_fn = nn.MSELoss().to(device)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['lr'], steps_per_epoch=len(train_loader), epochs=config['epochs'])\n",
    "\n",
    "#     train_model(model, train_loader, val_loader, loss_fn, optimizer, device, scheduler, scaler, config['epochs'])\n",
    "\n",
    "#     val_loss = eval_model(model, val_loader, loss_fn, device)\n",
    "#     tune.report(val_loss=val_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac1629bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.162816Z",
     "iopub.status.busy": "2024-08-30T04:01:33.162464Z",
     "iopub.status.idle": "2024-08-30T04:01:33.166985Z",
     "shell.execute_reply": "2024-08-30T04:01:33.166147Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041757,
     "end_time": "2024-08-30T04:01:33.168937",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.127180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'max_len': tune.choice([128, 192, 256]),\n",
    "#     'batch_size': tune.choice([8, 16, 32]),\n",
    "#     'lr': tune.loguniform(1e-5, 5e-5),\n",
    "#     'epochs': tune.choice([3, 5, 7])\n",
    "# }\n",
    "\n",
    "# scheduler = ASHAScheduler(\n",
    "#     metric='val_loss',\n",
    "#     mode='min',\n",
    "#     max_t=10,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )\n",
    "\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=['val_loss', 'training_iteration']\n",
    "# )\n",
    "\n",
    "# analysis = tune.run(\n",
    "#     tune_model,\n",
    "#     resources_per_trial={'cpu': 2, 'gpu': 1},\n",
    "#     config=config,\n",
    "#     num_samples=20,\n",
    "#     scheduler=scheduler,\n",
    "#     progress_reporter=reporter\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c05c6ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.237795Z",
     "iopub.status.busy": "2024-08-30T04:01:33.237297Z",
     "iopub.status.idle": "2024-08-30T04:01:33.244192Z",
     "shell.execute_reply": "2024-08-30T04:01:33.243357Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.04359,
     "end_time": "2024-08-30T04:01:33.246344",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.202754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "#     model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "#     model.train()\n",
    "#     for epoch in range(config[\"epochs\"]):\n",
    "#         total_train_loss = 0.0\n",
    "#         correct_train_preds = 0\n",
    "#         total_train_preds = 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "        \n",
    "#         avg_train_loss = total_train_loss / len(train_loader)\n",
    "#         train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         correct_val_preds = 0\n",
    "#         total_val_preds = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 input_ids = batch[\"input_ids\"].to(device)\n",
    "#                 attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#                 labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "\n",
    "#         avg_val_loss = total_val_loss / len(val_loader)\n",
    "#         val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "#         tune.report(loss=avg_val_loss, accuracy=val_accuracy, train_loss=avg_train_loss, train_accuracy=train_accuracy)\n",
    "#         model_save_path = os.path.join(tune.get_trial_dir(), \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def train_fn(config):\n",
    "#     # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#     train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "#     val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     train_model(config, train_dataset, val_dataset, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f49a5",
   "metadata": {
    "papermill": {
     "duration": 0.033996,
     "end_time": "2024-08-30T04:01:33.313726",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.279730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NEW TRAIN & VALIDATION 2.0\n",
    "### Custom metric is a metric for determining the best model free from any overfitting, underfitting. ```custom_metric = (avg_val_loss−val_accuracy) + α × ∣avg_train_loss−avg_val_loss∣ + β × ∣val_accuracy-train_accuracy∣``` This metric balances between low validation loss, high validation accuracy, and penalizing large discrepancies between training and validation loss. Note that, in the model selection we have used ```α = β = 0.5```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee5490",
   "metadata": {
    "papermill": {
     "duration": 0.03321,
     "end_time": "2024-08-30T04:01:33.380247",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.347037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### But since, we have probabilistic values as input and output we cannot directly calculate the accuracy, infact we need to rely on loss only. ```custom_metric = avg_val_loss + α × ∣avg_train_loss−avg_val_loss∣```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba43bfa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.449306Z",
     "iopub.status.busy": "2024-08-30T04:01:33.448935Z",
     "iopub.status.idle": "2024-08-30T04:01:33.454042Z",
     "shell.execute_reply": "2024-08-30T04:01:33.453102Z"
    },
    "papermill": {
     "duration": 0.042627,
     "end_time": "2024-08-30T04:01:33.456120",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.413493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom metric calculation function\n",
    "def calculate_custom_metric(avg_val_loss, avg_train_loss, alpha=0.5):\n",
    "    loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "    custom_metric = avg_val_loss + alpha * loss_diff\n",
    "    return custom_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd8c99e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.524282Z",
     "iopub.status.busy": "2024-08-30T04:01:33.523925Z",
     "iopub.status.idle": "2024-08-30T04:01:33.528127Z",
     "shell.execute_reply": "2024-08-30T04:01:33.527290Z"
    },
    "papermill": {
     "duration": 0.040393,
     "end_time": "2024-08-30T04:01:33.530075",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.489682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Custom metric calculation function\n",
    "# def calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy, alpha=0.5, beta=0.5):\n",
    "#     loss_diff = abs(avg_train_loss - avg_val_loss)\n",
    "#     accuracy_diff = abs(train_accuracy - val_accuracy)\n",
    "#     custom_metric = avg_val_loss - val_accuracy + alpha * loss_diff + beta * accuracy_diff\n",
    "#     return custom_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174bed1",
   "metadata": {
    "papermill": {
     "duration": 0.032992,
     "end_time": "2024-08-30T04:01:33.596527",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.563535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c975969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.666178Z",
     "iopub.status.busy": "2024-08-30T04:01:33.665856Z",
     "iopub.status.idle": "2024-08-30T04:01:33.673245Z",
     "shell.execute_reply": "2024-08-30T04:01:33.672337Z"
    },
    "papermill": {
     "duration": 0.045115,
     "end_time": "2024-08-30T04:01:33.675315",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.630200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        EarlyStopping to stop training when a metric has stopped improving.\n",
    "\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss improved to {val_loss:.4f}')\n",
    "#             torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "#             if self.verbose:\n",
    "#                 print(f'Validation loss did not improve. Patience: {self.patience}, Counter: {self.counter}')\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85ad58df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.743675Z",
     "iopub.status.busy": "2024-08-30T04:01:33.743315Z",
     "iopub.status.idle": "2024-08-30T04:01:33.765241Z",
     "shell.execute_reply": "2024-08-30T04:01:33.764458Z"
    },
    "papermill": {
     "duration": 0.058305,
     "end_time": "2024-08-30T04:01:33.767061",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.708756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training function with variable parameters\n",
    "def train_model(config, train_dataset, val_dataset, device):\n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = EmotionModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=7,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()  # Initialize GradScaler\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):  # Ensure epochs are passed from config\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):  # Mixed precision\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "            correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "                correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "#         custom_metric = avg_val_loss - val_accuracy\n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)\n",
    "\n",
    "        # Report metrics using ray.train.report\n",
    "        report({\n",
    "            \"loss\": avg_val_loss,\n",
    "            \"accuracy\": val_accuracy,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"custom_metric\": custom_metric,\n",
    "            \"early_stopping_epoch\": epoch + 1,\n",
    "        })\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "#         trial_dir = os.path.join(os.environ[\"TUNE_TRIAL_DIR\"], \"checkpoint.pt\")\n",
    "#         torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "#         checkpoint_path = f\"/kaggle/working/checkpoint_epoch_{epoch}.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_fn(config):\n",
    "    # Load your data here\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "    train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(config, train_dataset, val_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ece85",
   "metadata": {
    "papermill": {
     "duration": 0.033471,
     "end_time": "2024-08-30T04:01:33.833812",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.800341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34795c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.902358Z",
     "iopub.status.busy": "2024-08-30T04:01:33.901986Z",
     "iopub.status.idle": "2024-08-30T04:01:33.908210Z",
     "shell.execute_reply": "2024-08-30T04:01:33.907313Z"
    },
    "papermill": {
     "duration": 0.042727,
     "end_time": "2024-08-30T04:01:33.910087",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.867360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'dropout_rate': tune.choice([0.2, 0.25, 0.27, 0.3, 0.35, 0.4]),\n",
    "    'batch_size': tune.choice([32, 64]),\n",
    "    'weight_decay': tune.choice([1e-6, 2e-6, 1e-5, 2e-5, 1e-4, 2e-4, 5e-5, 5e-6, 1e-2, 1e-3]),\n",
    "    'lr': tune.choice([1e-6, 1e-5, 2e-5, 1e-4, 2e-4, 2e-6, 3e-6, 5e-6, 3e-5, 5e-5, 2e-7, 1e-7, 5e-7, 1e-3]),\n",
    "    'epochs': tune.choice([5, 7, 10, 12, 15, 20])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d26041f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:33.978925Z",
     "iopub.status.busy": "2024-08-30T04:01:33.978611Z",
     "iopub.status.idle": "2024-08-30T04:01:33.982670Z",
     "shell.execute_reply": "2024-08-30T04:01:33.981749Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.041126,
     "end_time": "2024-08-30T04:01:33.984591",
     "exception": false,
     "start_time": "2024-08-30T04:01:33.943465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([32, 64, 128, 256]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([3, 5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d03f9231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.062440Z",
     "iopub.status.busy": "2024-08-30T04:01:34.062086Z",
     "iopub.status.idle": "2024-08-30T04:01:34.066207Z",
     "shell.execute_reply": "2024-08-30T04:01:34.065414Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.048274,
     "end_time": "2024-08-30T04:01:34.067989",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.019715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.choice([3, 5, 7, 10, 12, 15, 20])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5386b7f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.136392Z",
     "iopub.status.busy": "2024-08-30T04:01:34.136095Z",
     "iopub.status.idle": "2024-08-30T04:01:34.139996Z",
     "shell.execute_reply": "2024-08-30T04:01:34.139066Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040351,
     "end_time": "2024-08-30T04:01:34.141814",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.101463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([32, 64, 128, 256]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([16, 32, 64]),\n",
    "#     'weight_decay': tune.loguniform(1e-6, 1e-1),\n",
    "#     'lr': tune.loguniform(1e-6, 1e-1),\n",
    "#     'epochs': tune.randint(3, 21)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b975a247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.211880Z",
     "iopub.status.busy": "2024-08-30T04:01:34.211571Z",
     "iopub.status.idle": "2024-08-30T04:01:34.215413Z",
     "shell.execute_reply": "2024-08-30T04:01:34.214700Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.040296,
     "end_time": "2024-08-30T04:01:34.217222",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.176926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'hidden_size': tune.choice([256, 512]),\n",
    "#     'dropout_rate': tune.uniform(0.1, 0.5),\n",
    "#     'batch_size': tune.choice([4, 8, 16]),\n",
    "#     'lr': tune.loguniform(1e-5, 1e-2)\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e3d08",
   "metadata": {
    "papermill": {
     "duration": 0.032924,
     "end_time": "2024-08-30T04:01:34.283051",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.250127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEARCH ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3b80390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.351740Z",
     "iopub.status.busy": "2024-08-30T04:01:34.351032Z",
     "iopub.status.idle": "2024-08-30T04:01:34.355485Z",
     "shell.execute_reply": "2024-08-30T04:01:34.354607Z"
    },
    "papermill": {
     "duration": 0.041278,
     "end_time": "2024-08-30T04:01:34.357355",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.316077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optuna_search = OptunaSearch(\n",
    "    metric=[\"accuracy\",\"custom_metric\"],\n",
    "    mode=[\"max\",\"min\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "495d680c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.426654Z",
     "iopub.status.busy": "2024-08-30T04:01:34.425977Z",
     "iopub.status.idle": "2024-08-30T04:01:34.430407Z",
     "shell.execute_reply": "2024-08-30T04:01:34.429514Z"
    },
    "papermill": {
     "duration": 0.041345,
     "end_time": "2024-08-30T04:01:34.432429",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.391084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optuna_search = OptunaSearch(\n",
    "#     metric=[\n",
    "#         tune.MultiObjective(\"loss\", \"min\"),\n",
    "#         tune.MultiObjective(\"custom_metric\", \"min\"),\n",
    "#         tune.MultiObjective(\"accuracy\", \"max\")\n",
    "#     ],\n",
    "#     mode=[\n",
    "#         \"min\",  # for loss\n",
    "#         \"min\",  # for custom metric\n",
    "#         \"max\"   # for accuracy\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f935898a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.502372Z",
     "iopub.status.busy": "2024-08-30T04:01:34.501572Z",
     "iopub.status.idle": "2024-08-30T04:01:34.505974Z",
     "shell.execute_reply": "2024-08-30T04:01:34.505057Z"
    },
    "papermill": {
     "duration": 0.040946,
     "end_time": "2024-08-30T04:01:34.507938",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.466992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optuna_search = OptunaSearch(\n",
    "#     metric=\"custom_metric\",\n",
    "#     mode=\"min\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c88d8a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.576398Z",
     "iopub.status.busy": "2024-08-30T04:01:34.575674Z",
     "iopub.status.idle": "2024-08-30T04:01:34.579589Z",
     "shell.execute_reply": "2024-08-30T04:01:34.578722Z"
    },
    "papermill": {
     "duration": 0.039796,
     "end_time": "2024-08-30T04:01:34.581531",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.541735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Setup Optuna for hyperparameter optimization\n",
    "# optuna_search = OptunaSearch(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b81b5",
   "metadata": {
    "papermill": {
     "duration": 0.033387,
     "end_time": "2024-08-30T04:01:34.648409",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.615022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SCHEDULER ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16f64131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.717705Z",
     "iopub.status.busy": "2024-08-30T04:01:34.717102Z",
     "iopub.status.idle": "2024-08-30T04:01:34.722040Z",
     "shell.execute_reply": "2024-08-30T04:01:34.721060Z"
    },
    "papermill": {
     "duration": 0.041141,
     "end_time": "2024-08-30T04:01:34.723961",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.682820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric='accuracy',\n",
    "    mode='max',\n",
    "    max_t=22,\n",
    "    grace_period=3,\n",
    "    reduction_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd2691a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.792128Z",
     "iopub.status.busy": "2024-08-30T04:01:34.791283Z",
     "iopub.status.idle": "2024-08-30T04:01:34.795480Z",
     "shell.execute_reply": "2024-08-30T04:01:34.794613Z"
    },
    "papermill": {
     "duration": 0.040365,
     "end_time": "2024-08-30T04:01:34.797411",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.757046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"loss\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=15,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "344e8491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.866160Z",
     "iopub.status.busy": "2024-08-30T04:01:34.865407Z",
     "iopub.status.idle": "2024-08-30T04:01:34.869371Z",
     "shell.execute_reply": "2024-08-30T04:01:34.868504Z"
    },
    "papermill": {
     "duration": 0.040928,
     "end_time": "2024-08-30T04:01:34.871356",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.830428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = AsyncHyperBandScheduler(\n",
    "#     metric='accuracy',\n",
    "#     mode='max',\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d783b90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:34.940367Z",
     "iopub.status.busy": "2024-08-30T04:01:34.940062Z",
     "iopub.status.idle": "2024-08-30T04:01:34.943864Z",
     "shell.execute_reply": "2024-08-30T04:01:34.942928Z"
    },
    "papermill": {
     "duration": 0.04054,
     "end_time": "2024-08-30T04:01:34.945857",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.905317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"accuracy\",\n",
    "#     mode=\"max\",\n",
    "#     max_t=15,\n",
    "#     grace_period=1,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "617b7674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.015153Z",
     "iopub.status.busy": "2024-08-30T04:01:35.014840Z",
     "iopub.status.idle": "2024-08-30T04:01:35.019040Z",
     "shell.execute_reply": "2024-08-30T04:01:35.018175Z"
    },
    "papermill": {
     "duration": 0.041543,
     "end_time": "2024-08-30T04:01:35.020978",
     "exception": false,
     "start_time": "2024-08-30T04:01:34.979435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=[\n",
    "#         tune.MultiObjective(\"loss\", \"min\"),\n",
    "#         tune.MultiObjective(\"custom_metric\", \"min\"),\n",
    "#         tune.MultiObjective(\"accuracy\", \"max\")\n",
    "#     ],\n",
    "#     mode=[\n",
    "#         \"min\",  # for loss\n",
    "#         \"min\",  # for custom metric\n",
    "#         \"max\"   # for accuracy\n",
    "#     ],\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70b4a89f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.091520Z",
     "iopub.status.busy": "2024-08-30T04:01:35.090693Z",
     "iopub.status.idle": "2024-08-30T04:01:35.094970Z",
     "shell.execute_reply": "2024-08-30T04:01:35.094044Z"
    },
    "papermill": {
     "duration": 0.041491,
     "end_time": "2024-08-30T04:01:35.096875",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.055384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(\n",
    "#     metric=\"custom_metric\",\n",
    "#     mode=\"min\",\n",
    "#     max_t=22,\n",
    "#     grace_period=3,\n",
    "#     reduction_factor=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42eee00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.166886Z",
     "iopub.status.busy": "2024-08-30T04:01:35.166067Z",
     "iopub.status.idle": "2024-08-30T04:01:35.170512Z",
     "shell.execute_reply": "2024-08-30T04:01:35.169598Z"
    },
    "papermill": {
     "duration": 0.041728,
     "end_time": "2024-08-30T04:01:35.172539",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.130811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler = HyperBandScheduler(\n",
    "#     time_attr='training_iteration',  # The attribute to use for the time dimension (similar to ASHA's `max_t`)\n",
    "#     metric='custom_metric',          # The metric to optimize (similar to ASHA's `metric`)\n",
    "#     mode='min',                      # Optimization mode (minimize `custom_metric`)\n",
    "#     max_t=22,                        # Maximum number of iterations per trial (similar to ASHA's `max_t`)\n",
    "#     reduction_factor=2               # Reduction factor (similar to ASHA)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16f31cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.241907Z",
     "iopub.status.busy": "2024-08-30T04:01:35.241535Z",
     "iopub.status.idle": "2024-08-30T04:01:35.246164Z",
     "shell.execute_reply": "2024-08-30T04:01:35.245334Z"
    },
    "papermill": {
     "duration": 0.041247,
     "end_time": "2024-08-30T04:01:35.247996",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.206749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False\n",
    ")\n",
    "reporter.verbosity = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b1b3c",
   "metadata": {
    "papermill": {
     "duration": 0.033319,
     "end_time": "2024-08-30T04:01:35.314410",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.281091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom CLI Reporter Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d43a7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.382725Z",
     "iopub.status.busy": "2024-08-30T04:01:35.382360Z",
     "iopub.status.idle": "2024-08-30T04:01:35.387118Z",
     "shell.execute_reply": "2024-08-30T04:01:35.386273Z"
    },
    "papermill": {
     "duration": 0.041017,
     "end_time": "2024-08-30T04:01:35.389067",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.348050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class FinalTableCLIReporter(CLIReporter):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.results = []\n",
    "\n",
    "#     def _update(self, *args, **kwargs):\n",
    "#         # Collect results without printing intermediate updates\n",
    "#         self.results.append(kwargs)\n",
    "\n",
    "#     def print_table(self):\n",
    "#         # Print only the final results\n",
    "#         if self.results:\n",
    "#             final_results = [result for result in self.results]\n",
    "#             headers = list(final_results[0].keys()) if final_results else []\n",
    "#             table = [headers] + [list(result.values()) for result in final_results]\n",
    "#             for row in table:\n",
    "#                 print(\" | \".join(str(cell) for cell in row))\n",
    "\n",
    "#     def _report(self, *args, **kwargs):\n",
    "#         # Override this to prevent intermediate print\n",
    "#         self._update(*args, **kwargs)\n",
    "\n",
    "#     def _finalize(self):\n",
    "#         # Call this to print the final table\n",
    "#         self.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527efbbd",
   "metadata": {
    "papermill": {
     "duration": 0.035061,
     "end_time": "2024-08-30T04:01:35.458408",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.423347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c3a95e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.528086Z",
     "iopub.status.busy": "2024-08-30T04:01:35.527750Z",
     "iopub.status.idle": "2024-08-30T04:01:35.532148Z",
     "shell.execute_reply": "2024-08-30T04:01:35.531243Z"
    },
    "papermill": {
     "duration": 0.041185,
     "end_time": "2024-08-30T04:01:35.534207",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.493022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To ignore warinings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d467d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T04:01:35.604458Z",
     "iopub.status.busy": "2024-08-30T04:01:35.603567Z",
     "iopub.status.idle": "2024-08-30T08:59:06.662303Z",
     "shell.execute_reply": "2024-08-30T08:59:06.661272Z"
    },
    "papermill": {
     "duration": 17851.097161,
     "end_time": "2024-08-30T08:59:06.664835",
     "exception": false,
     "start_time": "2024-08-30T04:01:35.567674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-30 08:59:06</td></tr>\n",
       "<tr><td>Running for: </td><td>04:57:14.06        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.2/31.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=48<br>Bracket: Iter 12.000: 0.9217179409077063 | Iter 6.000: 0.9113615595491928 | Iter 3.000: 0.8903441973804447<br>Logical resource usage: 2.0/4 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_rate</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  early_stopping_epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_add57ec9</td><td>TERMINATED</td><td>172.19.2.2:346 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0317074</td><td style=\"text-align: right;\">  0.919586</td><td style=\"text-align: right;\">      0.0351427</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0248368 </td><td style=\"text-align: right;\">        0.937784</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_5ec8c748</td><td>TERMINATED</td><td>172.19.2.2:381 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.077708 </td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0907448</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.103782  </td><td style=\"text-align: right;\">        0.886707</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_1fdb2f93</td><td>TERMINATED</td><td>172.19.2.2:487 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0532578</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0547496</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.0562414 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_d0297615</td><td>TERMINATED</td><td>172.19.2.2:575 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0535612</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539215</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0542818 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_80810858</td><td>TERMINATED</td><td>172.19.2.2:650 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.054185 </td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0558612</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0575374 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a18aec9d</td><td>TERMINATED</td><td>172.19.2.2:750 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0385408</td><td style=\"text-align: right;\">  0.911971</td><td style=\"text-align: right;\">      0.042955 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0473692 </td><td style=\"text-align: right;\">        0.893327</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_0124813f</td><td>TERMINATED</td><td>172.19.2.2:828 </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0322777</td><td style=\"text-align: right;\">  0.917454</td><td style=\"text-align: right;\">      0.0383924</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0200483 </td><td style=\"text-align: right;\">        0.950259</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_4d817e87</td><td>TERMINATED</td><td>172.19.2.2:922 </td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0588828</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0635911</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0682994 </td><td style=\"text-align: right;\">        0.889126</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_34252e20</td><td>TERMINATED</td><td>172.19.2.2:1027</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.195149 </td><td style=\"text-align: right;\">  0.250076</td><td style=\"text-align: right;\">      0.196266 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.197382  </td><td style=\"text-align: right;\">        0.331254</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_6f290be0</td><td>TERMINATED</td><td>172.19.2.2:1100</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0334983</td><td style=\"text-align: right;\">  0.915626</td><td style=\"text-align: right;\">      0.0386173</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.0232604 </td><td style=\"text-align: right;\">        0.941285</td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_f66d3bba</td><td>TERMINATED</td><td>172.19.2.2:1184</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0605446</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0654397</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0703349 </td><td style=\"text-align: right;\">        0.888967</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_0a0c1650</td><td>TERMINATED</td><td>172.19.2.2:1289</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0560583</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0584762</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.060894  </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_6552b9bd</td><td>TERMINATED</td><td>172.19.2.2:1376</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0293372</td><td style=\"text-align: right;\">  0.919281</td><td style=\"text-align: right;\">      0.0370153</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0139809 </td><td style=\"text-align: right;\">        0.962289</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_faf3a994</td><td>TERMINATED</td><td>172.19.2.2:1497</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0319695</td><td style=\"text-align: right;\">  0.920195</td><td style=\"text-align: right;\">      0.0365321</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0228442 </td><td style=\"text-align: right;\">        0.941699</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_4bb495b7</td><td>TERMINATED</td><td>172.19.2.2:1561</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0297843</td><td style=\"text-align: right;\">  0.92385 </td><td style=\"text-align: right;\">      0.0328333</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0236863 </td><td style=\"text-align: right;\">        0.941412</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_f241308b</td><td>TERMINATED</td><td>172.19.2.2:1688</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0307734</td><td style=\"text-align: right;\">  0.91989 </td><td style=\"text-align: right;\">      0.033852 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0246163 </td><td style=\"text-align: right;\">        0.937498</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_57723bad</td><td>TERMINATED</td><td>172.19.2.2:1754</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0544695</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0546436</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0541211 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_00355967</td><td>TERMINATED</td><td>172.19.2.2:1861</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0550008</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.058974 </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0629473 </td><td style=\"text-align: right;\">        0.889062</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_cf8ae0a2</td><td>TERMINATED</td><td>172.19.2.2:1950</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.195401 </td><td style=\"text-align: right;\">  0.359427</td><td style=\"text-align: right;\">      0.196923 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.198445  </td><td style=\"text-align: right;\">        0.366547</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e48e24e2</td><td>TERMINATED</td><td>172.19.2.2:2033</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0269677</td><td style=\"text-align: right;\">  0.925069</td><td style=\"text-align: right;\">      0.0342537</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0123956 </td><td style=\"text-align: right;\">        0.965185</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_c8ead3f8</td><td>TERMINATED</td><td>172.19.2.2:2107</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0322327</td><td style=\"text-align: right;\">  0.921413</td><td style=\"text-align: right;\">      0.0356488</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0254007 </td><td style=\"text-align: right;\">        0.935429</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5ffc40fd</td><td>TERMINATED</td><td>172.19.2.2:2204</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0536318</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539467</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0542616 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_7020b91c</td><td>TERMINATED</td><td>172.19.2.2:2297</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0302712</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.038417 </td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.0139795 </td><td style=\"text-align: right;\">        0.960252</td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_fec9f23e</td><td>TERMINATED</td><td>172.19.2.2:2370</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0264522</td><td style=\"text-align: right;\">  0.926287</td><td style=\"text-align: right;\">      0.0365842</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">  0.00618815</td><td style=\"text-align: right;\">        0.977023</td><td style=\"text-align: right;\">                    15</td></tr>\n",
       "<tr><td>train_fn_ebac6b49</td><td>TERMINATED</td><td>172.19.2.2:2494</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0278324</td><td style=\"text-align: right;\">  0.926287</td><td style=\"text-align: right;\">      0.0355387</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.01242   </td><td style=\"text-align: right;\">        0.964103</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_08d7fe01</td><td>TERMINATED</td><td>172.19.2.2:2586</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0301738</td><td style=\"text-align: right;\">  0.922327</td><td style=\"text-align: right;\">      0.0334125</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0236963 </td><td style=\"text-align: right;\">        0.940458</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_11b8d27b</td><td>TERMINATED</td><td>172.19.2.2:2685</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0313066</td><td style=\"text-align: right;\">  0.918672</td><td style=\"text-align: right;\">      0.0354575</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0230048 </td><td style=\"text-align: right;\">        0.942813</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_793f8efc</td><td>TERMINATED</td><td>172.19.2.2:2777</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0296267</td><td style=\"text-align: right;\">  0.924155</td><td style=\"text-align: right;\">      0.0311826</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0265149 </td><td style=\"text-align: right;\">        0.93457 </td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_87d87140</td><td>TERMINATED</td><td>172.19.2.2:2877</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0321093</td><td style=\"text-align: right;\">  0.917758</td><td style=\"text-align: right;\">      0.0342179</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0278919 </td><td style=\"text-align: right;\">        0.93056 </td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_875dabea</td><td>TERMINATED</td><td>172.19.2.2:2967</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0352774</td><td style=\"text-align: right;\">  0.919281</td><td style=\"text-align: right;\">      0.0380844</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0296636 </td><td style=\"text-align: right;\">        0.930083</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_701347b0</td><td>TERMINATED</td><td>172.19.2.2:3045</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0314559</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.0335169</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0273338 </td><td style=\"text-align: right;\">        0.930624</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_d2dc7605</td><td>TERMINATED</td><td>172.19.2.2:3134</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0539175</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0540316</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0541456 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_44956ce0</td><td>TERMINATED</td><td>172.19.2.2:3212</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0545415</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0571052</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0596689 </td><td style=\"text-align: right;\">        0.889285</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_0e878f03</td><td>TERMINATED</td><td>172.19.2.2:3306</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0340897</td><td style=\"text-align: right;\">  0.918977</td><td style=\"text-align: right;\">      0.0345506</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0331679 </td><td style=\"text-align: right;\">        0.923718</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_b982a6b4</td><td>TERMINATED</td><td>172.19.2.2:3384</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0411603</td><td style=\"text-align: right;\">  0.898873</td><td style=\"text-align: right;\">      0.0437211</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0462819 </td><td style=\"text-align: right;\">        0.895363</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_60298604</td><td>TERMINATED</td><td>172.19.2.2:3482</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0291687</td><td style=\"text-align: right;\">  0.924764</td><td style=\"text-align: right;\">      0.0334908</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">  0.0205246 </td><td style=\"text-align: right;\">        0.94695 </td><td style=\"text-align: right;\">                     9</td></tr>\n",
       "<tr><td>train_fn_67aa447c</td><td>TERMINATED</td><td>172.19.2.2:3556</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0534496</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0538877</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0543259 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5e8c22a6</td><td>TERMINATED</td><td>172.19.2.2:3656</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0540123</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0564026</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0587928 </td><td style=\"text-align: right;\">        0.88903 </td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_123119dd</td><td>TERMINATED</td><td>172.19.2.2:3745</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0536382</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0559168</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0581954 </td><td style=\"text-align: right;\">        0.889158</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_5532c59a</td><td>TERMINATED</td><td>172.19.2.2:3832</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0535181</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0561833</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0588486 </td><td style=\"text-align: right;\">        0.889221</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_ff0288b1</td><td>TERMINATED</td><td>172.19.2.2:3921</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.178426 </td><td style=\"text-align: right;\">  0.814804</td><td style=\"text-align: right;\">      0.178886 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.179346  </td><td style=\"text-align: right;\">        0.71069 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_79356eb1</td><td>TERMINATED</td><td>172.19.2.2:4009</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.195322 </td><td style=\"text-align: right;\">  0.301553</td><td style=\"text-align: right;\">      0.196731 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.19814   </td><td style=\"text-align: right;\">        0.308818</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_806ee046</td><td>TERMINATED</td><td>172.19.2.2:4077</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.184568 </td><td style=\"text-align: right;\">  0.712763</td><td style=\"text-align: right;\">      0.185462 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.186356  </td><td style=\"text-align: right;\">        0.61089 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_0d3c229c</td><td>TERMINATED</td><td>172.19.2.2:4168</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.194009 </td><td style=\"text-align: right;\">  0.510509</td><td style=\"text-align: right;\">      0.195218 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.196426  </td><td style=\"text-align: right;\">        0.393502</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8a6053ef</td><td>TERMINATED</td><td>172.19.2.2:4233</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.177246 </td><td style=\"text-align: right;\">  0.729211</td><td style=\"text-align: right;\">      0.183241 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.189237  </td><td style=\"text-align: right;\">        0.50654 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_e47a3e38</td><td>TERMINATED</td><td>172.19.2.2:4326</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.176085 </td><td style=\"text-align: right;\">  0.88608 </td><td style=\"text-align: right;\">      0.180214 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.184343  </td><td style=\"text-align: right;\">        0.614582</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_5561e24c</td><td>TERMINATED</td><td>172.19.2.2:4390</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.030207 </td><td style=\"text-align: right;\">  0.916235</td><td style=\"text-align: right;\">      0.0391718</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0122775 </td><td style=\"text-align: right;\">        0.966362</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_7cb78428</td><td>TERMINATED</td><td>172.19.2.2:4483</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0309148</td><td style=\"text-align: right;\">  0.912885</td><td style=\"text-align: right;\">      0.0411559</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">  0.0104325 </td><td style=\"text-align: right;\">        0.967985</td><td style=\"text-align: right;\">                    15</td></tr>\n",
       "<tr><td>train_fn_57794e70</td><td>TERMINATED</td><td>172.19.2.2:4598</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0539068</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0540941</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">  0.0542813 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     4</td></tr>\n",
       "<tr><td>train_fn_6b4e14ba</td><td>TERMINATED</td><td>172.19.2.2:4689</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0541026</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0541303</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0541581 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_124ea8fd</td><td>TERMINATED</td><td>172.19.2.2:4771</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0510927</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0534015</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0557103 </td><td style=\"text-align: right;\">        0.889126</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_94a7cf7b</td><td>TERMINATED</td><td>172.19.2.2:4860</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0338237</td><td style=\"text-align: right;\">  0.917758</td><td style=\"text-align: right;\">      0.0363128</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0288455 </td><td style=\"text-align: right;\">        0.929828</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_5ea91970</td><td>TERMINATED</td><td>172.19.2.2:4943</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0562428</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0604381</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0646335 </td><td style=\"text-align: right;\">        0.889158</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_690efdf8</td><td>TERMINATED</td><td>172.19.2.2:5048</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0285522</td><td style=\"text-align: right;\">  0.922023</td><td style=\"text-align: right;\">      0.0346478</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0163611 </td><td style=\"text-align: right;\">        0.955924</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_ee600f50</td><td>TERMINATED</td><td>172.19.2.2:5129</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0314295</td><td style=\"text-align: right;\">  0.922632</td><td style=\"text-align: right;\">      0.0351107</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0240672 </td><td style=\"text-align: right;\">        0.93858 </td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_ee0ed572</td><td>TERMINATED</td><td>172.19.2.2:5246</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0315101</td><td style=\"text-align: right;\">  0.917454</td><td style=\"text-align: right;\">      0.0328672</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0287959 </td><td style=\"text-align: right;\">        0.929097</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_bc968475</td><td>TERMINATED</td><td>172.19.2.2:5318</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0368232</td><td style=\"text-align: right;\">  0.909534</td><td style=\"text-align: right;\">      0.0385745</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0403258 </td><td style=\"text-align: right;\">        0.908952</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_8177c240</td><td>TERMINATED</td><td>172.19.2.2:5420</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0295583</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.0351269</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0184211 </td><td style=\"text-align: right;\">        0.952201</td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_6dade00a</td><td>TERMINATED</td><td>172.19.2.2:5493</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0276656</td><td style=\"text-align: right;\">  0.922632</td><td style=\"text-align: right;\">      0.035836 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0113248 </td><td style=\"text-align: right;\">        0.966776</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_00e64845</td><td>TERMINATED</td><td>172.19.2.2:5596</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-05 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0321879</td><td style=\"text-align: right;\">  0.923241</td><td style=\"text-align: right;\">      0.0359092</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0247454 </td><td style=\"text-align: right;\">        0.936098</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_276c7ed2</td><td>TERMINATED</td><td>172.19.2.2:5686</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0538435</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0540455</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">  0.0542474 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     4</td></tr>\n",
       "<tr><td>train_fn_c3da0df5</td><td>TERMINATED</td><td>172.19.2.2:5763</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0536935</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539722</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0542509 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_97531d4c</td><td>TERMINATED</td><td>172.19.2.2:5849</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0536582</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539915</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0543249 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_6fd54249</td><td>TERMINATED</td><td>172.19.2.2:5930</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0344474</td><td style=\"text-align: right;\">  0.918367</td><td style=\"text-align: right;\">      0.0409315</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">  0.0214792 </td><td style=\"text-align: right;\">        0.94695 </td><td style=\"text-align: right;\">                    11</td></tr>\n",
       "<tr><td>train_fn_0b71a9cf</td><td>TERMINATED</td><td>172.19.2.2:6021</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">0.0304143</td><td style=\"text-align: right;\">  0.922327</td><td style=\"text-align: right;\">      0.035324 </td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">  0.0205951 </td><td style=\"text-align: right;\">        0.947745</td><td style=\"text-align: right;\">                    12</td></tr>\n",
       "<tr><td>train_fn_8c4e3060</td><td>TERMINATED</td><td>172.19.2.2:6125</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0414577</td><td style=\"text-align: right;\">  0.902833</td><td style=\"text-align: right;\">      0.0437179</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.045978  </td><td style=\"text-align: right;\">        0.897527</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a272019b</td><td>TERMINATED</td><td>172.19.2.2:6222</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0471224</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0505889</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0540554 </td><td style=\"text-align: right;\">        0.889221</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_0a508075</td><td>TERMINATED</td><td>172.19.2.2:6296</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">0.0508935</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0534949</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0560963 </td><td style=\"text-align: right;\">        0.889221</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_4d6fe2a8</td><td>TERMINATED</td><td>172.19.2.2:6398</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0395011</td><td style=\"text-align: right;\">  0.90862 </td><td style=\"text-align: right;\">      0.042809 </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0461169 </td><td style=\"text-align: right;\">        0.894154</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_402f8a04</td><td>TERMINATED</td><td>172.19.2.2:6472</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0495016</td><td style=\"text-align: right;\">  0.886384</td><td style=\"text-align: right;\">      0.0497363</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0490322 </td><td style=\"text-align: right;\">        0.889921</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a796638b</td><td>TERMINATED</td><td>172.19.2.2:6573</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.180099 </td><td style=\"text-align: right;\">  0.644228</td><td style=\"text-align: right;\">      0.182639 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.185179  </td><td style=\"text-align: right;\">        0.58018 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_cad47028</td><td>TERMINATED</td><td>172.19.2.2:6644</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.188091 </td><td style=\"text-align: right;\">  0.499239</td><td style=\"text-align: right;\">      0.188423 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.188756  </td><td style=\"text-align: right;\">        0.532794</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_83786913</td><td>TERMINATED</td><td>172.19.2.2:6730</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">2e-07 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.188098 </td><td style=\"text-align: right;\">  0.496497</td><td style=\"text-align: right;\">      0.188405 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.188711  </td><td style=\"text-align: right;\">        0.510072</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_6caf321e</td><td>TERMINATED</td><td>172.19.2.2:6802</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        5e-06 </td><td style=\"text-align: right;\">0.0387004</td><td style=\"text-align: right;\">  0.906793</td><td style=\"text-align: right;\">      0.0399367</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0411731 </td><td style=\"text-align: right;\">        0.903256</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_eaf8343a</td><td>TERMINATED</td><td>172.19.2.2:6887</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0405504</td><td style=\"text-align: right;\">  0.906793</td><td style=\"text-align: right;\">      0.04271  </td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0448695 </td><td style=\"text-align: right;\">        0.897177</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_fdbd046b</td><td>TERMINATED</td><td>172.19.2.2:6975</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0311137</td><td style=\"text-align: right;\">  0.923241</td><td style=\"text-align: right;\">      0.0341184</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0251043 </td><td style=\"text-align: right;\">        0.939216</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_61001dfe</td><td>TERMINATED</td><td>172.19.2.2:7059</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        2e-06 </td><td style=\"text-align: right;\">0.0694935</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0775932</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0856929 </td><td style=\"text-align: right;\">        0.888839</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_c32a767d</td><td>TERMINATED</td><td>172.19.2.2:7160</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.25</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0553803</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0583036</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0612269 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a4bf0a38</td><td>TERMINATED</td><td>172.19.2.2:7238</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0319038</td><td style=\"text-align: right;\">  0.918977</td><td style=\"text-align: right;\">      0.0348506</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">  0.0260102 </td><td style=\"text-align: right;\">        0.93613 </td><td style=\"text-align: right;\">                     7</td></tr>\n",
       "<tr><td>train_fn_b663a7fd</td><td>TERMINATED</td><td>172.19.2.2:7335</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.127202 </td><td style=\"text-align: right;\">  0.815717</td><td style=\"text-align: right;\">      0.141606 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.15601   </td><td style=\"text-align: right;\">        0.78172 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_84af14f1</td><td>TERMINATED</td><td>172.19.2.2:7421</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.35</td><td style=\"text-align: right;\">       7</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0529984</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0533645</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">  0.0537305 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     4</td></tr>\n",
       "<tr><td>train_fn_aab1d64d</td><td>TERMINATED</td><td>172.19.2.2:7494</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">5e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.170851 </td><td style=\"text-align: right;\">  0.784648</td><td style=\"text-align: right;\">      0.174897 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.178944  </td><td style=\"text-align: right;\">        0.658689</td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "<tr><td>train_fn_8fa29fb4</td><td>TERMINATED</td><td>172.19.2.2:7582</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.0274156</td><td style=\"text-align: right;\">  0.928115</td><td style=\"text-align: right;\">      0.0343463</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0135543 </td><td style=\"text-align: right;\">        0.962352</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_78b05d6f</td><td>TERMINATED</td><td>172.19.2.2:7657</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0318134</td><td style=\"text-align: right;\">  0.918977</td><td style=\"text-align: right;\">      0.0350074</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0254254 </td><td style=\"text-align: right;\">        0.937784</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_d56a7c3c</td><td>TERMINATED</td><td>172.19.2.2:7773</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0377594</td><td style=\"text-align: right;\">  0.910143</td><td style=\"text-align: right;\">      0.0401428</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0425263 </td><td style=\"text-align: right;\">        0.901792</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_53589a5d</td><td>TERMINATED</td><td>172.19.2.2:7848</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0296983</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.0360456</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0170037 </td><td style=\"text-align: right;\">        0.956783</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_836f1851</td><td>TERMINATED</td><td>172.19.2.2:7945</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        0.01  </td><td style=\"text-align: right;\">0.0315918</td><td style=\"text-align: right;\">  0.91989 </td><td style=\"text-align: right;\">      0.0383974</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">  0.0179806 </td><td style=\"text-align: right;\">        0.955224</td><td style=\"text-align: right;\">                     8</td></tr>\n",
       "<tr><td>train_fn_ba76d170</td><td>TERMINATED</td><td>172.19.2.2:8041</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">3e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0317962</td><td style=\"text-align: right;\">  0.922023</td><td style=\"text-align: right;\">      0.032596 </td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0301966 </td><td style=\"text-align: right;\">        0.926201</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_03430721</td><td>TERMINATED</td><td>172.19.2.2:8126</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-06 </td><td style=\"text-align: right;\">        5e-05 </td><td style=\"text-align: right;\">0.0603003</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0651258</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0699513 </td><td style=\"text-align: right;\">        0.88903 </td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_5b807911</td><td>TERMINATED</td><td>172.19.2.2:8208</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0309461</td><td style=\"text-align: right;\">  0.921413</td><td style=\"text-align: right;\">      0.0325909</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0276564 </td><td style=\"text-align: right;\">        0.930051</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_8409be4b</td><td>TERMINATED</td><td>172.19.2.2:8293</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0287593</td><td style=\"text-align: right;\">  0.923546</td><td style=\"text-align: right;\">      0.0357683</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0147412 </td><td style=\"text-align: right;\">        0.959775</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_8970388c</td><td>TERMINATED</td><td>172.19.2.2:8376</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0302501</td><td style=\"text-align: right;\">  0.922936</td><td style=\"text-align: right;\">      0.0374312</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0158879 </td><td style=\"text-align: right;\">        0.960379</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_782d82b8</td><td>TERMINATED</td><td>172.19.2.2:8484</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.2 </td><td style=\"text-align: right;\">      15</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">        2e-05 </td><td style=\"text-align: right;\">0.0274478</td><td style=\"text-align: right;\">  0.922936</td><td style=\"text-align: right;\">      0.0368302</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">  0.00868296</td><td style=\"text-align: right;\">        0.972377</td><td style=\"text-align: right;\">                    15</td></tr>\n",
       "<tr><td>train_fn_fad21b68</td><td>TERMINATED</td><td>172.19.2.2:8567</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0537963</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0541364</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0544765 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_a25b3246</td><td>TERMINATED</td><td>172.19.2.2:8669</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">0.0550429</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0555006</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0541274 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_990b4f51</td><td>TERMINATED</td><td>172.19.2.2:8769</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      12</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0405686</td><td style=\"text-align: right;\">  0.906488</td><td style=\"text-align: right;\">      0.0431135</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">  0.0456584 </td><td style=\"text-align: right;\">        0.896509</td><td style=\"text-align: right;\">                     6</td></tr>\n",
       "<tr><td>train_fn_5482050d</td><td>TERMINATED</td><td>172.19.2.2:8835</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0407164</td><td style=\"text-align: right;\">  0.896741</td><td style=\"text-align: right;\">      0.0427855</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0448546 </td><td style=\"text-align: right;\">        0.900264</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_f78b41ba</td><td>TERMINATED</td><td>172.19.2.2:8940</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          0.3 </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">5e-06 </td><td style=\"text-align: right;\">        0.0002</td><td style=\"text-align: right;\">0.0394876</td><td style=\"text-align: right;\">  0.909534</td><td style=\"text-align: right;\">      0.0429784</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">  0.0464692 </td><td style=\"text-align: right;\">        0.895204</td><td style=\"text-align: right;\">                     5</td></tr>\n",
       "<tr><td>train_fn_0a4da854</td><td>TERMINATED</td><td>172.19.2.2:9015</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.4 </td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.032565 </td><td style=\"text-align: right;\">  0.920195</td><td style=\"text-align: right;\">      0.0358475</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">  0.0260001 </td><td style=\"text-align: right;\">        0.936448</td><td style=\"text-align: right;\">                    10</td></tr>\n",
       "<tr><td>train_fn_a528c056</td><td>TERMINATED</td><td>172.19.2.2:9111</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">          0.27</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">1e-07 </td><td style=\"text-align: right;\">        0.001 </td><td style=\"text-align: right;\">0.188194 </td><td style=\"text-align: right;\">  0.565946</td><td style=\"text-align: right;\">      0.188274 </td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">  0.188034  </td><td style=\"text-align: right;\">        0.55418 </td><td style=\"text-align: right;\">                     3</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 04:01:40,053\tINFO worker.py:1753 -- Started a local Ray instance.\n",
      "2024-08-30 04:01:41,255\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-08-30 04:01:41,261\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "[I 2024-08-30 04:01:41,314] A new study created in memory with name: optuna\n",
      "\u001b[36m(train_fn pid=346)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=346)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=381)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=381)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  custom_metric</th><th style=\"text-align: right;\">  early_stopping_epoch</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  train_accuracy</th><th style=\"text-align: right;\">  train_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fn_00355967</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.058974 </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0550008</td><td style=\"text-align: right;\">        0.889062</td><td style=\"text-align: right;\">  0.0629473 </td></tr>\n",
       "<tr><td>train_fn_00e64845</td><td style=\"text-align: right;\">  0.923241</td><td style=\"text-align: right;\">      0.0359092</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0321879</td><td style=\"text-align: right;\">        0.936098</td><td style=\"text-align: right;\">  0.0247454 </td></tr>\n",
       "<tr><td>train_fn_0124813f</td><td style=\"text-align: right;\">  0.917454</td><td style=\"text-align: right;\">      0.0383924</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0322777</td><td style=\"text-align: right;\">        0.950259</td><td style=\"text-align: right;\">  0.0200483 </td></tr>\n",
       "<tr><td>train_fn_03430721</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0651258</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0603003</td><td style=\"text-align: right;\">        0.88903 </td><td style=\"text-align: right;\">  0.0699513 </td></tr>\n",
       "<tr><td>train_fn_08d7fe01</td><td style=\"text-align: right;\">  0.922327</td><td style=\"text-align: right;\">      0.0334125</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0301738</td><td style=\"text-align: right;\">        0.940458</td><td style=\"text-align: right;\">  0.0236963 </td></tr>\n",
       "<tr><td>train_fn_0a0c1650</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0584762</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0560583</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.060894  </td></tr>\n",
       "<tr><td>train_fn_0a4da854</td><td style=\"text-align: right;\">  0.920195</td><td style=\"text-align: right;\">      0.0358475</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.032565 </td><td style=\"text-align: right;\">        0.936448</td><td style=\"text-align: right;\">  0.0260001 </td></tr>\n",
       "<tr><td>train_fn_0a508075</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0534949</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0508935</td><td style=\"text-align: right;\">        0.889221</td><td style=\"text-align: right;\">  0.0560963 </td></tr>\n",
       "<tr><td>train_fn_0b71a9cf</td><td style=\"text-align: right;\">  0.922327</td><td style=\"text-align: right;\">      0.035324 </td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.0304143</td><td style=\"text-align: right;\">        0.947745</td><td style=\"text-align: right;\">  0.0205951 </td></tr>\n",
       "<tr><td>train_fn_0d3c229c</td><td style=\"text-align: right;\">  0.510509</td><td style=\"text-align: right;\">      0.195218 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.194009 </td><td style=\"text-align: right;\">        0.393502</td><td style=\"text-align: right;\">  0.196426  </td></tr>\n",
       "<tr><td>train_fn_0e878f03</td><td style=\"text-align: right;\">  0.918977</td><td style=\"text-align: right;\">      0.0345506</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0340897</td><td style=\"text-align: right;\">        0.923718</td><td style=\"text-align: right;\">  0.0331679 </td></tr>\n",
       "<tr><td>train_fn_11b8d27b</td><td style=\"text-align: right;\">  0.918672</td><td style=\"text-align: right;\">      0.0354575</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0313066</td><td style=\"text-align: right;\">        0.942813</td><td style=\"text-align: right;\">  0.0230048 </td></tr>\n",
       "<tr><td>train_fn_123119dd</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0559168</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0536382</td><td style=\"text-align: right;\">        0.889158</td><td style=\"text-align: right;\">  0.0581954 </td></tr>\n",
       "<tr><td>train_fn_124ea8fd</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0534015</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0510927</td><td style=\"text-align: right;\">        0.889126</td><td style=\"text-align: right;\">  0.0557103 </td></tr>\n",
       "<tr><td>train_fn_1fdb2f93</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0547496</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.0532578</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0562414 </td></tr>\n",
       "<tr><td>train_fn_276c7ed2</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0540455</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0538435</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0542474 </td></tr>\n",
       "<tr><td>train_fn_34252e20</td><td style=\"text-align: right;\">  0.250076</td><td style=\"text-align: right;\">      0.196266 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.195149 </td><td style=\"text-align: right;\">        0.331254</td><td style=\"text-align: right;\">  0.197382  </td></tr>\n",
       "<tr><td>train_fn_402f8a04</td><td style=\"text-align: right;\">  0.886384</td><td style=\"text-align: right;\">      0.0497363</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0495016</td><td style=\"text-align: right;\">        0.889921</td><td style=\"text-align: right;\">  0.0490322 </td></tr>\n",
       "<tr><td>train_fn_44956ce0</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0571052</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0545415</td><td style=\"text-align: right;\">        0.889285</td><td style=\"text-align: right;\">  0.0596689 </td></tr>\n",
       "<tr><td>train_fn_4bb495b7</td><td style=\"text-align: right;\">  0.92385 </td><td style=\"text-align: right;\">      0.0328333</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0297843</td><td style=\"text-align: right;\">        0.941412</td><td style=\"text-align: right;\">  0.0236863 </td></tr>\n",
       "<tr><td>train_fn_4d6fe2a8</td><td style=\"text-align: right;\">  0.90862 </td><td style=\"text-align: right;\">      0.042809 </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0395011</td><td style=\"text-align: right;\">        0.894154</td><td style=\"text-align: right;\">  0.0461169 </td></tr>\n",
       "<tr><td>train_fn_4d817e87</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0635911</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0588828</td><td style=\"text-align: right;\">        0.889126</td><td style=\"text-align: right;\">  0.0682994 </td></tr>\n",
       "<tr><td>train_fn_53589a5d</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.0360456</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0296983</td><td style=\"text-align: right;\">        0.956783</td><td style=\"text-align: right;\">  0.0170037 </td></tr>\n",
       "<tr><td>train_fn_5482050d</td><td style=\"text-align: right;\">  0.896741</td><td style=\"text-align: right;\">      0.0427855</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0407164</td><td style=\"text-align: right;\">        0.900264</td><td style=\"text-align: right;\">  0.0448546 </td></tr>\n",
       "<tr><td>train_fn_5532c59a</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0561833</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0535181</td><td style=\"text-align: right;\">        0.889221</td><td style=\"text-align: right;\">  0.0588486 </td></tr>\n",
       "<tr><td>train_fn_5561e24c</td><td style=\"text-align: right;\">  0.916235</td><td style=\"text-align: right;\">      0.0391718</td><td style=\"text-align: right;\">                    12</td><td style=\"text-align: right;\">0.030207 </td><td style=\"text-align: right;\">        0.966362</td><td style=\"text-align: right;\">  0.0122775 </td></tr>\n",
       "<tr><td>train_fn_57723bad</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0546436</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0544695</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0541211 </td></tr>\n",
       "<tr><td>train_fn_57794e70</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0540941</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0539068</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0542813 </td></tr>\n",
       "<tr><td>train_fn_5b807911</td><td style=\"text-align: right;\">  0.921413</td><td style=\"text-align: right;\">      0.0325909</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0309461</td><td style=\"text-align: right;\">        0.930051</td><td style=\"text-align: right;\">  0.0276564 </td></tr>\n",
       "<tr><td>train_fn_5e8c22a6</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0564026</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0540123</td><td style=\"text-align: right;\">        0.88903 </td><td style=\"text-align: right;\">  0.0587928 </td></tr>\n",
       "<tr><td>train_fn_5ea91970</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0604381</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0562428</td><td style=\"text-align: right;\">        0.889158</td><td style=\"text-align: right;\">  0.0646335 </td></tr>\n",
       "<tr><td>train_fn_5ec8c748</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0907448</td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.077708 </td><td style=\"text-align: right;\">        0.886707</td><td style=\"text-align: right;\">  0.103782  </td></tr>\n",
       "<tr><td>train_fn_5ffc40fd</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539467</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0536318</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0542616 </td></tr>\n",
       "<tr><td>train_fn_60298604</td><td style=\"text-align: right;\">  0.924764</td><td style=\"text-align: right;\">      0.0334908</td><td style=\"text-align: right;\">                     9</td><td style=\"text-align: right;\">0.0291687</td><td style=\"text-align: right;\">        0.94695 </td><td style=\"text-align: right;\">  0.0205246 </td></tr>\n",
       "<tr><td>train_fn_61001dfe</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0775932</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0694935</td><td style=\"text-align: right;\">        0.888839</td><td style=\"text-align: right;\">  0.0856929 </td></tr>\n",
       "<tr><td>train_fn_6552b9bd</td><td style=\"text-align: right;\">  0.919281</td><td style=\"text-align: right;\">      0.0370153</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0293372</td><td style=\"text-align: right;\">        0.962289</td><td style=\"text-align: right;\">  0.0139809 </td></tr>\n",
       "<tr><td>train_fn_67aa447c</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0538877</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0534496</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0543259 </td></tr>\n",
       "<tr><td>train_fn_690efdf8</td><td style=\"text-align: right;\">  0.922023</td><td style=\"text-align: right;\">      0.0346478</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0285522</td><td style=\"text-align: right;\">        0.955924</td><td style=\"text-align: right;\">  0.0163611 </td></tr>\n",
       "<tr><td>train_fn_6b4e14ba</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0541303</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0541026</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0541581 </td></tr>\n",
       "<tr><td>train_fn_6caf321e</td><td style=\"text-align: right;\">  0.906793</td><td style=\"text-align: right;\">      0.0399367</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0387004</td><td style=\"text-align: right;\">        0.903256</td><td style=\"text-align: right;\">  0.0411731 </td></tr>\n",
       "<tr><td>train_fn_6dade00a</td><td style=\"text-align: right;\">  0.922632</td><td style=\"text-align: right;\">      0.035836 </td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0276656</td><td style=\"text-align: right;\">        0.966776</td><td style=\"text-align: right;\">  0.0113248 </td></tr>\n",
       "<tr><td>train_fn_6f290be0</td><td style=\"text-align: right;\">  0.915626</td><td style=\"text-align: right;\">      0.0386173</td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0334983</td><td style=\"text-align: right;\">        0.941285</td><td style=\"text-align: right;\">  0.0232604 </td></tr>\n",
       "<tr><td>train_fn_6fd54249</td><td style=\"text-align: right;\">  0.918367</td><td style=\"text-align: right;\">      0.0409315</td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0344474</td><td style=\"text-align: right;\">        0.94695 </td><td style=\"text-align: right;\">  0.0214792 </td></tr>\n",
       "<tr><td>train_fn_701347b0</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.0335169</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0314559</td><td style=\"text-align: right;\">        0.930624</td><td style=\"text-align: right;\">  0.0273338 </td></tr>\n",
       "<tr><td>train_fn_7020b91c</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.038417 </td><td style=\"text-align: right;\">                    11</td><td style=\"text-align: right;\">0.0302712</td><td style=\"text-align: right;\">        0.960252</td><td style=\"text-align: right;\">  0.0139795 </td></tr>\n",
       "<tr><td>train_fn_782d82b8</td><td style=\"text-align: right;\">  0.922936</td><td style=\"text-align: right;\">      0.0368302</td><td style=\"text-align: right;\">                    15</td><td style=\"text-align: right;\">0.0274478</td><td style=\"text-align: right;\">        0.972377</td><td style=\"text-align: right;\">  0.00868296</td></tr>\n",
       "<tr><td>train_fn_78b05d6f</td><td style=\"text-align: right;\">  0.918977</td><td style=\"text-align: right;\">      0.0350074</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0318134</td><td style=\"text-align: right;\">        0.937784</td><td style=\"text-align: right;\">  0.0254254 </td></tr>\n",
       "<tr><td>train_fn_79356eb1</td><td style=\"text-align: right;\">  0.301553</td><td style=\"text-align: right;\">      0.196731 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.195322 </td><td style=\"text-align: right;\">        0.308818</td><td style=\"text-align: right;\">  0.19814   </td></tr>\n",
       "<tr><td>train_fn_793f8efc</td><td style=\"text-align: right;\">  0.924155</td><td style=\"text-align: right;\">      0.0311826</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0296267</td><td style=\"text-align: right;\">        0.93457 </td><td style=\"text-align: right;\">  0.0265149 </td></tr>\n",
       "<tr><td>train_fn_7cb78428</td><td style=\"text-align: right;\">  0.912885</td><td style=\"text-align: right;\">      0.0411559</td><td style=\"text-align: right;\">                    15</td><td style=\"text-align: right;\">0.0309148</td><td style=\"text-align: right;\">        0.967985</td><td style=\"text-align: right;\">  0.0104325 </td></tr>\n",
       "<tr><td>train_fn_806ee046</td><td style=\"text-align: right;\">  0.712763</td><td style=\"text-align: right;\">      0.185462 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.184568 </td><td style=\"text-align: right;\">        0.61089 </td><td style=\"text-align: right;\">  0.186356  </td></tr>\n",
       "<tr><td>train_fn_80810858</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0558612</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.054185 </td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0575374 </td></tr>\n",
       "<tr><td>train_fn_8177c240</td><td style=\"text-align: right;\">  0.920804</td><td style=\"text-align: right;\">      0.0351269</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0295583</td><td style=\"text-align: right;\">        0.952201</td><td style=\"text-align: right;\">  0.0184211 </td></tr>\n",
       "<tr><td>train_fn_836f1851</td><td style=\"text-align: right;\">  0.91989 </td><td style=\"text-align: right;\">      0.0383974</td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">0.0315918</td><td style=\"text-align: right;\">        0.955224</td><td style=\"text-align: right;\">  0.0179806 </td></tr>\n",
       "<tr><td>train_fn_83786913</td><td style=\"text-align: right;\">  0.496497</td><td style=\"text-align: right;\">      0.188405 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.188098 </td><td style=\"text-align: right;\">        0.510072</td><td style=\"text-align: right;\">  0.188711  </td></tr>\n",
       "<tr><td>train_fn_8409be4b</td><td style=\"text-align: right;\">  0.923546</td><td style=\"text-align: right;\">      0.0357683</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0287593</td><td style=\"text-align: right;\">        0.959775</td><td style=\"text-align: right;\">  0.0147412 </td></tr>\n",
       "<tr><td>train_fn_84af14f1</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0533645</td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">0.0529984</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0537305 </td></tr>\n",
       "<tr><td>train_fn_875dabea</td><td style=\"text-align: right;\">  0.919281</td><td style=\"text-align: right;\">      0.0380844</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0352774</td><td style=\"text-align: right;\">        0.930083</td><td style=\"text-align: right;\">  0.0296636 </td></tr>\n",
       "<tr><td>train_fn_87d87140</td><td style=\"text-align: right;\">  0.917758</td><td style=\"text-align: right;\">      0.0342179</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0321093</td><td style=\"text-align: right;\">        0.93056 </td><td style=\"text-align: right;\">  0.0278919 </td></tr>\n",
       "<tr><td>train_fn_8970388c</td><td style=\"text-align: right;\">  0.922936</td><td style=\"text-align: right;\">      0.0374312</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0302501</td><td style=\"text-align: right;\">        0.960379</td><td style=\"text-align: right;\">  0.0158879 </td></tr>\n",
       "<tr><td>train_fn_8a6053ef</td><td style=\"text-align: right;\">  0.729211</td><td style=\"text-align: right;\">      0.183241 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.177246 </td><td style=\"text-align: right;\">        0.50654 </td><td style=\"text-align: right;\">  0.189237  </td></tr>\n",
       "<tr><td>train_fn_8c4e3060</td><td style=\"text-align: right;\">  0.902833</td><td style=\"text-align: right;\">      0.0437179</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0414577</td><td style=\"text-align: right;\">        0.897527</td><td style=\"text-align: right;\">  0.045978  </td></tr>\n",
       "<tr><td>train_fn_8fa29fb4</td><td style=\"text-align: right;\">  0.928115</td><td style=\"text-align: right;\">      0.0343463</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0274156</td><td style=\"text-align: right;\">        0.962352</td><td style=\"text-align: right;\">  0.0135543 </td></tr>\n",
       "<tr><td>train_fn_94a7cf7b</td><td style=\"text-align: right;\">  0.917758</td><td style=\"text-align: right;\">      0.0363128</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0338237</td><td style=\"text-align: right;\">        0.929828</td><td style=\"text-align: right;\">  0.0288455 </td></tr>\n",
       "<tr><td>train_fn_97531d4c</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539915</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0536582</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0543249 </td></tr>\n",
       "<tr><td>train_fn_990b4f51</td><td style=\"text-align: right;\">  0.906488</td><td style=\"text-align: right;\">      0.0431135</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0405686</td><td style=\"text-align: right;\">        0.896509</td><td style=\"text-align: right;\">  0.0456584 </td></tr>\n",
       "<tr><td>train_fn_a18aec9d</td><td style=\"text-align: right;\">  0.911971</td><td style=\"text-align: right;\">      0.042955 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0385408</td><td style=\"text-align: right;\">        0.893327</td><td style=\"text-align: right;\">  0.0473692 </td></tr>\n",
       "<tr><td>train_fn_a25b3246</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0555006</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0550429</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0541274 </td></tr>\n",
       "<tr><td>train_fn_a272019b</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0505889</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0471224</td><td style=\"text-align: right;\">        0.889221</td><td style=\"text-align: right;\">  0.0540554 </td></tr>\n",
       "<tr><td>train_fn_a4bf0a38</td><td style=\"text-align: right;\">  0.918977</td><td style=\"text-align: right;\">      0.0348506</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0319038</td><td style=\"text-align: right;\">        0.93613 </td><td style=\"text-align: right;\">  0.0260102 </td></tr>\n",
       "<tr><td>train_fn_a528c056</td><td style=\"text-align: right;\">  0.565946</td><td style=\"text-align: right;\">      0.188274 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.188194 </td><td style=\"text-align: right;\">        0.55418 </td><td style=\"text-align: right;\">  0.188034  </td></tr>\n",
       "<tr><td>train_fn_a796638b</td><td style=\"text-align: right;\">  0.644228</td><td style=\"text-align: right;\">      0.182639 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.180099 </td><td style=\"text-align: right;\">        0.58018 </td><td style=\"text-align: right;\">  0.185179  </td></tr>\n",
       "<tr><td>train_fn_aab1d64d</td><td style=\"text-align: right;\">  0.784648</td><td style=\"text-align: right;\">      0.174897 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.170851 </td><td style=\"text-align: right;\">        0.658689</td><td style=\"text-align: right;\">  0.178944  </td></tr>\n",
       "<tr><td>train_fn_add57ec9</td><td style=\"text-align: right;\">  0.919586</td><td style=\"text-align: right;\">      0.0351427</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0317074</td><td style=\"text-align: right;\">        0.937784</td><td style=\"text-align: right;\">  0.0248368 </td></tr>\n",
       "<tr><td>train_fn_b663a7fd</td><td style=\"text-align: right;\">  0.815717</td><td style=\"text-align: right;\">      0.141606 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.127202 </td><td style=\"text-align: right;\">        0.78172 </td><td style=\"text-align: right;\">  0.15601   </td></tr>\n",
       "<tr><td>train_fn_b982a6b4</td><td style=\"text-align: right;\">  0.898873</td><td style=\"text-align: right;\">      0.0437211</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0411603</td><td style=\"text-align: right;\">        0.895363</td><td style=\"text-align: right;\">  0.0462819 </td></tr>\n",
       "<tr><td>train_fn_ba76d170</td><td style=\"text-align: right;\">  0.922023</td><td style=\"text-align: right;\">      0.032596 </td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0317962</td><td style=\"text-align: right;\">        0.926201</td><td style=\"text-align: right;\">  0.0301966 </td></tr>\n",
       "<tr><td>train_fn_bc968475</td><td style=\"text-align: right;\">  0.909534</td><td style=\"text-align: right;\">      0.0385745</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0368232</td><td style=\"text-align: right;\">        0.908952</td><td style=\"text-align: right;\">  0.0403258 </td></tr>\n",
       "<tr><td>train_fn_c32a767d</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0583036</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0553803</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0612269 </td></tr>\n",
       "<tr><td>train_fn_c3da0df5</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539722</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0536935</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0542509 </td></tr>\n",
       "<tr><td>train_fn_c8ead3f8</td><td style=\"text-align: right;\">  0.921413</td><td style=\"text-align: right;\">      0.0356488</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0322327</td><td style=\"text-align: right;\">        0.935429</td><td style=\"text-align: right;\">  0.0254007 </td></tr>\n",
       "<tr><td>train_fn_cad47028</td><td style=\"text-align: right;\">  0.499239</td><td style=\"text-align: right;\">      0.188423 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.188091 </td><td style=\"text-align: right;\">        0.532794</td><td style=\"text-align: right;\">  0.188756  </td></tr>\n",
       "<tr><td>train_fn_cf8ae0a2</td><td style=\"text-align: right;\">  0.359427</td><td style=\"text-align: right;\">      0.196923 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.195401 </td><td style=\"text-align: right;\">        0.366547</td><td style=\"text-align: right;\">  0.198445  </td></tr>\n",
       "<tr><td>train_fn_d0297615</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0539215</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0535612</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0542818 </td></tr>\n",
       "<tr><td>train_fn_d2dc7605</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0540316</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0539175</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0541456 </td></tr>\n",
       "<tr><td>train_fn_d56a7c3c</td><td style=\"text-align: right;\">  0.910143</td><td style=\"text-align: right;\">      0.0401428</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0377594</td><td style=\"text-align: right;\">        0.901792</td><td style=\"text-align: right;\">  0.0425263 </td></tr>\n",
       "<tr><td>train_fn_e47a3e38</td><td style=\"text-align: right;\">  0.88608 </td><td style=\"text-align: right;\">      0.180214 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.176085 </td><td style=\"text-align: right;\">        0.614582</td><td style=\"text-align: right;\">  0.184343  </td></tr>\n",
       "<tr><td>train_fn_e48e24e2</td><td style=\"text-align: right;\">  0.925069</td><td style=\"text-align: right;\">      0.0342537</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0269677</td><td style=\"text-align: right;\">        0.965185</td><td style=\"text-align: right;\">  0.0123956 </td></tr>\n",
       "<tr><td>train_fn_eaf8343a</td><td style=\"text-align: right;\">  0.906793</td><td style=\"text-align: right;\">      0.04271  </td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0405504</td><td style=\"text-align: right;\">        0.897177</td><td style=\"text-align: right;\">  0.0448695 </td></tr>\n",
       "<tr><td>train_fn_ebac6b49</td><td style=\"text-align: right;\">  0.926287</td><td style=\"text-align: right;\">      0.0355387</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0278324</td><td style=\"text-align: right;\">        0.964103</td><td style=\"text-align: right;\">  0.01242   </td></tr>\n",
       "<tr><td>train_fn_ee0ed572</td><td style=\"text-align: right;\">  0.917454</td><td style=\"text-align: right;\">      0.0328672</td><td style=\"text-align: right;\">                     7</td><td style=\"text-align: right;\">0.0315101</td><td style=\"text-align: right;\">        0.929097</td><td style=\"text-align: right;\">  0.0287959 </td></tr>\n",
       "<tr><td>train_fn_ee600f50</td><td style=\"text-align: right;\">  0.922632</td><td style=\"text-align: right;\">      0.0351107</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0314295</td><td style=\"text-align: right;\">        0.93858 </td><td style=\"text-align: right;\">  0.0240672 </td></tr>\n",
       "<tr><td>train_fn_f241308b</td><td style=\"text-align: right;\">  0.91989 </td><td style=\"text-align: right;\">      0.033852 </td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0307734</td><td style=\"text-align: right;\">        0.937498</td><td style=\"text-align: right;\">  0.0246163 </td></tr>\n",
       "<tr><td>train_fn_f66d3bba</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0654397</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0605446</td><td style=\"text-align: right;\">        0.888967</td><td style=\"text-align: right;\">  0.0703349 </td></tr>\n",
       "<tr><td>train_fn_f78b41ba</td><td style=\"text-align: right;\">  0.909534</td><td style=\"text-align: right;\">      0.0429784</td><td style=\"text-align: right;\">                     5</td><td style=\"text-align: right;\">0.0394876</td><td style=\"text-align: right;\">        0.895204</td><td style=\"text-align: right;\">  0.0464692 </td></tr>\n",
       "<tr><td>train_fn_fad21b68</td><td style=\"text-align: right;\">  0.890344</td><td style=\"text-align: right;\">      0.0541364</td><td style=\"text-align: right;\">                     6</td><td style=\"text-align: right;\">0.0537963</td><td style=\"text-align: right;\">        0.889189</td><td style=\"text-align: right;\">  0.0544765 </td></tr>\n",
       "<tr><td>train_fn_faf3a994</td><td style=\"text-align: right;\">  0.920195</td><td style=\"text-align: right;\">      0.0365321</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0319695</td><td style=\"text-align: right;\">        0.941699</td><td style=\"text-align: right;\">  0.0228442 </td></tr>\n",
       "<tr><td>train_fn_fdbd046b</td><td style=\"text-align: right;\">  0.923241</td><td style=\"text-align: right;\">      0.0341184</td><td style=\"text-align: right;\">                    10</td><td style=\"text-align: right;\">0.0311137</td><td style=\"text-align: right;\">        0.939216</td><td style=\"text-align: right;\">  0.0251043 </td></tr>\n",
       "<tr><td>train_fn_fec9f23e</td><td style=\"text-align: right;\">  0.926287</td><td style=\"text-align: right;\">      0.0365842</td><td style=\"text-align: right;\">                    15</td><td style=\"text-align: right;\">0.0264522</td><td style=\"text-align: right;\">        0.977023</td><td style=\"text-align: right;\">  0.00618815</td></tr>\n",
       "<tr><td>train_fn_ff0288b1</td><td style=\"text-align: right;\">  0.814804</td><td style=\"text-align: right;\">      0.178886 </td><td style=\"text-align: right;\">                     3</td><td style=\"text-align: right;\">0.178426 </td><td style=\"text-align: right;\">        0.71069 </td><td style=\"text-align: right;\">  0.179346  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_fn pid=487)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=487)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=575)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=575)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=650)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=650)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=750)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=750)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=828)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=828)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=922)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=922)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1027)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1027)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1100)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1100)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1184)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1184)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1289)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1289)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1376)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1376)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1497)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1497)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1561)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1561)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1688)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1688)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1754)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1754)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1861)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1861)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=1950)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=1950)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2033)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2033)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2107)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2107)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2204)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2204)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2297)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2297)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2370)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2370)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2494)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2494)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2586)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2586)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2685)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2685)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2777)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2777)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2877)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2877)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=2967)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=2967)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3045)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3045)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3134)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3134)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3212)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3212)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3306)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3306)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3384)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3384)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3482)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3482)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3556)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3556)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3656)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3656)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3745)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3745)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3832)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3832)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=3921)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=3921)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4009)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4009)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4077)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4077)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4168)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4168)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4233)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4233)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4326)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4326)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4390)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4390)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4483)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4483)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4598)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4598)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4689)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4689)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4771)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4771)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4860)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4860)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=4943)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=4943)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5048)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5048)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5129)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5129)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5246)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5246)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5318)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5318)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5420)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5420)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5493)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5493)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5596)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5596)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5686)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5686)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5763)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5763)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5849)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5849)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=5930)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=5930)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6021)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6021)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6125)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6125)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6222)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6222)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6296)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6296)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6398)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6398)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6472)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6472)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6573)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6573)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6644)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6644)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6730)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6730)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6802)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6802)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6887)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6887)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=6975)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=6975)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7059)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7059)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7160)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7160)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7238)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7238)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7335)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7335)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7421)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7421)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7494)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7494)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7582)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7582)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7657)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7657)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7773)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7773)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7848)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7848)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=7945)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=7945)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8041)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8041)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8126)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8126)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8208)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8208)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8293)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8293)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8376)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8376)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8484)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8484)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8567)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8567)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8669)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8669)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8769)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8769)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8835)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8835)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=8940)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=8940)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=9015)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=9015)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_fn pid=9111)\u001b[0m /opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\u001b[36m(train_fn pid=9111)\u001b[0m   warnings.warn(\n",
      "2024-08-30 08:59:06,406\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_fn_2024-08-30_04-01-41' in 0.0454s.\n",
      "2024-08-30 08:59:06,459\tINFO tune.py:1041 -- Total run time: 17845.20 seconds (17834.01 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /kaggle/working/tensorboard_logs\n",
    "\n",
    "\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "# reporter = CLIReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "#     print_intermediate_tables=False\n",
    "# )\n",
    "\n",
    "reporter = JupyterNotebookReporter(\n",
    "    metric_columns=[\"loss\", \"accuracy\", \"custom_metric\", \"training_iteration\", \"train_loss\", \"train_accuracy\", \"early_stopping_epoch\"],\n",
    "    print_intermediate_tables=False,\n",
    ")\n",
    "\n",
    "# tuner = tune.run(\n",
    "#     train_fn,\n",
    "#     config=search_space,\n",
    "#     num_samples=25,\n",
    "#     progress_reporter=reporter,\n",
    "#     resources_per_trial={\"cpu\": 4, \"gpu\": 2},  # Adjust resources as needed\n",
    "#     scheduler=ASHAScheduler(\n",
    "#         metric=\"accuracy\",\n",
    "#         mode=\"max\",\n",
    "#         max_t=15,\n",
    "#         grace_period=1,\n",
    "#         reduction_factor=2\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# reporter = TensorBoardReporter(\n",
    "#     metric_columns=[\"loss\", \"accuracy\", \"training_iteration\", \"train_loss\", \"train_accuracy\"]\n",
    "# )\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    train_fn,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    search_alg=optuna_search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e21ac",
   "metadata": {
    "papermill": {
     "duration": 0.043918,
     "end_time": "2024-08-30T08:59:06.753395",
     "exception": false,
     "start_time": "2024-08-30T08:59:06.709477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPORTER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61a23d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T08:59:06.845025Z",
     "iopub.status.busy": "2024-08-30T08:59:06.843645Z",
     "iopub.status.idle": "2024-08-30T08:59:06.852757Z",
     "shell.execute_reply": "2024-08-30T08:59:06.851805Z"
    },
    "papermill": {
     "duration": 0.056734,
     "end_time": "2024-08-30T08:59:06.855031",
     "exception": false,
     "start_time": "2024-08-30T08:59:06.798297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'dropout_rate': 0.4, 'batch_size': 64, 'weight_decay': 0.001, 'lr': 3e-05, 'epochs': 10}\n",
      "Best trial final validation loss: 0.027415641583502293\n",
      "Best trial final validation accuracy: 0.9281145293938471\n",
      "Best trial final training loss: 0.013554268713358422\n",
      "Best trial final training accuracy: 0.9623524170193807\n",
      "Best trial final custom_metric: 0.03434632801857423\n",
      "Best trial final Early Stopping Epoch: 10\n",
      "NOTE: Both the accuracy are based on converting values > 0.5 to 1 and values < 0.5 to 0, hence, rely on the loss, MSE here.\n"
     ]
    }
   ],
   "source": [
    "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "best_trial = result.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "# best_checkpoint_dir = best_trial.checkpoint.value\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "print(f\"Best trial final training loss: {best_trial.last_result['train_loss']}\")\n",
    "print(f\"Best trial final training accuracy: {best_trial.last_result['train_accuracy']}\")\n",
    "print(f\"Best trial final custom_metric: {best_trial.last_result['custom_metric']}\")\n",
    "print(f\"Best trial final Early Stopping Epoch: {best_trial.last_result['early_stopping_epoch']}\")\n",
    "print(f\"NOTE: Both the accuracy are based on converting values > 0.5 to 1 and values < 0.5 to 0, hence, rely on the loss, MSE here.\")\n",
    "\n",
    "\n",
    "# print(\"\\nModel Parameters:\")\n",
    "# for name, param in best_model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18eff6",
   "metadata": {
    "papermill": {
     "duration": 0.045152,
     "end_time": "2024-08-30T08:59:06.946927",
     "exception": false,
     "start_time": "2024-08-30T08:59:06.901775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Train the Model with the Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f60f81b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T08:59:07.042190Z",
     "iopub.status.busy": "2024-08-30T08:59:07.041312Z",
     "iopub.status.idle": "2024-08-30T08:59:07.065536Z",
     "shell.execute_reply": "2024-08-30T08:59:07.064720Z"
    },
    "papermill": {
     "duration": 0.073796,
     "end_time": "2024-08-30T08:59:07.067522",
     "exception": false,
     "start_time": "2024-08-30T08:59:06.993726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_best_model(config, device, train_dataset = train_ds_pd, val_dataset = validation_ds_pd):\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Tuning Model with:\n",
    "    {config}\n",
    "    \"\"\")\n",
    "    \n",
    "#     model = EmotionModel(\n",
    "#         roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#         n_classes=7,\n",
    "#         dropout_rate=config[\"dropout_rate\"]\n",
    "#     )\n",
    "    model = EmotionModel(\n",
    "        roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "        n_classes=7,\n",
    "        dropout_rate=config[\"dropout_rate\"]\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    ## DATASET ENCODING\n",
    "    \n",
    "    train_dataset = EmotionDataset(train_ds_pd, tokenizer, max_len=128)\n",
    "    val_dataset = EmotionDataset(validation_ds_pd, tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_path = None\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    \n",
    "    ### Running for config epochs +patience+1 for introducing noise. \n",
    "    \n",
    "    for epoch in range((config[\"epochs\"]+patience+1)):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            with autocast(device_type=device.type):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "#             correct_train_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#             total_train_preds += labels.size(0)\n",
    "            correct_train_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "            total_train_preds += labels.numel()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_preds / total_train_preds\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct_val_preds = 0\n",
    "        total_val_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#                 outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "\n",
    "                with autocast(device_type=device.type):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "# NOTE: Apply the sigmoid fn to outputs after BCEWithLogitsLoss because the loss fn already comes with sigmoid applied, so passing the sigmoid applied ouput will give erraneous results.\n",
    "# If we use BCELoss then pass the sigmoid applied output.\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "#                 correct_val_preds += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "#                 total_val_preds += labels.size(0)\n",
    "                correct_val_preds += ((outputs > 0.5) == (labels > 0.5)).sum().item()\n",
    "                total_val_preds += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_preds / total_val_preds\n",
    "\n",
    "        # Save the model checkpoint after each epoch\n",
    "        checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#         torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "#         # Save the best model based on validation loss\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             best_model_path = checkpoint_path\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_since_improvement = 0\n",
    "            best_model_path = checkpoint_path\n",
    "            # Save the model checkpoint with the best validation loss\n",
    "#             checkpoint_path = f\"/kaggle/working/best_checkpoint.pt\"\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config  # Save configuration\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            print(f\"Best Model Epoch Saved: {epoch - patience+1}\")\n",
    "#             print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "#         if epochs_since_improvement >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "#             print(f\"Best Model Epoch Saved: {epoch - patience}\")\n",
    "#             break\n",
    "            \n",
    "        custom_metric = calculate_custom_metric(avg_val_loss, avg_train_loss)        \n",
    "#         custom_metric = calculate_custom_metric(avg_val_loss, val_accuracy, avg_train_loss, train_accuracy)\n",
    "        print(f\"\"\"\n",
    "        Validation Loss: {avg_val_loss},\n",
    "        Training Loss: {avg_train_loss},\n",
    "        Argmax Binary Validation Accuracy: {val_accuracy},\n",
    "        Argmax Binary Training Accuracy: {train_accuracy},\n",
    "        Custom Metric: {custom_metric},\n",
    "        Epochs: {epoch+1}\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"Best Validation Loss: {best_val_loss}, Best Validation accuracy: {val_accuracy}\")\n",
    "    \n",
    "    return model, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8a07b",
   "metadata": {
    "papermill": {
     "duration": 0.044113,
     "end_time": "2024-08-30T08:59:07.155945",
     "exception": false,
     "start_time": "2024-08-30T08:59:07.111832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BEST MODEL AFTER FINAL TRAINING & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77cdd958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T08:59:07.248345Z",
     "iopub.status.busy": "2024-08-30T08:59:07.247971Z",
     "iopub.status.idle": "2024-08-30T09:12:26.149432Z",
     "shell.execute_reply": "2024-08-30T09:12:26.148455Z"
    },
    "papermill": {
     "duration": 798.996574,
     "end_time": "2024-08-30T09:12:26.198343",
     "exception": false,
     "start_time": "2024-08-30T08:59:07.201769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Tuning Model with:\n",
      "    {'dropout_rate': 0.4, 'batch_size': 64, 'weight_decay': 0.001, 'lr': 3e-05, 'epochs': 10}\n",
      "    \n",
      "\n",
      "        Validation Loss: 0.05372150521725416,\n",
      "        Training Loss: 0.08907908306155406,\n",
      "        Argmax Binary Validation Accuracy: 0.8903441973804447,\n",
      "        Argmax Binary Training Accuracy: 0.8489004869044967,\n",
      "        Custom Metric: 0.0714002941394041,\n",
      "        Epochs: 1\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.05258408561348915,\n",
      "        Training Loss: 0.055628529421880214,\n",
      "        Argmax Binary Validation Accuracy: 0.8903441973804447,\n",
      "        Argmax Binary Training Accuracy: 0.8891576233968749,\n",
      "        Custom Metric: 0.05410630751768468,\n",
      "        Epochs: 2\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.0403012246824801,\n",
      "        Training Loss: 0.04977314407661767,\n",
      "        Argmax Binary Validation Accuracy: 0.908010965580262,\n",
      "        Argmax Binary Training Accuracy: 0.8902078095662412,\n",
      "        Custom Metric: 0.045037184379548886,\n",
      "        Epochs: 3\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03398801316507161,\n",
      "        Training Loss: 0.038303195013546606,\n",
      "        Argmax Binary Validation Accuracy: 0.9177581480353335,\n",
      "        Argmax Binary Training Accuracy: 0.9093339273780352,\n",
      "        Custom Metric: 0.03614560408930911,\n",
      "        Epochs: 4\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03802247624844313,\n",
      "        Training Loss: 0.031124567245723496,\n",
      "        Argmax Binary Validation Accuracy: 0.9116661590009137,\n",
      "        Argmax Binary Training Accuracy: 0.925086719918531,\n",
      "        Custom Metric: 0.041471430749802946,\n",
      "        Epochs: 5\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03134273085743189,\n",
      "        Training Loss: 0.027057751882034287,\n",
      "        Argmax Binary Validation Accuracy: 0.9220225403594273,\n",
      "        Argmax Binary Training Accuracy: 0.9328835566304936,\n",
      "        Custom Metric: 0.03348522034513069,\n",
      "        Epochs: 6\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.029827135615050793,\n",
      "        Training Loss: 0.023375688094488332,\n",
      "        Argmax Binary Validation Accuracy: 0.9238501370697533,\n",
      "        Argmax Binary Training Accuracy: 0.9400757406994876,\n",
      "        Custom Metric: 0.03305285937533202,\n",
      "        Epochs: 7\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.02823374909348786,\n",
      "        Training Loss: 0.01969397075894013,\n",
      "        Argmax Binary Validation Accuracy: 0.9238501370697533,\n",
      "        Argmax Binary Training Accuracy: 0.9480953441746491,\n",
      "        Custom Metric: 0.03250363826076172,\n",
      "        Epochs: 8\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.030717353569343686,\n",
      "        Training Loss: 0.016947995294147814,\n",
      "        Argmax Binary Validation Accuracy: 0.9201949436491015,\n",
      "        Argmax Binary Training Accuracy: 0.9542691658975909,\n",
      "        Custom Metric: 0.037602032706941624,\n",
      "        Epochs: 9\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.029793210793286562,\n",
      "        Training Loss: 0.014669839745308732,\n",
      "        Argmax Binary Validation Accuracy: 0.9195857447456595,\n",
      "        Argmax Binary Training Accuracy: 0.9596792158609936,\n",
      "        Custom Metric: 0.037354896317275475,\n",
      "        Epochs: 10\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.03198680025525391,\n",
      "        Training Loss: 0.012870849175772197,\n",
      "        Argmax Binary Validation Accuracy: 0.9156259518732867,\n",
      "        Argmax Binary Training Accuracy: 0.9647392037679406,\n",
      "        Custom Metric: 0.04154477579499477,\n",
      "        Epochs: 11\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027968540089204907,\n",
      "        Training Loss: 0.01162241615066436,\n",
      "        Argmax Binary Validation Accuracy: 0.9262869326835211,\n",
      "        Argmax Binary Training Accuracy: 0.9679534099226681,\n",
      "        Custom Metric: 0.03614160205847518,\n",
      "        Epochs: 12\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028177651343867183,\n",
      "        Training Loss: 0.011026655704202786,\n",
      "        Argmax Binary Validation Accuracy: 0.9253731343283582,\n",
      "        Argmax Binary Training Accuracy: 0.9694809534417465,\n",
      "        Custom Metric: 0.03675314916369938,\n",
      "        Epochs: 13\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.027795042376965284,\n",
      "        Training Loss: 0.010458994486277372,\n",
      "        Argmax Binary Validation Accuracy: 0.9256777337800792,\n",
      "        Argmax Binary Training Accuracy: 0.9711357922540814,\n",
      "        Custom Metric: 0.03646306632230924,\n",
      "        Epochs: 14\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028274751966819167,\n",
      "        Training Loss: 0.010378306790609176,\n",
      "        Argmax Binary Validation Accuracy: 0.9250685348766372,\n",
      "        Argmax Binary Training Accuracy: 0.9708812016675683,\n",
      "        Custom Metric: 0.03722297455492416,\n",
      "        Epochs: 15\n",
      "        \n",
      "\n",
      "        Validation Loss: 0.028108878526836634,\n",
      "        Training Loss: 0.010035195125555488,\n",
      "        Argmax Binary Validation Accuracy: 0.9241547365214743,\n",
      "        Argmax Binary Training Accuracy: 0.9720905069535054,\n",
      "        Custom Metric: 0.03714572022747721,\n",
      "        Epochs: 16\n",
      "        \n",
      "Best Validation Loss: 0.027795042376965284, Best Validation accuracy: 0.9241547365214743\n"
     ]
    }
   ],
   "source": [
    "best_config = best_trial.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model with the best configuration\n",
    "best_model, best_model_path = train_best_model(best_config, device, train_ds_pd, validation_ds_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde10e1",
   "metadata": {
    "papermill": {
     "duration": 0.045095,
     "end_time": "2024-08-30T09:12:26.288880",
     "exception": false,
     "start_time": "2024-08-30T09:12:26.243785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Save the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb740b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:26.381856Z",
     "iopub.status.busy": "2024-08-30T09:12:26.380970Z",
     "iopub.status.idle": "2024-08-30T09:12:27.591383Z",
     "shell.execute_reply": "2024-08-30T09:12:27.590035Z"
    },
    "papermill": {
     "duration": 1.260272,
     "end_time": "2024-08-30T09:12:27.594230",
     "exception": false,
     "start_time": "2024-08-30T09:12:26.333958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /kaggle/working/final_best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "# final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path) \n",
    "\n",
    "#### Comment when wanting to select the model which is stopped early to avoid over or under fitting\n",
    "\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# torch.save(best_model.state_dict(), final_model_path)\n",
    "torch.save({'model_state_dict': best_model.state_dict(),'config': best_config}, final_model_path)\n",
    "\n",
    "print(f\"Best model saved at: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89391bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:27.703962Z",
     "iopub.status.busy": "2024-08-30T09:12:27.703186Z",
     "iopub.status.idle": "2024-08-30T09:12:27.707377Z",
     "shell.execute_reply": "2024-08-30T09:12:27.706526Z"
    },
    "papermill": {
     "duration": 0.052896,
     "end_time": "2024-08-30T09:12:27.709219",
     "exception": false,
     "start_time": "2024-08-30T09:12:27.656323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=best_trial.config[\"dropout_rate\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8ac7ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:27.801379Z",
     "iopub.status.busy": "2024-08-30T09:12:27.801071Z",
     "iopub.status.idle": "2024-08-30T09:12:27.805150Z",
     "shell.execute_reply": "2024-08-30T09:12:27.804351Z"
    },
    "papermill": {
     "duration": 0.052569,
     "end_time": "2024-08-30T09:12:27.807066",
     "exception": false,
     "start_time": "2024-08-30T09:12:27.754497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kaggle_working_dir = '/kaggle/working'\n",
    "# best_model_path = os.path.join(kaggle_working_dir, \"checkpoint.pt\")\n",
    "# best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33fb195c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:27.901127Z",
     "iopub.status.busy": "2024-08-30T09:12:27.900342Z",
     "iopub.status.idle": "2024-08-30T09:12:27.904805Z",
     "shell.execute_reply": "2024-08-30T09:12:27.903881Z"
    },
    "papermill": {
     "duration": 0.053594,
     "end_time": "2024-08-30T09:12:27.906812",
     "exception": false,
     "start_time": "2024-08-30T09:12:27.853218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # best_config = result.get_best_config(metric='accuracy', mode='max')\n",
    "# # best_trial = result.get_best_trial(metric='accuracy', mode='max')\n",
    "\n",
    "# best_config = result.get_best_config(metric='custom_metric', mode='min')\n",
    "# best_trial = result.get_best_trial(metric='custom_metric', mode='min')\n",
    "\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "# print(\"\\nModel Parameters:\")\n",
    "# for name, param in best_model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3ca61c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:28.046016Z",
     "iopub.status.busy": "2024-08-30T09:12:28.045658Z",
     "iopub.status.idle": "2024-08-30T09:12:28.049759Z",
     "shell.execute_reply": "2024-08-30T09:12:28.048864Z"
    },
    "papermill": {
     "duration": 0.097958,
     "end_time": "2024-08-30T09:12:28.051775",
     "exception": false,
     "start_time": "2024-08-30T09:12:27.953817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_trial = tuner.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24539eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:28.144090Z",
     "iopub.status.busy": "2024-08-30T09:12:28.143775Z",
     "iopub.status.idle": "2024-08-30T09:12:28.148155Z",
     "shell.execute_reply": "2024-08-30T09:12:28.147341Z"
    },
    "papermill": {
     "duration": 0.052582,
     "end_time": "2024-08-30T09:12:28.150006",
     "exception": false,
     "start_time": "2024-08-30T09:12:28.097424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# best_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=best_trial.config[\"dropout_rate\"]\n",
    "# )\n",
    "\n",
    "# best_model_path = os.path.join(best_checkpoint_dir, \"checkpoint.pt\")\n",
    "# best_model.load_state_dict(torch.load(best_model_path))\n",
    "# best_model.eval()\n",
    "\n",
    "# print(f\"Best trial config: {best_trial.config}\")\n",
    "# print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "# print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86a63ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:28.243625Z",
     "iopub.status.busy": "2024-08-30T09:12:28.243277Z",
     "iopub.status.idle": "2024-08-30T09:12:28.247386Z",
     "shell.execute_reply": "2024-08-30T09:12:28.246591Z"
    },
    "papermill": {
     "duration": 0.052435,
     "end_time": "2024-08-30T09:12:28.249280",
     "exception": false,
     "start_time": "2024-08-30T09:12:28.196845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = EmotionClassifier(hidden_size=best_config['hidden_size'], dropout_rate=best_config['dropout_rate']).to(device)\n",
    "# checkpoint_path = \"best_model.pth\"\n",
    "# torch.save(best_model.state_dict(), checkpoint_path)\n",
    "# print(f\"Best model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efcfa226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:28.341491Z",
     "iopub.status.busy": "2024-08-30T09:12:28.341199Z",
     "iopub.status.idle": "2024-08-30T09:12:28.345137Z",
     "shell.execute_reply": "2024-08-30T09:12:28.344318Z"
    },
    "papermill": {
     "duration": 0.051811,
     "end_time": "2024-08-30T09:12:28.346950",
     "exception": false,
     "start_time": "2024-08-30T09:12:28.295139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the best model\n",
    "# best_model = EmotionClassifier(hidden_size=best_config['hidden_size'], dropout_rate=best_config['dropout_rate']).to(device)\n",
    "# bestmodel_path = f\"/kaggle/working/bestmodel.pt\"\n",
    "# torch.save(best_model.state_dict(), checkpoint_path)\n",
    "# print(f\"Best model saved to {bestmodel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17bdb53",
   "metadata": {
    "papermill": {
     "duration": 0.045555,
     "end_time": "2024-08-30T09:12:28.437785",
     "exception": false,
     "start_time": "2024-08-30T09:12:28.392230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32b67756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:28.531152Z",
     "iopub.status.busy": "2024-08-30T09:12:28.530823Z",
     "iopub.status.idle": "2024-08-30T09:12:30.188596Z",
     "shell.execute_reply": "2024-08-30T09:12:30.187638Z"
    },
    "papermill": {
     "duration": 1.706653,
     "end_time": "2024-08-30T09:12:30.190774",
     "exception": false,
     "start_time": "2024-08-30T09:12:28.484121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /kaggle/working/final_best_model.pt for inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "final_model_path = \"/kaggle/working/final_best_model.pt\"\n",
    "# final_model_path = \"/kaggle/input/tachygraphy-lv2-emotion-moodtags-v1/transformers/default/1/final_best_model.pt\"\n",
    "\n",
    "\n",
    "checkpoint = torch.load(final_model_path)\n",
    "config = checkpoint['config']\n",
    "\n",
    "# loaded_model = EmotionModel(\n",
    "#     roberta_model=RobertaModel.from_pretrained(\"roberta-base\"),\n",
    "#     n_classes=7,\n",
    "#     dropout_rate=config[\"dropout_rate\"]\n",
    "# )\n",
    "loaded_model = EmotionModel(\n",
    "    roberta_model=DebertaV2Model.from_pretrained('microsoft/deberta-v3-base'),\n",
    "    n_classes=7,\n",
    "    dropout_rate=config[\"dropout_rate\"]\n",
    ")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Loaded model from {final_model_path} for inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b0f78",
   "metadata": {
    "papermill": {
     "duration": 0.045232,
     "end_time": "2024-08-30T09:12:30.282918",
     "exception": false,
     "start_time": "2024-08-30T09:12:30.237686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PREDICT ON RANDOM INPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6316b50",
   "metadata": {
    "papermill": {
     "duration": 0.045054,
     "end_time": "2024-08-30T09:12:30.373444",
     "exception": false,
     "start_time": "2024-08-30T09:12:30.328390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fcd2336a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:30.466110Z",
     "iopub.status.busy": "2024-08-30T09:12:30.465455Z",
     "iopub.status.idle": "2024-08-30T09:12:30.472273Z",
     "shell.execute_reply": "2024-08-30T09:12:30.471403Z"
    },
    "papermill": {
     "duration": 0.054925,
     "end_time": "2024-08-30T09:12:30.474204",
     "exception": false,
     "start_time": "2024-08-30T09:12:30.419279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_len=128):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    return torch.sigmoid(outputs).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65773334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:30.566415Z",
     "iopub.status.busy": "2024-08-30T09:12:30.565840Z",
     "iopub.status.idle": "2024-08-30T09:12:30.575286Z",
     "shell.execute_reply": "2024-08-30T09:12:30.574568Z"
    },
    "papermill": {
     "duration": 0.057709,
     "end_time": "2024-08-30T09:12:30.577076",
     "exception": false,
     "start_time": "2024-08-30T09:12:30.519367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = loaded_model\n",
    "best_model = best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf1f7584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:30.673524Z",
     "iopub.status.busy": "2024-08-30T09:12:30.672867Z",
     "iopub.status.idle": "2024-08-30T09:12:31.320596Z",
     "shell.execute_reply": "2024-08-30T09:12:31.319748Z"
    },
    "papermill": {
     "duration": 0.699525,
     "end_time": "2024-08-30T09:12:31.322864",
     "exception": false,
     "start_time": "2024-08-30T09:12:30.623339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7b927",
   "metadata": {
    "papermill": {
     "duration": 0.04822,
     "end_time": "2024-08-30T09:12:31.420061",
     "exception": false,
     "start_time": "2024-08-30T09:12:31.371841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Samples & Random Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e904fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:31.514906Z",
     "iopub.status.busy": "2024-08-30T09:12:31.514476Z",
     "iopub.status.idle": "2024-08-30T09:12:33.506473Z",
     "shell.execute_reply": "2024-08-30T09:12:33.505566Z"
    },
    "papermill": {
     "duration": 2.042101,
     "end_time": "2024-08-30T09:12:33.508741",
     "exception": false,
     "start_time": "2024-08-30T09:12:31.466640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'hey! hru, wanna ply valo toni8?': [[0.01314835 0.01216354 0.04213838 0.17797935 0.23532231 0.05119939\n",
      "  0.45410016]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"3769dfd1-c3bd-4a73-9edc-8aca90b2c9fb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3769dfd1-c3bd-4a73-9edc-8aca90b2c9fb\")) {                    Plotly.newPlot(                        \"3769dfd1-c3bd-4a73-9edc-8aca90b2c9fb\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.013148351,0.012163539,0.042138383,0.17797935,0.23532231,0.05119939,0.45410016,0.013148351],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('3769dfd1-c3bd-4a73-9edc-8aca90b2c9fb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb411c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:33.605493Z",
     "iopub.status.busy": "2024-08-30T09:12:33.604739Z",
     "iopub.status.idle": "2024-08-30T09:12:33.973088Z",
     "shell.execute_reply": "2024-08-30T09:12:33.972098Z"
    },
    "papermill": {
     "duration": 0.41775,
     "end_time": "2024-08-30T09:12:33.975063",
     "exception": false,
     "start_time": "2024-08-30T09:12:33.557313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'what a badass char is arthur, he is the best game char ever made, i luv rdr2': [[0.008625   0.00915726 0.00969777 0.8995087  0.03677545 0.02132461\n",
      "  0.09309176]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"3408ed9a-3f3f-4270-8f8c-99cdb744cfd0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3408ed9a-3f3f-4270-8f8c-99cdb744cfd0\")) {                    Plotly.newPlot(                        \"3408ed9a-3f3f-4270-8f8c-99cdb744cfd0\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.008624998,0.009157258,0.0096977735,0.8995087,0.036775455,0.021324612,0.09309176,0.008624998],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('3408ed9a-3f3f-4270-8f8c-99cdb744cfd0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"what a badass char is arthur, he is the best game char ever made, i luv rdr2\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "# fig.show()\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73f6a5a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:34.073062Z",
     "iopub.status.busy": "2024-08-30T09:12:34.072716Z",
     "iopub.status.idle": "2024-08-30T09:12:34.444936Z",
     "shell.execute_reply": "2024-08-30T09:12:34.443915Z"
    },
    "papermill": {
     "duration": 0.422031,
     "end_time": "2024-08-30T09:12:34.446906",
     "exception": false,
     "start_time": "2024-08-30T09:12:34.024875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotions for 'I don't no fr y hes sooo sad.': [[0.02565536 0.00656853 0.01630233 0.01994199 0.01922075 0.96058047\n",
      "  0.02723373]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"d98751a2-2857-404d-aaa2-85f6c952e2e9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d98751a2-2857-404d-aaa2-85f6c952e2e9\")) {                    Plotly.newPlot(                        \"d98751a2-2857-404d-aaa2-85f6c952e2e9\",                        [{\"hovertemplate\":\"r=%{r}\\u003cbr\\u003etheta=%{theta}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"r\":[0.025655359,0.0065685324,0.016302332,0.019941993,0.019220749,0.96058047,0.027233735,0.025655359],\"showlegend\":false,\"subplot\":\"polar\",\"theta\":[\"anger\",\"disgust\",\"fear\",\"joy\",\"neutral\",\"sadness\",\"surprise\",\"anger\"],\"type\":\"scatterpolar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"angularaxis\":{\"direction\":\"clockwise\",\"rotation\":90}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d98751a2-2857-404d-aaa2-85f6c952e2e9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_text = \"I don't no fr y hes sooo sad.\"\n",
    "predictions = predict(sample_text, best_model, tokenizer, device, max_len=128)\n",
    "print(f\"Predicted emotions for '{sample_text}': {predictions}\")\n",
    "\n",
    "predictions_array = predictions.squeeze()  # Remove unnecessary dimensions\n",
    "\n",
    "\n",
    "emotion_moodtags = []\n",
    "for items in predictions_array:\n",
    "    emotion_moodtags.append(items)\n",
    "\n",
    "fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "fig.show()\n",
    "\n",
    "# fig = px.line_polar(pd.DataFrame(dict(r=emotion_moodtags, theta=EMOTION_LABELS)), r='r', theta='theta', line_close=True)\n",
    "# fig.show()\n",
    "\n",
    "    \n",
    "# Since predictions are probabilistic values, normalize to ensure they sum up to 1\n",
    "normalized_predictions = predictions_array / predictions_array.sum()  # Normalize the values\n",
    "\n",
    "## Horizontal Stacked Bar Plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "left = 0\n",
    "for i in range(len(normalized_predictions)):\n",
    "    ax.barh(0, normalized_predictions[i], color=plt.cm.tab10(i), left=left, label=EMOTION_LABELS[i])\n",
    "    left += normalized_predictions[i]\n",
    "\n",
    "# Add legend\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(EMOTION_LABELS))\n",
    "\n",
    "# Add title\n",
    "plt.title('Emotion Prediction Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7466e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:34.544673Z",
     "iopub.status.busy": "2024-08-30T09:12:34.544286Z",
     "iopub.status.idle": "2024-08-30T09:12:34.548407Z",
     "shell.execute_reply": "2024-08-30T09:12:34.547485Z"
    },
    "papermill": {
     "duration": 0.05505,
     "end_time": "2024-08-30T09:12:34.550332",
     "exception": false,
     "start_time": "2024-08-30T09:12:34.495282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# predictions = predict(sample_text, best_model, tokenizer, max_len=128, device=device)\n",
    "# print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2d635f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:34.648515Z",
     "iopub.status.busy": "2024-08-30T09:12:34.648188Z",
     "iopub.status.idle": "2024-08-30T09:12:34.652010Z",
     "shell.execute_reply": "2024-08-30T09:12:34.651157Z"
    },
    "papermill": {
     "duration": 0.055506,
     "end_time": "2024-08-30T09:12:34.653838",
     "exception": false,
     "start_time": "2024-08-30T09:12:34.598332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# predictions = predict(sample_text, best_model, tokenizer, max_len=128, device=device)\n",
    "# print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c0ed2dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-30T09:12:34.757930Z",
     "iopub.status.busy": "2024-08-30T09:12:34.757590Z",
     "iopub.status.idle": "2024-08-30T09:12:34.761633Z",
     "shell.execute_reply": "2024-08-30T09:12:34.760828Z"
    },
    "papermill": {
     "duration": 0.061824,
     "end_time": "2024-08-30T09:12:34.763439",
     "exception": false,
     "start_time": "2024-08-30T09:12:34.701615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_text = \"hey! hru, wanna ply valo toni8?\"\n",
    "# best_model = RoBERTaEmotionModel('roberta-base').to(device)\n",
    "# best_model.load_state_dict(torch.load(\"best_model.pth\"))  # Assuming best model is saved during training\n",
    "# predicted_emotions = predict_emotions(best_model, sample_text, tokenizer, best_params['max_len'], device)\n",
    "# print(predicted_emotions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5557640,
     "sourceId": 9273793,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18718.069441,
   "end_time": "2024-08-30T09:12:40.131589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-30T04:00:42.062148",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0289c76811454215aec7cc380fd0196e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "03b54c49e8f94360b9a019630b13d34d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "059b5ddbfd834d2a942fe72e7eae2c60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0b8979a4f8ca4e3cae1d6dcd3f4b3bfa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e47b93220474e988c299d529ef0907f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "104b795854384d84b93534bc90c838d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0b8979a4f8ca4e3cae1d6dcd3f4b3bfa",
       "placeholder": "​",
       "style": "IPY_MODEL_a98f8fc2656d43849f8dbd863dd768e1",
       "value": "spm.model: 100%"
      }
     },
     "1df7533d30494946af17889c2a7e89af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "206484e84efd448ca9778f24c5c615c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2d65d63331f544bb9eb87f76f634cd87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2fde23491f4246f197bad46e426c60bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0289c76811454215aec7cc380fd0196e",
       "max": 2464616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_89a9f2ce3bd64a11bfe50ead8759d3d4",
       "value": 2464616.0
      }
     },
     "522a0540e4fe48f1a4b0da77aba0d2f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e47b93220474e988c299d529ef0907f",
       "placeholder": "​",
       "style": "IPY_MODEL_f7396d4139304868b8ca8850256ee03e",
       "value": "config.json: 100%"
      }
     },
     "5287014b94be4ba3973d32df827eed92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "529db47286d94f4fbbbe2fa0c638fa20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55a5191f45024eb396e3c723fbf2bb1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_529db47286d94f4fbbbe2fa0c638fa20",
       "placeholder": "​",
       "style": "IPY_MODEL_059b5ddbfd834d2a942fe72e7eae2c60",
       "value": " 579/579 [00:00&lt;00:00, 48.3kB/s]"
      }
     },
     "55ecae91cb2747e3a6c047d1c1fa3b4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78eed03f3abd4a53861ce2774c3e05bc",
       "placeholder": "​",
       "style": "IPY_MODEL_f28e06179dba4c1cb38b9012239e5f37",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "66362755747d47b797045d3bed73840f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e5b501c8b86b4c34b61fd420aa99be73",
       "placeholder": "​",
       "style": "IPY_MODEL_f7235c00de9949869395175e93e1590e",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 24.5MB/s]"
      }
     },
     "78eed03f3abd4a53861ce2774c3e05bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7eababcc8f664d65b0aae155db771c2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_104b795854384d84b93534bc90c838d5",
        "IPY_MODEL_2fde23491f4246f197bad46e426c60bf",
        "IPY_MODEL_66362755747d47b797045d3bed73840f"
       ],
       "layout": "IPY_MODEL_5287014b94be4ba3973d32df827eed92"
      }
     },
     "7ee8857de8f745bb9ac71dd1ba800678": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "822e2843f6dd4d01a2e940af7d26ec71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_55ecae91cb2747e3a6c047d1c1fa3b4f",
        "IPY_MODEL_f04c7e2b3ea5411a9a19a76ddaebc5eb",
        "IPY_MODEL_fb51d08eb061468ca7776ee09076b525"
       ],
       "layout": "IPY_MODEL_e41f966054f04c0883bab4cd42a06fd0"
      }
     },
     "89a9f2ce3bd64a11bfe50ead8759d3d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a5f0b9afc8154c79abdaec4a92727c3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a98f8fc2656d43849f8dbd863dd768e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "af1dcefeb3984ec29ce985f44e286d95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b7deb19261d04e3e91fc6f3c19ffb16a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7ee8857de8f745bb9ac71dd1ba800678",
       "max": 579.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_03b54c49e8f94360b9a019630b13d34d",
       "value": 579.0
      }
     },
     "c16f39eefbe343b0b37c50827071f529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_522a0540e4fe48f1a4b0da77aba0d2f4",
        "IPY_MODEL_b7deb19261d04e3e91fc6f3c19ffb16a",
        "IPY_MODEL_55a5191f45024eb396e3c723fbf2bb1e"
       ],
       "layout": "IPY_MODEL_1df7533d30494946af17889c2a7e89af"
      }
     },
     "e41f966054f04c0883bab4cd42a06fd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5b501c8b86b4c34b61fd420aa99be73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f04c7e2b3ea5411a9a19a76ddaebc5eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d65d63331f544bb9eb87f76f634cd87",
       "max": 52.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_af1dcefeb3984ec29ce985f44e286d95",
       "value": 52.0
      }
     },
     "f28e06179dba4c1cb38b9012239e5f37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7235c00de9949869395175e93e1590e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7396d4139304868b8ca8850256ee03e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fb51d08eb061468ca7776ee09076b525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a5f0b9afc8154c79abdaec4a92727c3b",
       "placeholder": "​",
       "style": "IPY_MODEL_206484e84efd448ca9778f24c5c615c6",
       "value": " 52.0/52.0 [00:00&lt;00:00, 3.91kB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
